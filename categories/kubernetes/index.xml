<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>kubernetes on The road</title><link>https://kane.mx/categories/kubernetes/</link><description>Recent content in kubernetes on The road</description><generator>Hugo -- gohugo.io</generator><copyright>Copyright © 2021, Kane Zhu; all rights reserved.</copyright><lastBuildDate>Mon, 29 Apr 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://kane.mx/categories/kubernetes/index.xml" rel="self" type="application/rss+xml"/><item><title>Spring Cloud or Cloud Native</title><link>https://kane.mx/posts/effective-cloud-computing/spring-cloud-or-cloud-native/</link><pubDate>Mon, 29 Apr 2019 00:00:00 +0000</pubDate><guid>https://kane.mx/posts/effective-cloud-computing/spring-cloud-or-cloud-native/</guid><description>
&lt;p>基于Java的&lt;a href="https://spring.io/projects/spring-cloud">Spring Cloud&lt;/a>是由Java最大开源生态&lt;a href="https://spring.io/">Spring&lt;/a>社区推出的Out-of-Box分布式&lt;a href="https://en.wikipedia.org/wiki/Microservices">微服务&lt;/a>解决方案，&lt;a href="https://github.com/spring-projects/spring-cloud/wiki/Spring-Cloud-Angel-Release-Notes/6e0e1ba3d510d4a30b95c1468007b22f2427fa25">自2016年发布&lt;/a>起就被众多开发者看好。Java作为广为流行的服务端编程语言，&lt;a href="https://spring.io/projects/spring-cloud">Spring Cloud&lt;/a>也就越来越多的被用于微服务开发。&lt;/p>
&lt;p>&lt;a href="https://spring.io/projects/spring-cloud">Spring Cloud&lt;/a>集成了&lt;a href="https://netflix.github.io/">Netflix OSS&lt;/a>开源项目实现了很多功能(或作为实现之一)，包括服务治理、网关路由、客户端负载均衡、服务间调用、断路器等。&lt;a href="https://spring.io/projects/spring-cloud-netflix">Spring Cloud Netflix&lt;/a>将很多生产级别微服务能力开箱即用的带到了Spring Cloud架构下的微服务中，帮助开发者快速的构建满足&lt;a href="https://12factor.net/">12要素&lt;/a>的应用。&lt;/p>
&lt;p>在去年底发布的&lt;a href="https://spring.io/blog/2018/12/12/spring-cloud-greenwich-rc1-available-now#spring-cloud-netflix-projects-entering-maintenance-mode">Spring Cloud Greenwich版本&lt;/a>中宣布&lt;a href="https://spring.io/projects/spring-cloud-netflix">Spring Cloud Netflix&lt;/a>中重要的组件&lt;a href="https://github.com/Netflix/Hystrix#hystrix-status">Hystrix&lt;/a>、&lt;a href="https://github.com/Netflix/ribbon#project-status-on-maintenance">Ribbon&lt;/a>、&lt;code>Zuul 1&lt;/code>等由于上游开源项目进入维护状态，对应的Spring Cloud Netflix项目也进入到维护状态。这些项目将&lt;strong>不再适合&lt;/strong>用于长期维护的产品中！&lt;/p>
&lt;p>同时随着近年云计算的发展，特别是&lt;a href="https://kubernetes.io/">Kubernetes&lt;/a>成为容器编排平台的事实标准，加上&lt;a href="https://www.nginx.com/blog/what-is-a-service-mesh/">Service Mesh(服务网格)&lt;/a>对微服务的服务治理和流量控制，为&lt;a href="https://www.redhat.com/en/topics/cloud-native-apps">云原生应用&lt;/a>提供了更为现代、平台无关的解决方案。&lt;/p>
&lt;p>让我们逐一看看在&lt;a href="https://kubernetes.io/">Kubernetes&lt;/a>加上Serivce Mesh(例如&lt;a href="https://istio.io/">Istio&lt;/a>)如何实现微服务的服务发现、路由、链路追踪、断路器等功能。&lt;/p>
&lt;h3 id="配置中心">配置中心&lt;/h3>
&lt;p>&lt;a href="https://spring.io/projects/spring-cloud-config">Spring Cloud Config&lt;/a>默认提供了多种配置管理后端，例如&lt;code>Git&lt;/code>、&lt;code>Vault&lt;/code>、&lt;code>JDBC Backend&lt;/code>等。同时也有很多开源方案可以作为替换方案，比如&lt;a href="https://github.com/alibaba/nacos">Alibaba Nacos&lt;/a>。&lt;/p>
&lt;p>作为部署在&lt;a href="https://kubernetes.io/">Kubernetes&lt;/a>中的应用，最佳实践是平衡&lt;a href="https://kubernetes.io/docs/user-guide/configmap/">Configmap&lt;/a>和&lt;a href="https://spring.io/projects/spring-cloud-config">Spring Cloud Config&lt;/a>。将涉及程序功能的配置放置在&lt;a href="https://kubernetes.io/docs/user-guide/configmap/">Configmap&lt;/a>和Secret，随同微服务的发布一起做版本管理，可以做到&lt;strong>随着应用回退的时候同时回退到历史对应的配置版本&lt;/strong>，而不会因为历史版本的代码被最新版本的配置所中断。&lt;a href="https://github.com/spring-cloud/spring-cloud-kubernetes">Spring Cloud Kuberentes&lt;/a>项目很好的支持了Spring Cloud应用从&lt;a href="https://github.com/spring-cloud/spring-cloud-kubernetes#kubernetes-propertysource-implementations">Configmap&lt;/a>和&lt;a href="https://github.com/spring-cloud/spring-cloud-kubernetes#secrets-propertysource">Secret&lt;/a>中读取配置项。而涉及业务的配置选项，将可以考虑放到Spring Cloud Config后端实现统一管理。如果应用是部署在阿里云，使用阿里云托管的配置服务和&lt;a href="https://github.com/alibaba/nacos">Spring Cloud Config -- Nacos&lt;/a>将是很好的选择。&lt;/p>
&lt;h3 id="服务发现">服务发现&lt;/h3>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/#discovering-services">Kubernetes Services&lt;/a>提供了集群内原生的服务发现能力，是&lt;a href="https://spring.io/projects/spring-cloud-netflix">Eureka&lt;/a>或&lt;a href="https://spring.io/projects/spring-cloud-zookeeper">Spring Cloud Zookeeper&lt;/a>等服务发现服务的很好替代品。基于K8S Services的服务发现，很容易通过Service Mesh能力实现限流、A/B测试、金丝雀发布、断路器、chaos注入等服务治理能力。同时对微服务应用来说，不用在应用端添加对应三方库来实现服务注册及发现，减少了应用端开发需求。&lt;/p>
&lt;h3 id="各种流量治理场景">各种流量治理场景&lt;/h3>
&lt;p>应用被服务化后，一定会面临流量治理的问题。对于各种服务间如何实现限流、A/B测试、金丝雀发布、断路器、chaos注入测试、链接追踪等，这其实是一类通用的问题。&lt;/p>
&lt;p>&lt;a href="https://spring.io/projects/spring-cloud">Spring Cloud&lt;/a>提供的是一种客户端解决思路，需要每个应用引入对应功能的libraries的支持。即使通过&lt;a href="https://www.baeldung.com/spring-boot-starters">spring boot starter&lt;/a>提供了近似开箱即用的能力，但是每个应用仍然需要自行添加对应的能力，版本更新、安全漏洞fix等场景都需要手动升级、测试、打包、部署。在异构编程语言实现的微服务架构下，未必每种编程框架都能提供很好的对应能力支持。除非有特别的服务治理策略，不推荐在微服务自身来实现服务流量的控制。&lt;/p>
&lt;p>Service Mesh(例如&lt;a href="https://istio.io/">Istio&lt;/a>或&lt;a href="https://linkerd.io/">Linkerd&lt;/a>)从整个服务治理层面对上述需求提供了统一的解决方案，而不需要微服务做自身的升级或改动。在基于Kuberentes部署运行的微服务应用，Service Mesh提供了统一的服务治理方案，将用户从不同的微服务中自身维护服务治理功能中解放出来，从平台层面提供更加统一一致的解决方案。&lt;/p>
&lt;p>在去年的SpringOne Platform 2018上也有一个Topic &lt;a href="https://youtu.be/AMJQO9zs2eo">A Tale of Two Frameworks: Spring Cloud and Istio&lt;/a> 探讨什么场景应该使用Service Mesh，什么时候使用Spring Cloud服务治理组件，有兴趣的朋友可以看一看。&lt;/p>
&lt;div class="youtube container">
&lt;iframe class="youtube" type="text/html"
src="https://www.youtube.com/embed/AMJQO9zs2eo"
allowfullscreen frameborder="0">
&lt;/iframe>
&lt;/div></description></item><item><title>为Kubernetes中任意应用添加基于oauth2的认证保护 (下)</title><link>https://kane.mx/posts/effective-cloud-computing/oauth2-proxy-on-kubernetes/part2/</link><pubDate>Mon, 08 Apr 2019 00:00:00 +0000</pubDate><guid>https://kane.mx/posts/effective-cloud-computing/oauth2-proxy-on-kubernetes/part2/</guid><description>
&lt;p>本文是&lt;a href="https://kane.mx/posts/effective-cloud-computing/oauth2-proxy-on-kubernetes/part1/">为Kubernetes中任意应用添加基于oauth2的认证保护&lt;/a>的下篇，将图文详解如何使用基于&lt;a href="https://open-doc.dingtalk.com/microapp/serverapi2/kymkv6">钉钉认证&lt;/a>的&lt;a href="https://github.com/bitly/oauth2_proxy">oauth2 proxy&lt;/a>为自身本没有认证授权功能的Web站点实现认证及授权。&lt;/p>
&lt;blockquote>
&lt;p>示例是使用的&lt;a href="https://aws.amazon.com/eks/?nc1=f_ls">AWS EKS&lt;/a>服务作为K8S环境。鉴于K8S的应用运行时属性，该示例也可以部署在其他云厂商托管的K8S。&lt;/p>
&lt;/blockquote>
&lt;h3 id="示例模块简介">示例模块简介&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/ingress-nginx">Nginx Ingress Controller&lt;/a>为K8S集群内Web应用提供反向代理，以及支持外部认证。&lt;/li>
&lt;li>简单的Web站点，基于&lt;a href="https://hub.docker.com/_/nginx">Nginx docker容器&lt;/a>。该站点默认没有认证及授权功能，使用外部&lt;a href="https://open-doc.dingtalk.com/microapp/serverapi2/kymkv6">钉钉&lt;/a>应用作为认证及授权。&lt;/li>
&lt;li>&lt;a href="https://github.com/zxkane/oauth2_proxy">OAuth2 Proxy on Dingtalk&lt;/a>提供基于&lt;a href="https://open-doc.dingtalk.com/microapp/serverapi2/kymkv6">钉钉&lt;/a>应用的扫码认证及授权，只有认证且授权的用户才可以访问上面的Web站点。&lt;/li>
&lt;/ul>
&lt;h3 id="默认设定">默认设定&lt;/h3>
&lt;ul>
&lt;li>Web站点域名&lt;code>web.kane.mx&lt;/code>&lt;/li>
&lt;li>认证服务域名&lt;code>oauth.kane.mx&lt;/code>&lt;/li>
&lt;/ul>
&lt;h3 id="准备aws-eksaws-eks环境">准备&lt;a href="https://aws.amazon.com/eks/?nc1=f_ls">AWS EKS&lt;/a>环境&lt;/h3>
&lt;ol>
&lt;li>&lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/create-cluster.html">创建EKS集群&lt;/a>。由于&lt;a href="https://github.com/kubernetes/ingress-nginx">Nginx Ingress&lt;/a>服务是LoadBalancer类型，EKS创建NLB或ELB对应的targets时需要targets部署在public VPC subnets，所以为了简化部署EKS集群的VPC subnets都选择public subnet。新建的EKS集群允许公开访问。&lt;/li>
&lt;li>&lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/managing-auth.html">本地安装配置kubectl, aws-iam-authenticator&lt;/a>用于远程管理集群。&lt;/li>
&lt;li>&lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/launch-workers.html">为集群添加worker节点&lt;/a>。&lt;/li>
&lt;li>&lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/helm.html">配置Helm部署环境&lt;/a>。&lt;/li>
&lt;/ol>
&lt;h3 id="钉钉应用准备">钉钉应用准备&lt;/h3>
&lt;ol>
&lt;li>为企业或组织开通&lt;a href="https://open-dev.dingtalk.com/#/index">钉钉开发平台&lt;/a>&lt;/li>
&lt;li>创建一个新的&lt;a href="https://open-dev.dingtalk.com/#/loginAndShareApp">移动应用&lt;/a>。回调域名填写&lt;code>&amp;lt;http or https&amp;gt;/&amp;lt;认证服务域名&amp;gt;/oauth2/callback&lt;/code>。记录下来应用的&lt;code>appId&lt;/code>和&lt;code>appSecret&lt;/code>。&lt;/li>
&lt;li>创建一个&lt;a href="https://open-dev.dingtalk.com/#/create-bench/self">企业内部工作台应用&lt;/a>。地址可以随意设置。服务器出口IP设置为&lt;code>EKS集群中工作节点的公网IP&lt;/code>或者&lt;code>NAT EIP&lt;/code>，取决于工作节点如何访问Internet。并记录下来应用&lt;code>appKey&lt;/code>和&lt;code>appSecret&lt;/code>。&lt;/li>
&lt;/ol>
&lt;h3 id="部署示例应用">部署示例应用&lt;/h3>
&lt;ol>
&lt;li>克隆&lt;a href="https://github.com/zxkane/hands-on-dingtalk-oauth2-proxy">示例&lt;/a>部署脚本。&lt;/li>
&lt;li>替换&lt;code>values.yaml&lt;/code>中的&lt;code>dingtalk_corpid&lt;/code>为工作台应用的&lt;code>appKey&lt;/code>， &lt;code>dingtalk_corpsecret&lt;/code>为工作台应用的&lt;code>appSecret&lt;/code>。
由于社区维护的&lt;a href="https://github.com/helm/charts/tree/master/stable/oauth2-proxy">oauth2-proxy charts&lt;/a>并不支持dingtalk扩展的SECRET ENV，所以将密钥配置到了&lt;code>configmap&lt;/code>中。用于生产环境的话，建议按&lt;a href="https://github.com/pilipa-cn/charts/commit/7ac0f67acc71577275f743bdcf9a870bd65361b0">这个commit&lt;/a>使用&lt;code>secret&lt;/code>保存应用secret。
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">62
&lt;/span>&lt;span class="lnt">63
&lt;/span>&lt;span class="lnt">64
&lt;/span>&lt;span class="lnt">65
&lt;/span>&lt;span class="lnt">66
&lt;/span>&lt;span class="lnt">67
&lt;/span>&lt;span class="lnt">68
&lt;/span>&lt;span class="lnt">69
&lt;/span>&lt;span class="lnt">70
&lt;/span>&lt;span class="hl">&lt;span class="lnt">71
&lt;/span>&lt;/span>&lt;span class="hl">&lt;span class="lnt">72
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="nt">oauth2-proxy&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">config&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">clientID&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">aaa&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">clientSecret&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">bbb&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">cookieSecret&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ccc&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">configFile&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">|+&lt;/span>&lt;span class="sd">
&lt;/span>&lt;span class="sd"> email_domains = [ &amp;#34;*&amp;#34; ]
&lt;/span>&lt;span class="sd"> cookie_domain = &amp;#34;kane.mx&amp;#34;
&lt;/span>&lt;span class="sd"> cookie_secure = false
&lt;/span>&lt;span class="hl">&lt;span class="sd"> dingtalk_corpid = &amp;#34;&amp;lt;appkey of dingtalk app&amp;gt;&amp;#34;
&lt;/span>&lt;/span>&lt;span class="hl">&lt;span class="sd"> dingtalk_corpsecret = &amp;#34;&amp;lt;appsecret of dingtalk app&amp;gt;&amp;#34;&lt;/span>&lt;span class="w"> &lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>
如果仅希望企业部分部门的员工可以获得授权，在上面&lt;code>configFile&lt;/code>配置下添加如下配置，
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="ln">1&lt;/span>&lt;span class="l">dingtalk_departments = [&amp;#34;xx公司/产品技术中心&amp;#34;,&amp;#34;xx公司/部门2/子部门3&amp;#34;]&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>替换部署应用的域名为你的域名。&lt;/li>
&lt;li>执行以下命令安装Helm部署依赖。
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="ln">1&lt;/span>helm dep up&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>执行以下命令部署nginx ingress controller, web应用以及oauth2 proxy
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="ln">1&lt;/span>helm upgrade --install -f values.yaml --set oauth2-proxy.config.clientID&lt;span class="o">=&lt;/span>&amp;lt;移动应用appid&amp;gt;,oauth2-proxy.config.clientSecret&lt;span class="o">=&lt;/span>&amp;lt;移动应用appsecret&amp;gt; site-with-auth --wait ./&lt;/code>&lt;/pre>&lt;/div>
如果集群中已经部署了&lt;code>Nginx Ingress Controller&lt;/code>，修改&lt;code>values.yaml&lt;/code>如下将忽略部署Nginx ingress，
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre class="chroma">&lt;code>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;span class="hl">&lt;span class="lnt">50
&lt;/span>&lt;/span>&lt;span class="lnt">51
&lt;/span>&lt;span class="lnt">52
&lt;/span>&lt;span class="lnt">53
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="nt">affinity&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>{}&lt;span class="w">
&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">nginx-ingress&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="hl">&lt;span class="w"> &lt;/span>&lt;span class="nt">enabled&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">false&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">controller&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">ingressClass&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">nginx&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">config:&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;/li>
&lt;li>部署成功后，获取&lt;code>ELB&lt;/code>地址。
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-zsh" data-lang="zsh">&lt;span class="ln">1&lt;/span>kubectl get svc -o &lt;span class="nv">jsonpath&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;{ $.status.loadBalancer.ingress[*].hostname }&amp;#39;&lt;/span> &amp;lt;deployment name&amp;gt;-nginx-ingress-controller&lt;span class="p">;&lt;/span>&lt;span class="nb">echo&lt;/span>
&lt;span class="ln">2&lt;/span>a3afe672259c511e98e2a0a0d88fda3e-xx.elb.ap-southeast-1.amazonaws.com&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ol>
&lt;h3 id="部署成功后配置">部署成功后配置&lt;/h3>
&lt;p>将站点和oauth服务域名解析到上面部署创建的ELB上。&lt;/p>
&lt;h3 id="测试">测试&lt;/h3>
&lt;p>访问Web站点(如本示例中的&lt;code>http://web.kane.mx&lt;/code>)，未授权的情况下，调转到钉钉应用扫码登录界面。使用组织内成员的钉钉扫码授权后，将跳转回Web站点应用，可以正常浏览该域名下的页面。&lt;/p></description></item><item><title>为Kubernetes中任意应用添加基于oauth2的认证保护 (上)</title><link>https://kane.mx/posts/effective-cloud-computing/oauth2-proxy-on-kubernetes/part1/</link><pubDate>Sun, 03 Feb 2019 00:00:00 +0000</pubDate><guid>https://kane.mx/posts/effective-cloud-computing/oauth2-proxy-on-kubernetes/part1/</guid><description>
&lt;p>企业随着业务的发展，必然会部署各种各样的IT系统。出于安全性的考虑，一些系统仅可企业内部使用，甚至仅开放给企业部分部门员工使用。&lt;/p>
&lt;p>这些IT系统大致可分为两类，&lt;/p>
&lt;ol>
&lt;li>系统本身不支持任何认证机制，例如资讯或文档类系统。需要增加认证保护，能够限制非企业员工访问即可。系统运维通常的做法是，为站点设置&lt;a href="https://en.wikipedia.org/wiki/Basic_access_authentication">HTTP Basic认证&lt;/a>保护。由于&lt;a href="https://en.wikipedia.org/wiki/Basic_access_authentication">HTTP Basic认证&lt;/a>是通过预设的用户、密码认证，认证信息比较容易泄露。即使定期更换密码，但需要额外的机制通知用户密码的变更，用户体验也不好。&lt;/li>
&lt;li>系统自身支持认证，甚至支持多种认证机制。比如最常用的开源CI/CD工具，&lt;a href="https://jenkins.io/">Jenkins&lt;/a>内置支持本地数据库认证、通过&lt;a href="https://plugins.jenkins.io/#">插件&lt;/a>支持多种第三方系统集成认证。如果大量的IT系统都有一套独立的用户管理，随着企业的员工的变更，用户的增删等操作对系统管理员来说是不小的工作量。同时，也很容易由于人为疏忽，造成资产、数据的安全隐患。&lt;/li>
&lt;/ol>
&lt;p>假设企业自身已经有了一套OA系统包含员工、组织结构管理，例如，国内目前最为普及流行的&lt;a href="https://www.dingtalk.com/">钉钉&lt;/a>或&lt;a href="https://work.weixin.qq.com/">企业微信&lt;/a>。我们完全可以提供一套基于&lt;a href="https://oauth.net/2/">oauth 2.0协议&lt;/a>的认证方式，让以上两类IT系统使用企业已有的OA系统(&lt;a href="https://www.dingtalk.com/">钉钉&lt;/a>或&lt;a href="https://work.weixin.qq.com/">企业微信&lt;/a>)来实现登录认证。做到这一点后，企业无论有多少IT系统都不再需要额外管理用户的成本，并且也避免了数据安全隐患。&lt;/p>
&lt;p>&lt;a href="https://www.dingtalk.com/">钉钉&lt;/a>通过&lt;a href="https://open-dev.dingtalk.com">钉钉开放平台&lt;/a>提供的API开放了许多钉钉内部的能力，例如，&lt;a href="https://open-doc.dingtalk.com/microapp/serverapi2/vt6khw">身份验证&lt;/a>、&lt;a href="https://open-doc.dingtalk.com/microapp/serverapi2/cqfmel">通讯录管理&lt;/a>等等。然而&lt;a href="https://open-doc.dingtalk.com/microapp/serverapi2/kymkv6">钉钉的三方网站登录接口&lt;/a>并不是标准的&lt;a href="https://oauth.net/2/">oauth 2.0协议&lt;/a>实现，我们需要通过一个&lt;a href="https://github.com/zxkane/oauth2_proxy">oauth2 proxy&lt;/a>代理工具实现将&lt;a href="https://open-doc.dingtalk.com/microapp/serverapi2/kymkv6">钉钉的三方网站登录&lt;/a>兼容&lt;a href="https://oauth.net/2/">oauth2&lt;/a>协议。同理，使用&lt;a href="https://github.com/bitly/oauth2_proxy">这个oauth2代理工具&lt;/a>，可以使用&lt;a href="https://github.com/bitly/oauth2_proxy#google-auth-provider">Google&lt;/a>、&lt;a href="https://github.com/bitly/oauth2_proxy#facebook-auth-provider">Facebook&lt;/a>等三方网站作为统一认证方式。&lt;/p>
&lt;p>有了基于&lt;a href="https://github.com/zxkane/oauth2_proxy">钉钉的oauth2代理&lt;/a>作为企业统一登录方式，对于上面两大类系统的认证需求解决方案分别如下，&lt;/p>
&lt;ol>
&lt;li>部署在&lt;a href="https://kubernetes.io/">Kubernetes&lt;/a>中无内置认证机制的Web应用，通过&lt;a href="https://kubernetes.github.io/ingress-nginx/">nginx-ingress&lt;/a>的&lt;a href="https://kubernetes.github.io/ingress-nginx/examples/auth/oauth-external-auth/">外部OAUTH认证&lt;/a>实现基于oauth2的安全认证。&lt;/li>
&lt;li>&lt;a href="https://jenkins.io/">Jenkins&lt;/a>可以通过&lt;a href="https://plugins.jenkins.io/reverse-proxy-auth-plugin">反向代理插件&lt;/a>实现使用oauth2认证登录。&lt;/li>
&lt;/ol>
&lt;p>在&lt;a href="https://kane.mx/posts/effective-cloud-computing/oauth2-proxy-on-kubernetes/part2/">下篇&lt;/a>中，我们将图文详解如何一步步实现为一个无认证的企业文档Web应用添加基于&lt;a href="https://open-doc.dingtalk.com/microapp/serverapi2/vt6khw">钉钉的统一认证&lt;/a>。&lt;/p></description></item><item><title>不要自建Kubernetes</title><link>https://kane.mx/posts/effective-cloud-computing/using-kubernetes-on-cloud/</link><pubDate>Tue, 22 Jan 2019 00:00:00 +0000</pubDate><guid>https://kane.mx/posts/effective-cloud-computing/using-kubernetes-on-cloud/</guid><description>
&lt;p>这是“如何高效使用云服务”系列文章的首篇分享。可能有朋友好奇为什么不是从云计算最基础的服务--计算资源&lt;a href="https://cn.aliyun.com/product/ecs">ECS&lt;/a>/&lt;a href="https://aws.amazon.com/cn/ec2/">EC2&lt;/a>讲起呢？在&lt;a href="https://pivotal.io/cloud-native">Cloud Native&lt;/a>已经被越来越接受的今天，基于&lt;a href="https://kubernetes.io/">Kubernetes&lt;/a>部署、编排应用的方式已经是业界的事实标准。无论是互联网巨头，传统500强企业，还是创业团队都在使用或规划使用&lt;a href="https://kubernetes.io/">Kubernetes&lt;/a>作为应用程序的自动化部署、可扩展管理平台。在云计算平台，虚拟机越来越不需要单独的管理，在绝大多数的业务场景下，它们只是作为容器集群所管理的计算资源。甚至虚拟机的创建到销毁整个生命周期管理都可以由&lt;a href="https://kubernetes.io/">Kubernetes&lt;/a>根据集群的负载来自动完成。&lt;/p>
&lt;p>所有主流的云计算厂商都在解决方案中力推托管的&lt;a href="https://kubernetes.io/">Kubernetes&lt;/a>，&lt;a href="https://aws.amazon.com/cn/">AWS&lt;/a>的&lt;a href="https://aws.amazon.com/eks">EKS&lt;/a>，&lt;a href="https://azure.microsoft.com/en-us/">Azure&lt;/a>上的&lt;a href="https://azure.microsoft.com/en-us/services/kubernetes-service/">AKS&lt;/a>，当然少不了Google家&lt;a href="https://cloud.google.com/">GCP&lt;/a>上的&lt;a href="https://cloud.google.com/kubernetes-engine/">Kubernetes Engine&lt;/a>。国内&lt;a href="https://www.aliyun.com/product/kubernetes">阿里云&lt;/a>，&lt;a href="https://cloud.tencent.com/product/tke">腾讯云&lt;/a>等每一个公有云玩家也都基于开源&lt;a href="https://kubernetes.io/">Kubernetes&lt;/a>推出了托管服务。如果一家云计算厂商在提供托管&lt;a href="https://kubernetes.io/">Kubernetes&lt;/a>这一服务上没跟上业界的步伐，将来极大可能被淘汰出这个市场。&lt;/p>
&lt;h2 id="托管的kubernetes类型">托管的Kubernetes类型&lt;/h2>
&lt;p>以国内的阿里云为例，目前提供了两大类三种不同的&lt;a href="https://help.aliyun.com/document_detail/86737.html">Kubernetes托管服务&lt;/a>。&lt;/p>
&lt;ul>
&lt;li>经典Dedicated Kubernetes模式。这种模式下用户可以选择宿主机实例规格和操作系统，指定Kubernetes版本、自定义Kubernetes特性开关设置等。用户需要手动维护集群，例如升级Kubernetes版本，内置组件版本等。可以手动或自动伸缩集群节点数目。目前该模式下有两种类型，第一种集群主节点需要使用用户的ECS，用户可远程登录或管理这些ECS。另一种是，主节点也由云厂商托管，用户只能通过API Server管理Kubernetes。在费用方面，无论是否托管集群主节点，集群服务免费，按使用的ECS实例及计费方式收费。&lt;/li>
&lt;li>Serverless 模式(目前公测中，暂时免费)。无需创建底层虚拟化资源，可以利用 Kubernetes 命令指明应用容器镜像、CPU和内存要求以及对外服务方式，直接启动应用程序。按容器使用的CPU和内存资源量计费。这种模式下应该是在一个集群内实现多租户，目前有些&lt;a href="https://help.aliyun.com/document_detail/86371.html">features不被支持&lt;/a>。例如，部署不支持DaemonSet，Ingress不支持NodePort类型，存储不支持PV和PVC等。&lt;/li>
&lt;/ul>
&lt;p>用户可以根据自己的业务类型来选择适合的托管Kubernetes集群。如果部署的应用是&lt;a href="https://kubernetes.io/docs/tutorials/stateless-application/">无状态的Web服务&lt;/a>，可以选择Serverless Kubernetes集群，进一步减少运维工作量。&lt;/p>
&lt;p>如果用户部署的应用有状态，需要挂载外部存储，例如MongDB集群，MQ集群，可以选择经典Dedicated Kubernetes模式。如果用户需要通过Kubernetes组件扩展或自定义实现某些功能，这些需求云厂商的标准版并没有提供，这时可以选择经典Dedicated Kubernetes模式，利用Kubernetes高度灵活的扩展机制来满足自定义需求。&lt;/p>
&lt;h2 id="托管kuberentes的优势">托管Kuberentes的优势&lt;/h2>
&lt;p>国内的阿里云有篇技术文档对比&lt;a href="https://help.aliyun.com/document_detail/69575.html">阿里云Kubernetes vs. 自建Kubernetes&lt;/a>，文章看起来虽然有厂商自卖自夸的嫌疑。作为&lt;a href="https://www.aliyun.com/product/kubernetes">阿里云K8S&lt;/a>的客户，在使用托管K8S近一年来，深切的体会到云厂商托管K8S带来的种种好处，文档中提到的种种优势确实是言之凿凿。&lt;/p>
&lt;p>接下来具体看看云厂商托管K8S到底有哪些优势。&lt;/p>
&lt;h3 id="便捷">便捷&lt;/h3>
&lt;ul>
&lt;li>通过Web界面/API一键创建Kubernetes集群，集群升级。&lt;/li>
&lt;li>Web界面/API实现集群的扩容或缩容。&lt;/li>
&lt;/ul>
&lt;p>集群的安装，补丁以及常规版本升级在运维工作中属于体力活。在规模不大的时候，使用人工实现需要花费不少时间准备环境测试验证，且易错。如果集群体量不够大的话，开发自动化运维脚本又浪费人力成本。云计算厂商的托管K8S集群将提供专业、稳定的技术运维服务，和几乎为零的人力成本。&lt;/p>
&lt;p>从效率和人力成本上看，&lt;strong>托管K8S集群完胜自建Kubernetes集群&lt;/strong>。&lt;/p>
&lt;h3 id="功能更强大">功能更强大&lt;/h3>
&lt;p>&lt;a href="https://kubernetes.io/">Kubernetes&lt;/a>作为一个容器编排系统，开源版本中许多组件没有默认实现或实现有限，需要跟运行环境(如托管K8S的云平台)集成。例如，存储，Load Balancer，网络等核心组件。官方文档&lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer">Internal load balancer&lt;/a>就提供了在不同的云厂商环境中的使用示例。部署一个强大且完整的K8S集群需要同许多云计算的基础组件集成(且只能通过API完成)，这往往是云计算厂商的强项。&lt;/p>
&lt;p>云厂商托管的K8S可以在以下方面提供强大的云计算平台支持，&lt;/p>
&lt;h4 id="网络">网络&lt;/h4>
&lt;ul>
&lt;li>高性能 VPC 网络插件。&lt;/li>
&lt;li>支持 network policy 和流控。&lt;/li>
&lt;/ul>
&lt;h4 id="负载均衡">负载均衡&lt;/h4>
&lt;p>支持创建公网或内网负载均衡实例，或者复用已有实例。支持指定带宽大小、计费方式、4层或7层协议代理等云厂商负载均衡功能。对应用运维来说可以把负载均衡的配置通过代码实现，并且支持版本控制。对比传统的云端部署，也可以将应用部署和应用运维集成在一起统一管理，避免应用发布和运维配置的割裂，减少人为运维失误。&lt;/p>
&lt;p>阿里云托管K8S的负载均衡详细配置可以参考这个&lt;a href="https://help.aliyun.com/document_detail/53759.html?spm=a2c4g.11186623.2.15.73364c07mR8rhS#h2-url-4">文档&lt;/a>，AWS上见此&lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/load-balancing.html">文档&lt;/a>。&lt;/p>
&lt;h4 id="存储">存储&lt;/h4>
&lt;p>集成了云厂商的云盘、文件存储NAS、块存储等存储方案，基于标准的&lt;a href="https://github.com/kubernetes/community/blob/master/contributors/devel/flexvolume.md">FlexVolume&lt;/a>驱动，提供了最佳的无缝集成。&lt;/p>
&lt;p>如果是在云厂商的虚拟机上自建&lt;a href="https://kubernetes.io/">Kubernetes&lt;/a>集群，默认无法使用云上的存储资源。如果需要利用云厂商提供的存储方案，例如对象存储，就需要自行开发基于&lt;a href="https://github.com/kubernetes/community/blob/master/contributors/devel/flexvolume.md">FlexVolume&lt;/a>的驱动。在厂商托管K8S已经完美解决了存储集成的问题，何必自己又去费时费力的定制开发呢？&lt;/p>
&lt;p>可以看到，云厂商托管的K8S集群在网络、负载均衡和存储上有许多天然的优势。在其他几个维度，托管的K8S集群同样也优于自建的K8S，&lt;/p>
&lt;h4 id="运维">运维&lt;/h4>
&lt;ul>
&lt;li>集成厂商的日志服务，监控服务。&lt;/li>
&lt;li>K8S集群cluster autoscaler自动利用云厂商的弹性伸缩扩缩容集群节点。&lt;/li>
&lt;/ul>
&lt;h4 id="镜像仓库">镜像仓库&lt;/h4>
&lt;ul>
&lt;li>高可用，支持大并发。&lt;/li>
&lt;li>支持镜像加速。&lt;/li>
&lt;li>支持 p2p 分发。&lt;/li>
&lt;li>可集成云平台的用户权限。&lt;/li>
&lt;li>部分厂商目前免费且不限容量。&lt;/li>
&lt;/ul>
&lt;h4 id="高可用">高可用&lt;/h4>
&lt;ul>
&lt;li>提供多可用区支持。&lt;/li>
&lt;li>支持备份和容灾。&lt;/li>
&lt;/ul>
&lt;h4 id="技术支持">技术支持&lt;/h4>
&lt;ul>
&lt;li>专门的技术团队保障容器的稳定性。&lt;/li>
&lt;li>每个 Linux 版本，每个 Kubernetes 版本都会在经过严格测试之后之后才会提供给用户。&lt;/li>
&lt;li>提供 Kubernetes 升级能力，新版本一键升级。&lt;/li>
&lt;li>为开源软件提供兜底，无论是K8S、Docker甚至Linux自身的问题提供支持。&lt;/li>
&lt;/ul>
&lt;p>专业的技术团队是提供稳定K8S服务必不可少的。但绝大多数企业是无法做到有专业的技术团队来维护K8S、提供K8S或容器技术自身的各种最佳实践、发现以及修复开源软件Bug。&lt;/p>
&lt;p>在笔者的使用托管K8S的时候就遇到这样的状况。其中一个集群升级到新版本&lt;a href="https://kubernetes.io/">Kubernetes&lt;/a>后，内置DNS组件从&lt;a href="https://github.com/kubernetes/dns">KubeDNS&lt;/a>被替换为全新的&lt;a href="https://coredns.io/">CoreDNS&lt;/a>，而当时的&lt;a href="https://coredns.io/">CoreDNS&lt;/a>版本在&lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/#externalname">Service ExternalName&lt;/a>支持上有Bug，导致已有的这种Service无法提供服务。在同云厂商的技术团队沟通后，先用workaround将问题快速绕过，不影响业务的使用。同时，云厂商的技术人员（也是K8S社区committer）继续调研，发现该问题是&lt;a href="https://coredns.io/">CoreDNS&lt;/a>的Bug。在为开源&lt;a href="https://coredns.io/">CoreDNS&lt;/a>项目创建Issue后，同时提供Patch，又在CoreDNS committer建议下完善了测试用例，推动了该问题快速在CoreDNS中被修复。CoreDNS包含Fix的版本发布后，云厂商技术支持团队将更完美的解决方案提供给了我们。作为K8S服务的用户，这种体验是极好的。当时我们的技术团队既没有精力也没有能力快速发现并修复开源软件中的这类问题，而云厂商的服务间接帮我们实现了这种能力。&lt;/p>
&lt;p>&lt;strong>这其实是一种非常好的共赢商业模式，云厂商有能力且有动力投入顶尖技术团队将开源技术商业化，云厂商的用户则用最小的代价获得了最优的基础服务来为核心业务赋能。&lt;/strong>&lt;/p></description></item></channel></rss>