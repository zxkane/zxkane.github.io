[{"body":"","link":"https://kane.mx/tags/ai-agents/","section":"tags","tags":null,"title":"AI Agents"},{"body":"","link":"https://kane.mx/categories/ai-development/","section":"categories","tags":null,"title":"AI Development"},{"body":"","link":"https://kane.mx/tags/amazon-quick-suite/","section":"tags","tags":null,"title":"Amazon Quick Suite"},{"body":"Introduction Business intelligence has long been the domain of specialists, requiring complex tools and time-consuming analysis. But what if you could simply ask your data questions in plain English and receive comprehensive, visualized answers in seconds? What if you could automate your weekly reporting with a simple conversation?\nAmazon Quick Suite is AWS's answer. It's a generative AI-powered business intelligence platform designed to democratize data analysis, making it accessible to everyone in your organization‚Äîno machine learning expertise required.\nIn this deep dive, we'll explore the game-changing benefits of Quick Suite and share five essential best practices for building production-ready data analysis agents. Whether you're new to AWS or an experienced developer, this guide will help you avoid common pitfalls and accelerate your path to AI-driven insights.\nWhat is Amazon Quick Suite? Amazon Quick Suite is a comprehensive business intelligence platform that combines five powerful components:\nQuick Index: A unified knowledge base that consolidates documents, files, and application data to power AI-driven insights. It creates a secure, searchable repository and automatically indexes unstructured data. Quick Research: A powerful agent that conducts research across enterprise data and external sources to deliver contextual, actionable insights in minutes. It breaks down complex questions, gathers information, and validates findings with citations. Quick Sight: An AI-powered BI solution that transforms data into insights through natural language queries and interactive visualizations. It enables users to build dashboards, perform what-if analysis, and respond with one-click actions. Quick Flows: A tool for automating repetitive tasks using natural language. It fetches information, takes action in business applications, generates content, and handles process-specific requirements. Quick Automate: A solution for enterprise-scale process automation that transforms complex business processes into multi-agent workflows, complete with advanced orchestration, governance, and observability. The platform serves three user personas:\nReaders: Access dashboards, run automations, and consume insights. Authors: Build datasets, create agents, and design workflows. Administrators: Manage permissions, monitor costs, and maintain data sources. What truly sets Quick Suite apart is its conversational AI interface. Instead of wrestling with complex query languages, you can ask questions and get intelligent, context-aware responses backed by your organization's data.\nFour Powerful Benefits of Amazon Quick Suite 1. Custom Chat Agents with Your Data Create AI agents that understand your business domain and answer questions using your organization's private data‚Äîno coding required.\nKey Features:\nUpload company documentation, policies, and procedures as agent knowledge. Connect to multiple data sources: databases (RDS, PostgreSQL), SaaS apps (Salesforce, Jira), data warehouses (Redshift), and spreadsheets. Ask questions in plain English and receive answers with auto-generated visualizations. Export results as reports, dashboards, or data files. Example in Action:\n1You: \u0026#34;Show me our top 10 products by revenue this quarter 2 compared to last quarter, broken down by region.\u0026#34; 3 4Agent: [Queries database, performs analysis, generates comparative charts] 5 6You: \u0026#34;Why did the Northeast region decline?\u0026#34; 7 8Agent: [Analyzes details, identifies patterns, visualizes root causes] The agent maintains conversational context, understands your business logic, and automatically chooses the right visualizations, turning complex data analysis into a natural dialogue.\nExample: An Amazon Quick Suite agent analyzing business data with natural language queries.\n2. Simple Workflow Automation with Quick Flows Automate repetitive tasks by describing them in natural language.\nHow It Works: Simply describe your workflow, and Quick Flows builds it for you.\n1You describe: \u0026#34;Every Monday morning, check last week\u0026#39;s sales data, 2 generate a summary report, and email it to leadership.\u0026#34; 3 4Quick Flows creates: A complete, automated workflow that: 5‚Üí Connects to the sales database 6‚Üí Filters the previous week\u0026#39;s data 7‚Üí Calculates key metrics 8‚Üí Generates a formatted report 9‚Üí Emails it to the specified recipients What You Can Automate:\nData Gathering: Fetch information from databases, applications, and external APIs. Notifications: Send updates via email, Slack, or Microsoft Teams. Content Generation: Create brand-compliant reports and documentation. Approval Routing: Handle conditional logic and multi-step approvals. Scheduled Tasks: Run daily, weekly, or monthly processes automatically. With over 50 pre-built connectors for services like Slack, Jira, and Salesforce, Quick Flows streamlines your operations.\n3. Enterprise-Scale Automation with Quick Automate For complex business processes requiring sophisticated orchestration and governance, Quick Automate provides enterprise-grade automation powered by Amazon Bedrock agents.\nKey Differentiators:\nMulti-Agent Orchestration: Coordinate specialized agents for research, analysis, and execution. Dynamic AI Planning: Break down complex tasks and adapt based on real-time results. Human-in-the-Loop: Pause workflows for human approval at critical decision points. Enterprise Governance: Enforce role-based access, audit logging, and compliance tracking. Full Observability: Monitor performance with real-time metrics and execution history. When to Use Quick Flows vs. Quick Automate:\nQuick Flows: Ideal for simple tasks, notifications, and basic data workflows. Quick Automate: Best for mission-critical processes, multi-system orchestration, and regulated compliance workflows. 4. Seamless Collaboration and Sharing Share insights, agents, and workflows across your organization while maintaining enterprise-grade security.\nShareable Assets:\nDashboards: Share or embed visualizations with granular permissions and row-level security. AI Agents: Package custom agents as reusable organizational assets. Workflows: Distribute Quick Flows templates across teams with version control. Datasets: Publish curated datasets for governed, self-service analysis. Enterprise Security:\nSingle Sign-On (SSO) integration with your identity provider. Data encryption at rest and in transit. Complete audit logging for compliance (GDPR, HIPAA, SOC 2). Row-level security to ensure users only see authorized data. 5 Best Practices for Production-Ready BI Agents These five practices are critical for building reliable and accurate data analysis agents.\nPractice 1: Preview Every Transformation Step Amazon Quick Suite's visual data preparation interface lets you transform data without SQL, but the key to success is previewing after every single change. One bad transformation can corrupt everything downstream.\nThe Visual Workflow: Amazon Quick Suite visualizes your transformations as a sequential pipeline, where each step is editable.\nflowchart LR A[Import Data] --\u0026gt; P1{Preview} P1 --\u0026gt; B[Filter Rows] --\u0026gt; P2{Preview} P2 --\u0026gt; C[Cast Types] --\u0026gt; P3{Preview} P3 --\u0026gt; D[Create Columns] --\u0026gt; P4{Preview} P4 --\u0026gt; E[Aggregate] --\u0026gt; P5{Preview} P5 --\u0026gt; F[Join Tables] --\u0026gt; P6{Preview} P6 --\u0026gt; G[Publish Dataset] style P1 fill:#60a5fa,stroke:#3b82f6,color:#fff style P2 fill:#60a5fa,stroke:#3b82f6,color:#fff style P3 fill:#60a5fa,stroke:#3b82f6,color:#fff style P4 fill:#60a5fa,stroke:#3b82f6,color:#fff style P5 fill:#60a5fa,stroke:#3b82f6,color:#fff style P6 fill:#60a5fa,stroke:#3b82f6,color:#fff style G fill:#10b981,stroke:#059669,color:#fff Example:\n1Step 1: Import sales_data.csv ‚Üí Preview shows 10,000 rows. ‚úì 2Step 2: Filter out test transactions ‚Üí Preview shows 9,847 rows. ‚úì 3Step 3: Cast \u0026#34;sale_date\u0026#34; from String to Date ‚Üí Preview shows proper dates. ‚úì 4Step 4: Cast \u0026#34;amount\u0026#34; from String to Decimal ‚Üí Preview shows 12 nulls. ‚ö†Ô∏è 5 ‚Üí Investigation: Found \u0026#34;$1,234.56\u0026#34; format needs cleaning. 6 ‚Üí Add step: Remove \u0026#34;$\u0026#34; and \u0026#34;,\u0026#34; before casting. 7 ‚Üí Preview again: 0 nulls. ‚úì This iterative validation is crucial for building reliable datasets.\nPractice 2: Handle DateTime Conversion Carefully When datetime strings don't match supported formats, Amazon Quick Suite doesn't fail‚Äîit silently converts them to null. This can lead to incomplete results that are difficult to detect.\nThe Problem:\n1Your data: \u0026#34;2025.10.29\u0026#34; (dot separator - not supported) 2After cast to Date: NULL 3Import status: ‚úì Success (no error!) 4Query result: Missing dates, inaccurate metrics. Supported Formats (AWS docs):\nISO 8601 (Recommended): yyyy-MM-dd'T'HH:mm:ss.SSSZ Common US Formats: MM/dd/yyyy HH:mm:ss and MM-dd-yyyy ‚ö†Ô∏è Important: Always use a 4-digit year (yyyy). Two-digit years (yy) are not supported and will result in nulls.\nPrevention: Always preview your data after a datetime conversion and check for unexpected nulls.\nPractice 3: Sanitize Monetary Values Financial data is often formatted with currency symbols and commas (\u0026quot;$1,234.56\u0026quot;), which Amazon Quick Suite interprets as strings. Attempts to perform calculations on these fields will fail or produce incorrect results.\nThe Solution: Clean the data either before import or during dataset preparation in Amazon Quick Suite.\nCreate a calculated field to remove symbols and commas: 1parseDecimal(replace(replace(revenue, \u0026#34;$\u0026#34;, \u0026#34;\u0026#34;), \u0026#34;,\u0026#34;, \u0026#34;\u0026#34;)) Preview the conversion to ensure all values are now numeric. Set the field role to \u0026quot;Measure\u0026quot; (see Practice 4). Hide the original string field to avoid confusion. For high-precision financial calculations, use the Decimal-fixed data type to prevent floating-point errors.\nPractice 4: Set Field Roles Explicitly (Measure vs. Dimension) Amazon Quick Suite assigns every field a role: Measure (for aggregation) or Dimension (for grouping). The auto-detection can be wrong, leading to broken analytics.\nMeasure: A field you can aggregate (e.g., SUM, AVG). Examples: revenue, quantity. Dimension: A field you use to group or filter. Examples: product_category, region, customer_id. Common Mistake: A numeric customer_id might be auto-detected as a Measure, leading to meaningless calculations like AVG(customer_id).\nBest Practice: Manually set roles for all fields.\nNumeric IDs (customer_id, zip_code) should be Dimensions. Calculated metrics (profit, conversion_rate) should be Measures. Practice 5: Implement Row-Level Security (RLS) Row-Level Security (RLS) is essential for controlling data access in multi-user environments. It restricts which rows users can see in a dataset. (RLS is an Amazon QuickSight Enterprise Edition feature).\nHow It Works: RLS uses a separate rules dataset to define permissions.\n1Main Dataset (sales_data): Rules Dataset (sales_rls_rules): 2order_id, region, amount UserName, region 3O-1001, East, $5000 alice@company.com, East 4O-1002, West, $8000 bob@company.com, West 5O-1003, East, $3000 alice@company.com, East 6 7When alice queries ‚Üí She only sees \u0026#34;East\u0026#34; region rows. 8When bob queries ‚Üí He only sees \u0026#34;West\u0026#34; region rows. Critical Limitation: RLS rules only work with text fields. To filter by numbers or dates, you must first cast those fields to strings in your rules dataset.\nImplementation Steps:\nCreate a Rules Dataset: A CSV or database table with UserName and the fields to restrict. Upload the Rules: Create a new dataset in Amazon Quick Suite and mark it as containing RLS rules. Apply to Main Dataset: In your main dataset's permissions, apply the rules dataset. Quick Start: Build Your First AI Agent in 7 Steps Let's build a Customer Support Analytics Agent.\nGoal: Analyze support tickets to identify trends, measure team performance, and predict resolution times.\nPrepare Your Data: Start with a support_tickets.csv file containing ticket details. Create \u0026amp; Transform Dataset: In Amazon Quick Suite, create a new dataset and apply transformations. Fix Dates: Use parseDate to convert date strings to the correct format. Clean Money: Use parseDecimal and replace to clean monetary values. Calculate Metrics: Create new fields like resolution_hours using dateDiff. Set Roles: Define agent_id as a Dimension and resolution_hours as a Measure. Create a Topic: Make your dataset conversational by linking it to a new Topic and adding custom instructions like \u0026quot;SLA is 24 hours\u0026quot;. Create a Space: Bundle your Topic and related documentation into a Space for your support team. Create the Chat Agent: Create an agent, link it to your Space, and give it a persona, such as \u0026quot;You are a support analytics assistant.\u0026quot; Test with Real Questions: Validate the agent with progressively complex queries, from simple counts to comparative analyses. Share with Your Team: Deploy the agent, space, and topic for your organization to use. Conclusion Amazon Quick Suite represents a fundamental shift in business intelligence. By combining generative AI with a comprehensive BI platform, it empowers everyone in an organization to interact with data conversationally.\nThe key to success lies in a disciplined approach. By following the five best practices‚Äîpreviewing every transformation, handling data types carefully, and implementing robust security‚Äîyou can build reliable, production-ready AI agents that deliver real business value.\nThe future of BI is conversational, intelligent, and accessible. Amazon Quick Suite brings that future to your organization today.\nResources Amazon Quick Suite Official Documentation Supported Data Types and Formats in QuickSight Row-Level Security (RLS) Guide for QuickSight Have you built AI-powered analytics agents with Amazon Quick Suite? Share your experiences and lessons learned in the comments below!\n","link":"https://kane.mx/posts/2025/amazon-quicksuite-deep-dive/","section":"posts","tags":["Amazon Quick Suite","AWS","QuickSight","Business Intelligence","AI Agents","Data Analysis","Workflow Automation","QuickFlows","QuickAutomate"],"title":"Amazon Quick Suite Deep Dive: Build AI-Powered Business Intelligence on AWS"},{"body":"","link":"https://kane.mx/tags/aws/","section":"tags","tags":null,"title":"AWS"},{"body":"","link":"https://kane.mx/categories/business-intelligence/","section":"categories","tags":null,"title":"Business Intelligence"},{"body":"","link":"https://kane.mx/tags/business-intelligence/","section":"tags","tags":null,"title":"Business Intelligence"},{"body":"","link":"https://kane.mx/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"","link":"https://kane.mx/categories/cloud-computing/","section":"categories","tags":null,"title":"Cloud-Computing"},{"body":"","link":"https://kane.mx/tags/data-analysis/","section":"tags","tags":null,"title":"Data Analysis"},{"body":"","link":"https://kane.mx/posts/","section":"posts","tags":null,"title":"Posts"},{"body":"","link":"https://kane.mx/tags/quickautomate/","section":"tags","tags":null,"title":"QuickAutomate"},{"body":"","link":"https://kane.mx/tags/quickflows/","section":"tags","tags":null,"title":"QuickFlows"},{"body":"","link":"https://kane.mx/tags/quicksight/","section":"tags","tags":null,"title":"QuickSight"},{"body":"","link":"https://kane.mx/tags/","section":"tags","tags":null,"title":"Tags"},{"body":"","link":"https://kane.mx/","section":"","tags":null,"title":"The road"},{"body":"","link":"https://kane.mx/tags/workflow-automation/","section":"tags","tags":null,"title":"Workflow Automation"},{"body":"","link":"https://kane.mx/tags/agent-skills/","section":"tags","tags":null,"title":"Agent Skills"},{"body":"","link":"https://kane.mx/tags/api-gateway/","section":"tags","tags":null,"title":"API Gateway"},{"body":"","link":"https://kane.mx/tags/aws-cdk/","section":"tags","tags":null,"title":"AWS CDK"},{"body":"","link":"https://kane.mx/tags/aws-skills/","section":"tags","tags":null,"title":"AWS Skills"},{"body":"Introduction Building on AWS is powerful but complex. What if your AI assistant had deep AWS expertise built-in? That's what AWS Skills brings to Claude Code.\nAWS Skills is a plugin that transforms Claude Code into an intelligent AWS development partner. It understands CDK best practices, estimates costs before you deploy, and guides you through serverless patterns. This post shows you how to build a serverless REST API using Claude Code supercharged with AWS Skills.\nHow It Works: Agent Skills AWS Skills uses Claude Agent Skills, which are modular extensions that give Claude new capabilities. Claude autonomously decides when to use a skill based on your request, allowing it to interact with external tools like the AWS CDK, pricing calculators, and documentation.\nThe AWS Skills Capabilities AWS Skills bundles this power into three plugins:\nAWS CDK Plugin: Brings CDK expertise, best practices, and cdk-nag security checks into your workflow. Cost \u0026amp; Operations Plugin: Provides pre-deployment cost estimates, billing analysis, and CloudWatch monitoring. Serverless \u0026amp; EDA Plugin: Offers patterns for event-driven architectures using EventBridge, Step Functions, and SAM. Building a Serverless REST API Let's build a simple task management REST API to see how AWS Skills improves the development workflow.\nArchitecture We'll build an API with three main components:\nAPI Gateway: Provides the RESTful HTTP endpoint. Lambda Function: Contains the business logic. DynamoDB: Stores the task data. Step 1: Ask Claude to Create the CDK Infrastructure With AWS Skills, you can ask for the infrastructure in plain English.\nYou:\n1/aws-cdk-development Create a CDK stack for a task management API with an API Gateway, a Lambda function(without detailed implementation), and a DynamoDB table. Follow AWS best practices and write in Python. Claude (with AWS Skills):\nThe AWS CDK Plugin helps Claude generate a complete, best-practice CDK stack in TypeScript based on the prompt. It defines the DynamoDB table, the Lambda function, and the API Gateway, including setting up the necessary IAM permissions.\nStep 2: Implement the Lambda Handler You can then ask Claude to generate the Lambda function code with best practices.\nYou:\n1/aws-serverless-eda Now, create the Python Lambda handler for the task API. It should handle POST requests to create a task and GET requests to retrieve one. Claude (with AWS Skills):\nClaude generates the Lambda handler code, including the logic for creating and retrieving tasks from the DynamoDB table using the AWS SDK.\nStep 3: Estimate Costs Before Deployment This is where AWS Skills really shines. Ask for a cost estimate before deploying anything.\nYou:\n1Estimate the monthly cost for this API assuming 1 million requests per month, 100ms average Lambda duration, and 10GB of data in DynamoDB. Claude (with Cost \u0026amp; Operations Plugin):\nClaude uses its cost estimation skill to give you a clear breakdown of the estimated monthly costs for API Gateway, Lambda, and DynamoDB based on your usage estimates. This helps you make informed decisions before deployment.\nStep 4: Deploy with Confidence When you're ready, Claude guides you through deployment.\nYou:\n1Deploy this stack to us-east-1. Claude:\nClaude provides the exact cdk commands to bootstrap your environment (if needed) and deploy the stack. It will also show you the API endpoint URL from the stack outputs once the deployment is complete.\nDevelopment Workflow: Before vs. After AWS Skills streamlines your workflow by keeping you in your IDE, saving time and reducing context switching.\nBefore: The Old Way\nWrite code in your IDE. Switch to a browser to search documentation. Open another tab for the AWS Pricing Calculator. Manually check for best practices. Deploy from the CLI. Debug issues in the AWS Console. After: With AWS Skills\nDescribe your goal in plain language. Claude generates the code and a cost estimate. Review and refine in your IDE. Deploy automatically from Claude. You ship better code faster, all from one place.\nConclusion AWS Skills turns Claude Code into a specialized AWS development partner. It brings real-time AWS knowledge directly into your editor, helping you build faster, more cost-effective, and more reliable applications.\nBy automating best practices, providing pre-deployment cost insights, and guiding you through complex patterns, AWS Skills lets you focus on what matters most: your application's business logic.\nResources AWS Skills GitHub Repository Claude Code Agent Skills Model Context Protocol Ready to build on AWS faster? Install AWS Skills and let me know what you think in the comments!\n","link":"https://kane.mx/posts/2025/aws-skills-claude-code/","section":"posts","tags":["Claude Code","AWS Skills","AWS CDK","Serverless","Lambda","DynamoDB","API Gateway","Agent Skills","MCP Protocol","Infrastructure as Code"],"title":"Build on AWS Faster with Claude Code and AWS Skills"},{"body":"","link":"https://kane.mx/tags/claude-code/","section":"tags","tags":null,"title":"Claude Code"},{"body":"","link":"https://kane.mx/categories/developer-tools/","section":"categories","tags":null,"title":"Developer Tools"},{"body":"","link":"https://kane.mx/tags/dynamodb/","section":"tags","tags":null,"title":"DynamoDB"},{"body":"","link":"https://kane.mx/tags/infrastructure-as-code/","section":"tags","tags":null,"title":"Infrastructure as Code"},{"body":"","link":"https://kane.mx/tags/lambda/","section":"tags","tags":null,"title":"Lambda"},{"body":"","link":"https://kane.mx/tags/mcp-protocol/","section":"tags","tags":null,"title":"MCP Protocol"},{"body":"","link":"https://kane.mx/tags/serverless/","section":"tags","tags":null,"title":"Serverless"},{"body":"","link":"https://kane.mx/tags/agent-framework/","section":"tags","tags":null,"title":"Agent Framework"},{"body":"","link":"https://kane.mx/tags/ai-automation/","section":"tags","tags":null,"title":"AI Automation"},{"body":"","link":"https://kane.mx/categories/automation/","section":"categories","tags":null,"title":"Automation"},{"body":"","link":"https://kane.mx/tags/claude-agent-sdk/","section":"tags","tags":null,"title":"Claude Agent SDK"},{"body":"","link":"https://kane.mx/tags/sdk-migration/","section":"tags","tags":null,"title":"SDK Migration"},{"body":"","link":"https://kane.mx/tags/typescript/","section":"tags","tags":null,"title":"TypeScript"},{"body":"Heads up, builders! You may have noticed that Anthropic has rebranded the Claude Code SDK to the new Claude Agent SDK.\nThis is more than just a new name. As the official announcement explains, this change reflects a strategic focus on making it easier than ever to build, debug, and deploy powerful AI agents.\nIf you've been following our deep dive into building agentic applications, you'll be happy to know that the migration is incredibly straightforward. All the core concepts and powerful features like conversation management, MCP integration, and response streaming are still there.\nHere‚Äôs the TL;DR on how to upgrade your project.\n1. Update Your Dependencies First, uninstall the old package and install the new one.\n1npm uninstall @anthropic-ai/claude-code 2npm install @anthropic-ai/agent-sdk 2. Update Your Imports Next, just find and replace the import statements in your TypeScript files.\nBefore:\n1import { query, type SDKMessage } from \u0026#39;@anthropic-ai/claude-code\u0026#39;; After:\n1import { query, type SDKMessage } from \u0026#39;@anthropic-ai/agent-sdk\u0026#39;; 3. Handle the Breaking Change in query Options This is the most important part of the migration. The new SDK introduces a breaking change in how you provide a system prompt. The appendSystemPrompt option has been replaced with a more structured systemPrompt object.\nHere‚Äôs how to adapt your query calls:\nBefore (claude-code-sdk):\n1const response = query({ 2 prompt: prompt, 3 options: { 4 appendSystemPrompt: systemPrompt, // This is now deprecated 5 mcpServers: mcpServers, 6 // ...other options 7 } 8}); After (claude-agent-sdk):\n1const response = query({ 2 prompt: prompt, 3 options: { 4 systemPrompt: systemPrompt, // Use the new systemPrompt option 5 mcpServers: mcpServers, 6 // ...other options 7 } 8}); In some cases, you might also want to specify the systemPrompt as a preset, like so:\n1const response = query({ 2 prompt: slashCommand, 3 options: { 4 systemPrompt: { type: \u0026#39;preset\u0026#39;, preset: \u0026#39;claude_code\u0026#39; }, 5 // ...other options 6 } 7}); And that's it! With these changes, your existing code, including all your slash commands and MCP configurations, will work exactly as before.\nThe move to the Claude Agent SDK is a clear signal of the agent-first future of AI. By making this small update, you're keeping your projects aligned with the latest and greatest from Anthropic.\nFor a complete guide to the architecture and patterns for building with the SDK, be sure to check out our original, in-depth tutorial.\nHappy building!\n","link":"https://kane.mx/posts/2025/claude-agent-sdk-update/","section":"posts","tags":["Claude Agent SDK","Claude Code","Agent Framework","AI Automation","TypeScript","SDK Migration"],"title":"Upgrade to Claude Agent SDK: A Quick Migration Guide from Claude Code"},{"body":"","link":"https://kane.mx/tags/agentic-ai/","section":"tags","tags":null,"title":"Agentic AI"},{"body":"Introduction If you've been following the AI space, you know that \u0026quot;agents\u0026quot; are the next big thing. These autonomous, intelligent systems promise to revolutionize how we automate complex tasks. But building them can be a daunting undertaking, often involving a tangled web of conversation management, tool integration, and state tracking.\nWhat if I told you there's a framework that handles the heavy lifting, letting you focus on your agent's unique logic? Enter Claude Code. It has evolved beyond a simple CLI tool into a powerful AI application framework for building sophisticated AI applications.\nIn this deep dive and Claude Code tutorial, I'll show you how to leverage Claude Code's SDK and infrastructure to create intelligent agents that can interact with external services, maintain conversations, and execute complex agentic workflows. This guide is based on the social-agents project, a real-world implementation that demonstrates Claude Code's capabilities for multi-platform social media automation. We'll explore the AI agent architecture, patterns, and best practices that make it an excellent choice for your AI agent development.\nWhat Makes Claude Code Special for Agent Development? So what sets Claude Code apart from the crowd of AI frameworks? While many tools leave you wrestling with conversation state, tool integration, and response streaming, Claude Code delivers these as core, out-of-the-box features for building AI agents:\nüß† Intelligent Conversation Management Built-in session tracking and resumption for stateful workflows. Maintains context across multiple interactions. Automatically stores conversation history in a simple JSONL format. üîß Seamless Tool Integration via MCP Natively supports the Model Context Protocol (MCP) for connecting to local or remote tools. Provides standardized tool discovery, execution, and permission management. ‚ö° Effortless Streaming Responses Processes AI responses in real-time with progress tracking. Natively handles different message types (system, assistant, user, tool results). Comes with built-in error handling and recovery mechanisms. üéØ Intuitive Agent Customization Define agent behaviors declaratively using simple Markdown files (.claude/commands/*.md). Easily customize system prompts and agent personalities. Provides a natural language interface for triggering complex operations. Architecture Overview Let's examine how the social-agents project structures a multi-platform agent system. The architecture is designed for modularity and scalability.\n1social-agents/ 2‚îú‚îÄ‚îÄ src/ 3‚îÇ ‚îú‚îÄ‚îÄ social-sdk-executor.ts # Core agent execution engine 4‚îÇ ‚îú‚îÄ‚îÄ env-loader.ts # Environment configuration 5‚îÇ ‚îú‚îÄ‚îÄ logger.ts # Structured logging 6‚îÇ ‚îî‚îÄ‚îÄ types.ts # TypeScript definitions 7‚îú‚îÄ‚îÄ .claude/ 8‚îÇ ‚îî‚îÄ‚îÄ commands/ 9‚îÇ ‚îú‚îÄ‚îÄ twitter.md # Twitter agent configuration 10‚îÇ ‚îú‚îÄ‚îÄ reddit.md # Reddit agent configuration 11‚îÇ ‚îî‚îÄ‚îÄ linkedin.md # LinkedIn agent configuration 12‚îú‚îÄ‚îÄ twitter.ts # Twitter command interface 13‚îú‚îÄ‚îÄ reddit.ts # Reddit command interface 14‚îú‚îÄ‚îÄ linkedin.ts # LinkedIn command interface 15‚îú‚îÄ‚îÄ .mcp.json # MCP server configuration 16‚îî‚îÄ‚îÄ package.json # Scripts and dependencies This architecture demonstrates several key patterns for building agentic applications:\nGeneric Executor Pattern: A single SocialSDKExecutor handles the core logic for all platforms. Platform-Specific Commands: Individual TypeScript files create dedicated command-line interfaces for each social media platform. Declarative Agent Configuration: Markdown files in the .claude/commands/ directory define each agent's unique behavior, tools, and prompts. Standardized MCP Integration: External services are connected through a single, unified protocol. Here‚Äôs a look at the interaction flow:\nsequenceDiagram participant User as User participant CLI as Platform CLI (e.g., twitter.ts) participant Executor as SocialSDKExecutor participant ClaudeCode as Claude Code SDK participant MCP as MCP Server (Rube) participant SocialAPI as Social Media API (e.g., Twitter) User-\u0026gt;\u0026gt;+CLI: npm run twitter -- \u0026#34;post a tweet\u0026#34; CLI-\u0026gt;\u0026gt;+Executor: execute(\u0026#39;twitter\u0026#39;, \u0026#39;post a tweet\u0026#39;, options) Executor-\u0026gt;\u0026gt;+ClaudeCode: query({ prompt: \u0026#39;/twitter post a tweet\u0026#39; }) ClaudeCode-\u0026gt;\u0026gt;ClaudeCode: Load .claude/commands/twitter.md ClaudeCode-\u0026gt;\u0026gt;+MCP: list_tools() MCP--\u0026gt;\u0026gt;-ClaudeCode: Available tools ClaudeCode-\u0026gt;\u0026gt;+MCP: call_tool(\u0026#39;post_tweet\u0026#39;, {text: \u0026#39;...\u0026#39;}) MCP-\u0026gt;\u0026gt;+SocialAPI: POST /2/tweets SocialAPI--\u0026gt;\u0026gt;-MCP: Tweet created MCP--\u0026gt;\u0026gt;-ClaudeCode: Tool result ClaudeCode--\u0026gt;\u0026gt;-Executor: Streaming messages (assistant, result) Executor--\u0026gt;\u0026gt;-CLI: Process and log messages CLI--\u0026gt;\u0026gt;-User: Display output Core Implementation: The Agent Executor At the heart of our agent is the executor. This is the engine that drives the AI interaction, orchestrating everything from loading configurations to processing streaming responses. Let's break down how it works.\nHere's a simplified version of the implementation:\n1import { query, type SDKMessage, type McpServerConfig } from \u0026#39;@anthropic-ai/claude-code\u0026#39;; 2 3export class SocialSDKExecutor { 4 static async execute(platform: string, prompt: string, options: SocialOptions): Promise\u0026lt;void\u0026gt; { 5 // Load MCP server configuration 6 const mcpServers = this.loadMCPServers(); 7 8 // Build the slash command with platform and options 9 const slashCommand = `/${platform} ${prompt} ${options.dryRun ? \u0026#39;--dry-run\u0026#39; : \u0026#39;\u0026#39;}`.trim(); 10 11 // Execute the command using the Claude Code SDK 12 const response = query({ 13 prompt: slashCommand, 14 options: { 15 mcpServers: mcpServers, 16 cwd: process.cwd(), 17 ...(options.resume \u0026amp;\u0026amp; { resume: options.resume }) 18 } 19 }); 20 21 // Process the streaming responses 22 for await (const message of response) { 23 await this.processMessage(message, options.verbose); 24 } 25 } 26 27 private static async processMessage(message: SDKMessage, verbose: boolean): Promise\u0026lt;void\u0026gt; { 28 switch (message.type) { 29 case \u0026#39;assistant\u0026#39;: 30 // Handle AI responses 31 console.log(message.message.content); 32 break; 33 case \u0026#39;result\u0026#39;: 34 // Handle execution results 35 if (message.subtype === \u0026#39;success\u0026#39;) { 36 console.log(\u0026#39;Operation completed successfully!\u0026#39;); 37 } 38 break; 39 case \u0026#39;system\u0026#39;: 40 // Handle system messages (e.g., model info, MCP status) 41 if (verbose) { 42 console.log(`System: ${message.model}`); 43 } 44 break; 45 } 46 } 47} Slash Command Configuration The real power of this framework comes from configuring agent behaviors through simple Markdown files. These \u0026quot;slash commands\u0026quot; are the brains of the operation, defining what an agent is and what it can do.\nHere‚Äôs how the Twitter agent is defined in .claude/commands/twitter.md:\n1--- 2allowed-tools: mcp__rube__RUBE_SEARCH_TOOLS, mcp__rube__RUBE_MULTI_EXECUTE_TOOL, mcp__rube__RUBE_CREATE_PLAN 3description: Twitter/X engagement and content creation specialist 4argument-hint: [natural language request] [--dry-run] [--verbose] 5--- 6 7You are an expert Twitter operations specialist with comprehensive AI-driven capabilities. 8 9AVAILABLE OPERATIONS (when RUBE tools are accessible): 10‚Ä¢ Generate viral tweets, threads, and engaging content 11‚Ä¢ Search and analyze Twitter posts, trends, and conversations 12‚Ä¢ Engage with tweets through likes, retweets, and replies 13‚Ä¢ Monitor topics, hashtags, and mentions for social listening 14‚Ä¢ Analyze sentiment, engagement metrics, and performance data 15 16EXECUTION APPROACH: 171. Try to use RUBE_SEARCH_TOOLS to discover available Twitter tools. 182. If permission is needed, explain the requirement clearly. 193. Execute operations using RUBE_MULTI_EXECUTE_TOOL. 204. Provide detailed results, insights, and recommendations. 21 22Execute the following Twitter operation: $ARGUMENTS This configuration file tells Claude Code:\nTools: Which MCP tools the agent is permitted to use. Persona: The agent's designated role and capabilities. Logic: How to handle different scenarios, such as permissions or failures. Task: The specific operation to execute, passing in the user's arguments. MCP Integration for External Services The Model Context Protocol (MCP) enables seamless integration with external services. Instead of writing custom API clients for every service, you just point to an MCP server. The configuration is straightforward in .mcp.json:\n1{ 2 \u0026#34;mcpServers\u0026#34;: { 3 \u0026#34;rube\u0026#34;: { 4 \u0026#34;type\u0026#34;: \u0026#34;http\u0026#34;, 5 \u0026#34;url\u0026#34;: \u0026#34;https://rube.app/mcp\u0026#34;, 6 \u0026#34;headers\u0026#34;: { 7 \u0026#34;Authorization\u0026#34;: \u0026#34;Bearer ${RUBE_API_TOKEN}\u0026#34; 8 } 9 } 10 } 11} In this project, we connect to RUBE, an MCP server that provides a massive library of pre-built tool integrations. This instantly gives our agent access to:\nTwitter/X API operations Reddit API integration LinkedIn automation Over 500 other applications A unified way to discover and execute tools. Platform-Specific Command Interfaces To make the agents easy to use from the command line, each platform gets its own wrapper script. This provides a clean, dedicated entry point for each agent.\n1// twitter.ts 2import { SocialSDKExecutor, type SocialOptions } from \u0026#39;./src/social-sdk-executor.js\u0026#39;; 3 4async function main() { 5 const args = process.argv.slice(2); 6 7 // Parse command-line options 8 const options: SocialOptions = { 9 dryRun: args.includes(\u0026#39;--dry-run\u0026#39;), 10 verbose: args.includes(\u0026#39;--verbose\u0026#39;), 11 resume: extractResumeId(args) 12 }; 13 14 // Extract the natural language prompt 15 const prompt = args 16 .filter(arg =\u0026gt; !arg.startsWith(\u0026#39;--\u0026#39;)) 17 .join(\u0026#39; \u0026#39;); 18 19 // Execute with the generic executor, specifying the \u0026#39;twitter\u0026#39; platform 20 await SocialSDKExecutor.execute(\u0026#39;twitter\u0026#39;, prompt, options); 21} 22 23main().catch(console.error); This approach provides:\nA Consistent Interface: The same patterns are used across all platforms. Platform Flexibility: It's easy to add new platforms by creating a new command file. Command-Line Integration: It uses standard Unix-style flags and arguments. Type Safety: It offers full TypeScript support with proper error handling. Session Management and Conversation Continuity One of the most frustrating parts of building chatbots is managing conversation history. Claude Code turns this into a superpower with its built-in session management. You don't have to do anything; it just works.\n1# Start a new conversation 2npm run twitter -- \u0026#34;create viral content about TypeScript\u0026#34; 3# Output: üìå Session ID: 77552924-a31c-4c1a-a07c-990855aa95a3 4 5# Resume and continue the conversation 6npm run twitter -- \u0026#34;now create a follow-up thread\u0026#34; --resume 77552924-a31c-4c1a-a07c-990855aa95a3 7 8# Keep iterating within the same context 9npm run twitter -- \u0026#34;make it more technical\u0026#34; --resume 77552924-a31c-4c1a-a07c-990855aa95a3 Sessions are automatically stored locally in ~/.claude/projects/ as JSONL transcripts, enabling:\nStateful Workflows: Maintain context across multiple interactions. Conversation History: Review previous exchanges and decisions. Debugging: Trace the execution flow to identify issues. Collaboration: Share session IDs with team members to pick up where you left off. Error Handling and Fallback Strategies Robust agent applications require comprehensive error handling. The social-agents project demonstrates several useful patterns for building resilient agents.\nPermission Management The agent can detect when it needs permissions for a tool and inform the user.\n1// Handle MCP tool permission requirements 2if (!toolsAccessible) { 3 logger.info(\u0026#39;üîë Permission Required for RUBE MCP Server\u0026#39;); 4 logger.info(\u0026#39;Please grant permission when prompted to enable operations.\u0026#39;); 5 return; 6} Graceful Degradation If a tool fails or isn't available, the agent can fall back to a different mode of operation, such as providing strategic advice instead of executing a task.\n1// Provide strategic guidance when tools aren\u0026#39;t available 2if (message.type === \u0026#39;result\u0026#39; \u0026amp;\u0026amp; message.subtype === \u0026#39;error\u0026#39;) { 3 logger.warning(\u0026#39;Tools not accessible - providing strategic guidance instead.\u0026#39;); 4 // Continue with educational/planning mode 5} Advanced Patterns and Best Practices As you build more complex agents, you'll find these patterns from the social-agents project invaluable.\nPrioritized Environment Configuration The project uses a sophisticated system for loading environment variables, ensuring that local overrides are respected while maintaining sensible defaults.\n1export function loadEnvironment(): EnvironmentConfig { 2 // Priority: .env.local ‚Üí system environment variables 3 const localEnvPath = path.join(process.cwd(), \u0026#39;.env.local\u0026#39;); 4 5 // Load and merge configurations 6 const env = { 7 ...process.env, 8 ...loadEnvFile(localEnvPath) // .env.local has the highest priority 9 }; 10 11 return validateEnvironment(env); 12} Streaming Response Processing Handle different message types as they arrive to provide a rich, real-time user experience.\n1for await (const message of response) { 2 switch (message.type) { 3 case \u0026#39;assistant\u0026#39;: 4 // Stream AI responses in real-time 5 process.stdout.write(message.message.content); 6 break; 7 case \u0026#39;system\u0026#39;: 8 // Capture session IDs and server status 9 if (message.session_id) { 10 sessionId = message.session_id; 11 } 12 break; 13 case \u0026#39;result\u0026#39;: 14 // Handle final outcomes of tool executions 15 displayResults(message); 16 break; 17 } 18} Type Safety with Zod Validation Ensure runtime type safety for configurations and options using Zod.\n1import { z } from \u0026#39;zod\u0026#39;; 2 3const SocialOptionsSchema = z.object({ 4 dryRun: z.boolean(), 5 verbose: z.boolean(), 6 resume: z.string().optional() 7}); 8 9export type SocialOptions = z.infer\u0026lt;typeof SocialOptionsSchema\u0026gt;; Building Your Own Agent Application Ready to build your own autonomous agent? Here's a quick-start guide to get you up and running in minutes.\n1. Project Setup 1npm init -y 2npm install @anthropic-ai/claude-code tsx typescript @types/node 2. Create the Core Executor Create a generic executor to handle the agent's core logic.\n1// src/agent-executor.ts 2import { query } from \u0026#39;@anthropic-ai/claude-code\u0026#39;; 3 4export class AgentExecutor { 5 static async execute(agentType: string, prompt: string, options: AgentOptions) { 6 const response = query({ 7 prompt: `/${agentType} ${prompt}`, 8 options: { 9 mcpServers: await this.loadMCPServers(), 10 cwd: process.cwd() 11 } 12 }); 13 14 for await (const message of response) { 15 // Your message processing logic here 16 } 17 } 18} 3. Configure Agent Behaviors Create a file at .claude/commands/my-agent.md to define your agent's persona and tools.\n1--- 2allowed-tools: your_mcp_tools_here 3description: A short description of your agent 4--- 5 6You are an expert assistant for [your domain]. 7 8Execute the following operation: $ARGUMENTS 4. Create the Command Interface Create a simple command-line entry point for your agent.\n1// my-agent.ts 2import { AgentExecutor } from \u0026#39;./src/agent-executor.js\u0026#39;; 3 4async function main() { 5 const args = process.argv.slice(2); 6 const prompt = args.join(\u0026#39; \u0026#39;); 7 8 await AgentExecutor.execute(\u0026#39;my-agent\u0026#39;, prompt, { 9 dryRun: args.includes(\u0026#39;--dry-run\u0026#39;), 10 verbose: args.includes(\u0026#39;--verbose\u0026#39;) 11 }); 12} 13 14main().catch(console.error); 5. Configure MCP Servers Create a .mcp.json file to connect to your tools.\n1{ 2 \u0026#34;mcpServers\u0026#34;: { 3 \u0026#34;your-service\u0026#34;: { 4 \u0026#34;type\u0026#34;: \u0026#34;http\u0026#34;, 5 \u0026#34;url\u0026#34;: \u0026#34;https://your-mcp-server.com\u0026#34;, 6 \u0026#34;headers\u0026#34;: { 7 \u0026#34;Authorization\u0026#34;: \u0026#34;Bearer ${YOUR_API_TOKEN}\u0026#34; 8 } 9 } 10 } 11} Conclusion Claude Code represents a significant step forward in AI application development. By providing built-in conversation management, MCP integration, and streaming responses, it eliminates much of the boilerplate that traditionally plagued agent development.\nThe social-agents project demonstrates how these capabilities enable sophisticated, multi-platform automation with surprisingly little code. The slash command architecture makes agents configurable and maintainable, while the generic executor pattern ensures consistency across different domains.\nKey takeaways for your own agent applications:\nStart with the Executor Pattern: Build a generic executor that can handle multiple agent types. Use Slash Commands: Configure agent behaviors through .claude/commands/*.md files. Embrace MCP Integration: Connect to external services through standardized protocols. Implement Session Management: Support conversation continuity and stateful workflows. Plan for Fallbacks: Handle permissions, errors, and degraded functionality gracefully. Test with Dry Run: Build safe testing into every operation. Whether you're building social media automation, customer service bots, or complex workflow orchestration, Claude Code provides the foundation for sophisticated agentic applications. The patterns from the social-agents project offer a proven template for scaling AI automation across multiple domains and platforms.\nThe future of AI application development is agentic, and Claude Code gives you the tools to build it today. I encourage you to clone the social-agents repository, experiment with the patterns, and start building your own intelligent agents.\nResources Social Agents Repository - Complete implementation example Claude Code SDK Documentation - Official documentation and guides RUBE MCP Server - 500+ app integrations for your agents Model Context Protocol - Learn about MCP standards Have you built agentic applications with Claude Code? Share your experiences and patterns in the comments below!\n","link":"https://kane.mx/posts/2025/claude-code-agent-framework/","section":"posts","tags":["Claude Code","Agent Framework","AI Automation","TypeScript","MCP Protocol","Social Media Automation","Agentic AI"],"title":"Building Agentic Applications with Claude Code: A Developer's Guide to AI-Powered Automation"},{"body":"","link":"https://kane.mx/tags/social-media-automation/","section":"tags","tags":null,"title":"Social Media Automation"},{"body":"","link":"https://kane.mx/tags/authentication/","section":"tags","tags":null,"title":"Authentication"},{"body":"","link":"https://kane.mx/tags/aws-agentcore-gateway/","section":"tags","tags":null,"title":"AWS AgentCore Gateway"},{"body":"","link":"https://kane.mx/tags/aws-agentcore-runtime/","section":"tags","tags":null,"title":"AWS AgentCore Runtime"},{"body":"","link":"https://kane.mx/categories/blogging/","section":"categories","tags":null,"title":"Blogging"},{"body":"Overview Building on my previous exploration of connecting to MCP servers hosted on AWS AgentCore, I've been working extensively with the native MCP SDK's OAuth Client Provider to streamline authentication workflows. The MCP SDK's built-in OAuth support has evolved significantly, offering robust solutions for both interactive user authentication and machine-to-machine (M2M) flows.\nIn this follow-up article, I'll share the key improvements and special techniques I've discovered for using the MCP Client's OAuthClientProvider with AWS AgentCore, including handling AgentCore's unique behavior with 403 responses, implementing M2M authentication flows, and leveraging automatic token refresh capabilities.\nWhat makes this approach particularly compelling is how the native SDK abstracts away much of the OAuth complexity while providing the flexibility needed for enterprise-grade deployments on AWS AgentCore.\nKey Improvements Over Manual OAuth Implementation The native MCP SDK OAuth Client Provider offers several advantages over the manual OAuth implementations I covered in my previous post:\n1. Automatic Token Management Built-in token storage and refresh mechanisms Seamless handling of expired tokens with automatic retry logic Support for both refresh_token (interactive) and client_credentials (M2M) flows 2. AgentCore-Specific Compatibility Custom handling of 403 HTTP responses (AgentCore returns 403 instead of 401 for unauthorized requests) Proper cross-domain OAuth metadata configuration Enhanced error handling and debugging capabilities 3. Dual-Mode Authentication Automatic detection of M2M vs Interactive mode based on client configuration Single codebase supporting both authentication patterns Intelligent scope selection based on OAuth provider type The AgentCoreOAuthClientProvider The heart of this improved implementation is a custom OAuth provider that extends the native MCP SDK's OAuthClientProvider:\n1class AgentCoreOAuthClientProvider(OAuthClientProvider): 2 \u0026#34;\u0026#34;\u0026#34;Custom OAuth provider that triggers on 403 (not just 401) for AgentCore compatibility. 3 4 Supports both interactive OAuth flows and M2M (client_credentials) flows with automatic 5 token refresh for both modes. 6 \u0026#34;\u0026#34;\u0026#34; 7 8 def __init__(self, *args, **kwargs): 9 super().__init__(*args, **kwargs) 10 self.is_m2m_mode = False # Will be set after client info is available 11 12 def _detect_m2m_mode(self) -\u0026gt; bool: 13 \u0026#34;\u0026#34;\u0026#34;Detect if we\u0026#39;re in M2M mode based on client_secret availability.\u0026#34;\u0026#34;\u0026#34; 14 return bool( 15 self.context.client_info and 16 self.context.client_info.client_secret and 17 hasattr(self.context, \u0026#39;client_metadata\u0026#39;) and 18 not hasattr(self.context.client_metadata, \u0026#39;redirect_uris\u0026#39;) or 19 not self.context.client_metadata.redirect_uris 20 ) 21 22 async def async_auth_flow(self, request: httpx.Request) -\u0026gt; AsyncGenerator[httpx.Request, httpx.Response]: 23 \u0026#34;\u0026#34;\u0026#34;HTTPX auth flow integration with 403 support and M2M mode.\u0026#34;\u0026#34;\u0026#34; 24 # ... initialization logic ... 25 26 response = yield request 27 28 # CUSTOM FIX: Trigger OAuth flow on 403 OR 401 (AgentCore returns 403) 29 if response.status_code in (401, 403): 30 # Perform appropriate OAuth flow based on mode 31 if self.is_m2m_mode: 32 # M2M mode: Use client_credentials directly, no browser interaction 33 token_request = await self._get_m2m_token() 34 token_response = yield token_request 35 await self._handle_m2m_token_response(token_response) 36 else: 37 # Interactive mode: Use authorization code flow 38 auth_code, code_verifier = await self._perform_authorization() 39 token_request = await self._exchange_token(auth_code, code_verifier) 40 token_response = yield token_request 41 await self._handle_token_response(token_response) 42 43 # Retry with new tokens 44 self._add_auth_header(request) 45 yield request Special Tricks for AgentCore Runtime 1. 403 Response Handling AWS AgentCore returns HTTP 403 (Forbidden) instead of the standard HTTP 401 (Unauthorized) when authentication is required. This is a critical detail that trips up most OAuth implementations:\n1# Standard OAuth implementations only handle 401 2if response.status_code == 401: 3 # Trigger OAuth flow 4 5# AgentCore-compatible implementation handles both 6if response.status_code in (401, 403): 7 # Trigger OAuth flow - works with both standard servers and AgentCore 2. Cross-Domain Metadata Configuration AgentCore MCP servers run on a different domain from the OAuth provider (typically AWS Cognito). This requires manual configuration of protected resource metadata:\n1# Extract OAuth server URL from discovery URL 2oauth_server_url = config[\u0026#39;discovery_url\u0026#39;].replace(\u0026#39;/.well-known/openid_configuration\u0026#39;, \u0026#39;\u0026#39;) 3 4# Create protected resource metadata pointing to Cognito 5protected_metadata = ProtectedResourceMetadata( 6 resource=PydanticUrl(config[\u0026#39;mcp_server_url\u0026#39;]), 7 authorization_servers=[PydanticUrl(oauth_server_url)] 8) 9 10# Manually inject the metadata into the OAuth context 11oauth_auth.context.protected_resource_metadata = protected_metadata 12oauth_auth.context.auth_server_url = oauth_server_url 3. Pre-configured Client Information AWS Cognito doesn't support OAuth dynamic client registration, so we need to pre-configure client information:\n1# Pre-configure client info to skip registration 2client_info = OAuthClientInformationFull( 3 client_id=config[\u0026#39;client_id\u0026#39;], 4 client_secret=config.get(\u0026#39;client_secret\u0026#39;), 5 authorization_endpoint=\u0026#34;\u0026#34;, # Will be populated during OAuth metadata discovery 6 token_endpoint=\u0026#34;\u0026#34;, # Will be populated during OAuth metadata discovery 7 redirect_uris=redirect_uris 8) 9await token_storage.set_client_info(client_info) M2M Authentication Flow Support One of the most significant improvements is robust support for M2M authentication using the OAuth 2.0 client credentials flow:\nAutomatic Mode Detection The system automatically detects whether to use M2M or interactive authentication based on the presence of a client secret:\n1# Detect M2M mode based on client_secret presence 2is_m2m_mode = bool(config.get(\u0026#39;client_secret\u0026#39;)) 3 4if is_m2m_mode: 5 print(\u0026#34;üè≠ Detected client_secret - using M2M authentication\u0026#34;) 6 print(\u0026#34;üöÄ No user interaction required - fully automated\u0026#34;) 7else: 8 print(\u0026#34;üîê No client_secret detected - using interactive authentication\u0026#34;) 9 print(\u0026#34;üåê Browser-based user authentication required\u0026#34;) M2M Token Acquisition The M2M flow bypasses browser-based authorization entirely:\n1async def _get_m2m_token(self) -\u0026gt; httpx.Request: 2 \u0026#34;\u0026#34;\u0026#34;Get M2M access token using client_credentials flow.\u0026#34;\u0026#34;\u0026#34; 3 token_data = { 4 \u0026#34;grant_type\u0026#34;: \u0026#34;client_credentials\u0026#34;, 5 \u0026#34;client_id\u0026#34;: self.context.client_info.client_id, 6 \u0026#34;client_secret\u0026#34;: self.context.client_info.client_secret, 7 } 8 9 # Add scope if specified 10 if self.context.client_metadata.scope: 11 token_data[\u0026#34;scope\u0026#34;] = self.context.client_metadata.scope 12 13 return httpx.Request( 14 \u0026#34;POST\u0026#34;, 15 token_url, 16 data=token_data, 17 headers={\u0026#34;Content-Type\u0026#34;: \u0026#34;application/x-www-form-urlencoded\u0026#34;} 18 ) AWS Cognito M2M Configuration For AWS Cognito M2M flows, specific configuration is required:\n1# For AWS Cognito M2M, configure appropriate scopes 2if \u0026#39;cognito-idp\u0026#39; in discovery_url.lower(): 3 if is_m2m_mode: 4 # Use configured M2M scopes or None for default 5 scope = config[\u0026#39;m2m_scopes\u0026#39;] # e.g., \u0026#34;mcp-server/read mcp-server/write\u0026#34; 6 else: 7 scope = \u0026#39;openid email aws.cognito.signin.user.admin\u0026#39; Complete Implementation Example Here's how to use the improved OAuth Client Provider:\n1async def test_native_sdk_oauth_flow(config: dict): 2 \u0026#34;\u0026#34;\u0026#34;Test native MCP SDK OAuth flow with auto-detection of M2M vs interactive mode.\u0026#34;\u0026#34;\u0026#34; 3 # Detect M2M mode based on client_secret presence 4 is_m2m_mode = bool(config.get(\u0026#39;client_secret\u0026#39;)) 5 6 # Configure appropriate scopes based on provider and mode 7 if \u0026#39;cognito-idp\u0026#39; in config[\u0026#39;discovery_url\u0026#39;].lower(): 8 if is_m2m_mode: 9 scope = config[\u0026#39;m2m_scopes\u0026#39;] # Resource server scopes 10 else: 11 scope = \u0026#39;openid email aws.cognito.signin.user.admin\u0026#39; 12 else: 13 scope = \u0026#39;openid email profile\u0026#39; if not is_m2m_mode else config[\u0026#39;m2m_scopes\u0026#39;] 14 15 # Create OAuth client metadata 16 client_metadata = OAuthClientMetadata( 17 client_name=\u0026#34;MCP AgentCore OAuth Client\u0026#34;, 18 redirect_uris=[AnyUrl(\u0026#34;http://localhost:3000\u0026#34;)], 19 grant_types=[\u0026#34;authorization_code\u0026#34;, \u0026#34;refresh_token\u0026#34;], 20 response_types=[\u0026#34;code\u0026#34;], 21 scope=scope, 22 ) 23 24 # Create token storage with debugging 25 token_storage = DebugTokenStorage() 26 27 # Pre-configure client info 28 client_info = OAuthClientInformationFull( 29 client_id=config[\u0026#39;client_id\u0026#39;], 30 client_secret=config.get(\u0026#39;client_secret\u0026#39;), 31 authorization_endpoint=\u0026#34;\u0026#34;, 32 token_endpoint=\u0026#34;\u0026#34;, 33 redirect_uris=[AnyUrl(\u0026#34;http://localhost:3000\u0026#34;)] 34 ) 35 await token_storage.set_client_info(client_info) 36 37 # Create custom OAuth client provider with AgentCore compatibility 38 oauth_auth = AgentCoreOAuthClientProvider( 39 server_url=config[\u0026#39;mcp_server_url\u0026#39;], 40 client_metadata=client_metadata, 41 storage=token_storage, 42 redirect_handler=handle_redirect if not is_m2m_mode else dummy_handler, 43 callback_handler=handle_callback if not is_m2m_mode else dummy_handler, 44 ) 45 46 # Configure protected resource metadata for cross-domain support 47 oauth_server_url = config[\u0026#39;discovery_url\u0026#39;].replace(\u0026#39;/.well-known/openid_configuration\u0026#39;, \u0026#39;\u0026#39;) 48 protected_metadata = ProtectedResourceMetadata( 49 resource=PydanticUrl(config[\u0026#39;mcp_server_url\u0026#39;]), 50 authorization_servers=[AnyUrl(oauth_server_url)] 51 ) 52 oauth_auth.context.protected_resource_metadata = protected_metadata 53 oauth_auth.context.auth_server_url = oauth_server_url 54 55 # Use the OAuth provider with streamable HTTP client 56 async with streamablehttp_client(config[\u0026#39;mcp_server_url\u0026#39;], auth=oauth_auth) as (read, write, _): 57 async with ClientSession(read, write) as session: 58 await session.initialize() 59 60 # List and invoke tools 61 tools_result = await session.list_tools() 62 print(f\u0026#34;Found {len(tools_result.tools)} tools available\u0026#34;) 63 64 return True Configuration and Environment Setup The improved implementation supports flexible configuration through environment variables:\n1# OAuth 2.0 Configuration 2export OAUTH_DISCOVERY_URL=\u0026#34;https://cognito-idp.us-east-1.amazonaws.com/us-east-1_ABC123/.well-known/openid_configuration\u0026#34; 3export OAUTH_CLIENT_ID=\u0026#34;your-cognito-client-id\u0026#34; 4 5# M2M Mode (optional - enables machine-to-machine authentication) 6export OAUTH_CLIENT_SECRET=\u0026#34;your-client-secret\u0026#34; 7export OAUTH_M2M_SCOPES=\u0026#34;mcp-server/read mcp-server/write\u0026#34; 8 9# AgentCore Runtime Configuration 10export AGENTCORE_RUNTIME_ARN=\u0026#34;arn:aws:bedrock-agentcore:us-west-2:123456789012:runtime/my-server\u0026#34; 11export AGENTCORE_REGION=\u0026#34;us-west-2\u0026#34; 12 13# Interactive Mode Testing (optional) 14export OAUTH_TEST_USERNAME=\u0026#34;testuser@example.com\u0026#34; 15export OAUTH_TEST_PASSWORD=\u0026#34;your-password\u0026#34; Key Advantages 1. Simplified Integration The native SDK OAuth provider handles all the complex OAuth state management, token storage, and refresh logic automatically.\n2. Production-Ready M2M Support M2M authentication enables fully automated server-to-server communication without user intervention, perfect for production deployments.\n3. AgentCore Compatibility Custom handling of AgentCore's 403 responses and cross-domain metadata configuration ensures seamless integration.\n4. Automatic Token Refresh Both interactive and M2M modes support automatic token refresh, ensuring long-running applications maintain connectivity.\n5. Comprehensive Error Handling Detailed logging and error handling makes troubleshooting authentication issues much easier.\nTroubleshooting Common Issues M2M Authentication Failures 1# Ensure client_credentials flow is enabled in Cognito 2aws cognito-idp update-user-pool-client \\ 3 --user-pool-id \u0026lt;your-user-pool-id\u0026gt; \\ 4 --client-id \u0026lt;your-client-id\u0026gt; \\ 5 --allowed-o-auth-flows \u0026#34;client_credentials\u0026#34; \\ 6 --generate-secret Scope Configuration For AWS Cognito M2M, you may need to configure resource server scopes:\nInteractive mode: openid email aws.cognito.signin.user.admin M2M mode: Custom resource server scopes like mcp-server/read mcp-server/write Cross-Domain Issues Ensure the protected resource metadata correctly maps your MCP server URL to the OAuth authorization server.\nConclusion The native MCP SDK's OAuth Client Provider, enhanced with AgentCore-specific compatibility fixes, provides a robust foundation for production MCP client applications. The automatic detection of M2M vs interactive modes, combined with comprehensive error handling and token management, significantly reduces the complexity of integrating with OAuth-protected MCP servers on AWS AgentCore.\nThe key innovations‚Äîhandling 403 responses, cross-domain metadata configuration, and dual-mode authentication‚Äîmake this approach far more reliable than manual OAuth implementations for enterprise deployments.\nAs the MCP ecosystem continues to evolve, I expect we'll see these patterns become standard practice for production MCP client implementations, particularly in enterprise environments where M2M authentication and automated token management are essential requirements.\nResources Previous Post: How invoking remote MCP servers hosted on AWS AgentCore AWS AgentCore Documentation Model Context Protocol Specification Amazon Bedrock AgentCore MCP Guide MCP Inspector Tool Complete Sample Implementation ","link":"https://kane.mx/posts/2025/use-mcp-client-oauthclientprovider-invoke-mcp-hosted-on-aws-agentcore/","section":"posts","tags":["MCP","MCP Client","OAuth Client Provider","AWS AgentCore Runtime","AWS AgentCore Gateway","OAuth","M2M Authentication","Authentication"],"title":"Leveraging MCP Client's OAuthClientProvider for Seamless AWS AgentCore Authentication"},{"body":"","link":"https://kane.mx/tags/m2m-authentication/","section":"tags","tags":null,"title":"M2M Authentication"},{"body":"","link":"https://kane.mx/tags/mcp/","section":"tags","tags":null,"title":"MCP"},{"body":"","link":"https://kane.mx/tags/mcp-client/","section":"tags","tags":null,"title":"MCP Client"},{"body":"","link":"https://kane.mx/tags/oauth/","section":"tags","tags":null,"title":"OAuth"},{"body":"","link":"https://kane.mx/tags/oauth-client-provider/","section":"tags","tags":null,"title":"OAuth Client Provider"},{"body":"Overview Recently, I've been exploring AWS AgentCore's new capability to host Model Context Protocol (MCP) servers, and I wanted to share my experience with connecting to these remote servers as a client. The Model Context Protocol is an open standard that enables AI assistants to securely connect with external data sources and tools, and AWS AgentCore provides a managed hosting environment for these servers with built-in authentication and scaling capabilities.\nIn this article, I'll walk through the process of invoking MCP servers hosted on AWS AgentCore Runtime or proxied via AgentCore Gateway, covering different authentication methods, client implementation patterns, and practical considerations. What struck me most about this approach is how it bridges the gap between local development and enterprise-grade deployment while maintaining the flexibility that makes MCP so powerful.\nUnderstanding AWS AgentCore and MCP Before diving into the implementation details, let's understand what we're working with. AWS AgentCore is Amazon's managed runtime environment for AI agent applications that supports the Model Context Protocol natively. When you deploy an MCP server to AgentCore Runtime or Gateway, you get:\nManaged Infrastructure: No need to worry about scaling, monitoring, or infrastructure management Built-in Authentication: OAuth 2.0 and AWS SigV4 authentication out of the box Session Isolation: Each client connection gets its own isolated session Serverless Scaling: Automatically scales based on demand The MCP servers deployed on AgentCore expose their tools and resources through a standardized HTTP interface, making them accessible from any MCP-compatible client regardless of where it's running.\nAuthentication Methods One of the first challenges I encountered was understanding the authentication options. AWS AgentCore supports several authentication mechanisms for MCP servers:\n1. OAuth 2.0 Authentication This is the most common approach for production deployments. The OAuth flow involves several modes:\nManual Mode: Interactive browser-based authentication\n1class OAuth2Handler: 2 def __init__(self, discovery_url: str, client_id: str, client_secret: str = None): 3 self.discovery_url = discovery_url.rstrip(\u0026#39;/\u0026#39;) 4 self.client_id = client_id 5 self.client_secret = client_secret 6 self.redirect_uri = \u0026#34;http://localhost:3000\u0026#34; 7 8 async def discover_endpoints(self) -\u0026gt; dict: 9 \u0026#34;\u0026#34;\u0026#34;Discover OAuth 2.0 endpoints using well-known configuration.\u0026#34;\u0026#34;\u0026#34; 10 async with httpx.AsyncClient() as client: 11 response = await client.get(self.discovery_url, timeout=10.0) 12 if response.status_code == 200: 13 return response.json() 14 raise ValueError(f\u0026#34;Discovery failed: HTTP {response.status_code}\u0026#34;) 15 16 async def get_authorization_url(self) -\u0026gt; str: 17 config = await self.discover_endpoints() 18 auth_endpoint = config.get(\u0026#39;authorization_endpoint\u0026#39;) 19 20 params = { 21 \u0026#39;response_type\u0026#39;: \u0026#39;code\u0026#39;, 22 \u0026#39;client_id\u0026#39;: self.client_id, 23 \u0026#39;redirect_uri\u0026#39;: self.redirect_uri, 24 \u0026#39;scope\u0026#39;: \u0026#39;openid email aws.cognito.signin.user.admin\u0026#39;, 25 \u0026#39;state\u0026#39;: \u0026#39;random_state_12345\u0026#39; 26 } 27 return f\u0026#34;{auth_endpoint}?{urlencode(params)}\u0026#34; 28 29 async def exchange_code_for_tokens(self, authorization_code: str) -\u0026gt; dict: 30 config = await self.discover_endpoints() 31 token_endpoint = config.get(\u0026#39;token_endpoint\u0026#39;) 32 33 data = { 34 \u0026#39;grant_type\u0026#39;: \u0026#39;authorization_code\u0026#39;, 35 \u0026#39;client_id\u0026#39;: self.client_id, 36 \u0026#39;code\u0026#39;: authorization_code, 37 \u0026#39;redirect_uri\u0026#39;: self.redirect_uri 38 } 39 40 async with httpx.AsyncClient() as client: 41 response = await client.post(token_endpoint, data=data) 42 if response.status_code == 200: 43 return response.json() 44 raise ValueError(f\u0026#34;Token exchange failed: {response.status_code}\u0026#34;) 45 46# Usage example 47async def manual_oauth_flow(): 48 handler = OAuth2Handler( 49 discovery_url=\u0026#34;https://cognito-idp.us-east-1.amazonaws.com/.../openid-configuration\u0026#34;, 50 client_id=\u0026#34;your-cognito-client-id\u0026#34; 51 ) 52 53 auth_url = await handler.get_authorization_url() 54 webbrowser.open(auth_url) 55 56 # User completes auth and provides callback URL 57 callback_url = input(\u0026#34;Enter callback URL: \u0026#34;) 58 code = parse_qs(urlparse(callback_url).query)[\u0026#34;code\u0026#34;][0] 59 60 tokens = await handler.exchange_code_for_tokens(code) 61 return tokens[\u0026#39;access_token\u0026#39;] Machine-to-Machine Mode: For automated systems using client credentials\n1async def get_m2m_token(oauth_handler: OAuth2Handler) -\u0026gt; dict: 2 \u0026#34;\u0026#34;\u0026#34;Get M2M access token using client_credentials flow.\u0026#34;\u0026#34;\u0026#34; 3 config = await oauth_handler.discover_endpoints() 4 token_endpoint = config.get(\u0026#39;token_endpoint\u0026#39;) 5 6 data = { 7 \u0026#39;grant_type\u0026#39;: \u0026#39;client_credentials\u0026#39;, 8 \u0026#39;client_id\u0026#39;: oauth_handler.client_id, 9 \u0026#39;client_secret\u0026#39;: oauth_handler.client_secret, 10 } 11 12 async with httpx.AsyncClient() as client: 13 response = await client.post(token_endpoint, data=data) 14 if response.status_code == 200: 15 return response.json() 16 raise ValueError(f\u0026#34;M2M token request failed: {response.status_code}\u0026#34;) 17 18# Usage example 19async def m2m_oauth_flow(): 20 handler = OAuth2Handler( 21 discovery_url=\u0026#34;https://cognito-idp.us-east-1.amazonaws.com/.../openid-configuration\u0026#34;, 22 client_id=\u0026#34;your-m2m-client-id\u0026#34;, 23 client_secret=\u0026#34;your-m2m-client-secret\u0026#34; 24 ) 25 26 tokens = await get_m2m_token(handler) 27 return tokens[\u0026#39;access_token\u0026#39;] Quick Mode: For AWS Cognito with existing user credentials\n1async def cognito_quick_mode(discovery_url: str, client_id: str, username: str, password: str): 2 \u0026#34;\u0026#34;\u0026#34;Quick token retrieval using AWS Cognito direct authentication.\u0026#34;\u0026#34;\u0026#34; 3 # Extract region from discovery URL 4 region = re.search(r\u0026#39;cognito-idp\\.([^.]+)\\.amazonaws\\.com\u0026#39;, discovery_url).group(1) 5 6 # Use boto3 for direct authentication 7 session = boto3.Session() 8 cognito_client = session.client(\u0026#39;cognito-idp\u0026#39;, region_name=region) 9 10 response = cognito_client.initiate_auth( 11 ClientId=client_id, 12 AuthFlow=\u0026#39;USER_PASSWORD_AUTH\u0026#39;, 13 AuthParameters={\u0026#39;USERNAME\u0026#39;: username, \u0026#39;PASSWORD\u0026#39;: password} 14 ) 15 16 return response[\u0026#39;AuthenticationResult\u0026#39;][\u0026#39;AccessToken\u0026#39;] 2. AWS SigV4 Authentication For AWS-native integrations, you can use SigV4 signing with your AWS credentials:\n1from botocore.auth import SigV4Auth 2from botocore.awsrequest import AWSRequest 3 4class HTTPXSigV4Auth(httpx.Auth): 5 def __init__(self, credentials, service: str, region: str): 6 self.credentials = credentials 7 self.service = service 8 self.region = region 9 10 def auth_flow(self, request: httpx.Request): 11 # Extract request body for signing 12 body = request.content if hasattr(request, \u0026#39;content\u0026#39;) else b\u0026#39;\u0026#39; 13 14 # Create AWS request for signing 15 aws_request = AWSRequest(method=request.method, url=str(request.url), data=body) 16 aws_request.headers[\u0026#39;Host\u0026#39;] = request.url.host 17 18 # Sign the request 19 signer = SigV4Auth(self.credentials, self.service, self.region) 20 signer.add_auth(aws_request) 21 22 # Update HTTPX request with signed headers 23 for name, value in aws_request.headers.items(): 24 request.headers[name] = value 25 26 yield request 27 28class SigV4AgentCoreMCPClient: 29 def __init__(self, agent_arn: str, region: str = \u0026#34;us-west-2\u0026#34;): 30 self.agent_arn = agent_arn 31 self.region = region 32 self.session = boto3.Session() 33 self.credentials = self.session.get_credentials() 34 35 def get_mcp_url(self) -\u0026gt; str: 36 encoded_arn = self.agent_arn.replace(\u0026#39;:\u0026#39;, \u0026#39;%3A\u0026#39;).replace(\u0026#39;/\u0026#39;, \u0026#39;%2F\u0026#39;) 37 return f\u0026#34;https://bedrock-agentcore.{self.region}.amazonaws.com/runtimes/{encoded_arn}/invocations?qualifier=DEFAULT\u0026#34; 38 39 async def connect(self): 40 mcp_url = self.get_mcp_url() 41 auth = HTTPXSigV4Auth(self.credentials, \u0026#39;bedrock-agentcore\u0026#39;, self.region) 42 43 async with streamablehttp_client(url=mcp_url, auth=auth) as (read, write, _): 44 async with ClientSession(read, write) as session: 45 await session.initialize() 46 return session 47 48# Usage example 49async def sigv4_connection_example(): 50 client = SigV4AgentCoreMCPClient( 51 agent_arn=\u0026#34;arn:aws:bedrock-agentcore:us-west-2:123456789012:runtime/my-server\u0026#34; 52 ) 53 session = await client.connect() 54 return session Client Implementation Patterns Based on my experience, here are the key patterns I've found effective for implementing MCP clients that connect to AgentCore-hosted servers:\nBasic Client Structure 1async def connect_to_agentcore_server(agent_arn, bearer_token): 2 # Encode the ARN for URL usage 3 encoded_arn = agent_arn.replace(\u0026#39;:\u0026#39;, \u0026#39;%3A\u0026#39;).replace(\u0026#39;/\u0026#39;, \u0026#39;%2F\u0026#39;) 4 mcp_url = f\u0026#34;https://bedrock-agentcore.us-west-2.amazonaws.com/runtimes/{encoded_arn}/invocations?qualifier=DEFAULT\u0026#34; 5 6 headers = {\u0026#34;authorization\u0026#34;: f\u0026#34;Bearer {bearer_token}\u0026#34;} 7 8 async with streamablehttp_client(mcp_url, headers) as (read, write, _): 9 async with ClientSession(read, write) as session: 10 await session.initialize() 11 12 # Discover capabilities 13 tools = await session.list_tools() 14 resources = await session.list_resources() 15 16 return session 17 18# Usage 19async def main(): 20 session = await connect_to_agentcore_server(agent_arn, bearer_token) 21 result = await session.call_tool(\u0026#34;add_numbers\u0026#34;, {\u0026#34;a\u0026#34;: 5, \u0026#34;b\u0026#34;: 3}) 22 print(f\u0026#34;Result: {result}\u0026#34;) MCP Connection Testing Here's a simplified approach to test MCP connections:\n1async def test_mcp_connection(mcp_server_url: str, access_token: str): 2 \u0026#34;\u0026#34;\u0026#34;Test MCP connection with access token.\u0026#34;\u0026#34;\u0026#34; 3 headers = {\u0026#34;Authorization\u0026#34;: f\u0026#34;Bearer {access_token}\u0026#34;} 4 5 async with streamablehttp_client(mcp_server_url, headers) as (read, write, _): 6 async with ClientSession(read, write) as session: 7 await session.initialize() 8 9 # List available tools and resources 10 tools = await session.list_tools() 11 resources = await session.list_resources() 12 13 print(f\u0026#34;Found {len(tools.tools)} tools and {len(resources.resources)} resources\u0026#34;) 14 return session 15 16def load_config() -\u0026gt; dict: 17 \u0026#34;\u0026#34;\u0026#34;Load configuration from environment variables.\u0026#34;\u0026#34;\u0026#34; 18 return { 19 \u0026#39;discovery_url\u0026#39;: os.getenv(\u0026#39;OAUTH_DISCOVERY_URL\u0026#39;), 20 \u0026#39;client_id\u0026#39;: os.getenv(\u0026#39;OAUTH_CLIENT_ID\u0026#39;), 21 \u0026#39;agentcore_runtime_arn\u0026#39;: os.getenv(\u0026#39;AGENTCORE_RUNTIME_ARN\u0026#39;), 22 \u0026#39;agentcore_region\u0026#39;: os.getenv(\u0026#39;AGENTCORE_REGION\u0026#39;, \u0026#39;us-west-2\u0026#39;), 23 } 24 25async def test_oauth_flow(config: dict): 26 \u0026#34;\u0026#34;\u0026#34;Test OAuth flow and MCP connection.\u0026#34;\u0026#34;\u0026#34; 27 # Get OAuth token 28 handler = OAuth2Handler(config[\u0026#39;discovery_url\u0026#39;], config[\u0026#39;client_id\u0026#39;]) 29 auth_url = await handler.get_authorization_url() 30 webbrowser.open(auth_url) 31 32 # User provides callback URL 33 callback_url = input(\u0026#34;Enter callback URL: \u0026#34;) 34 code = parse_qs(urlparse(callback_url).query)[\u0026#34;code\u0026#34;][0] 35 36 tokens = await handler.exchange_code_for_tokens(code) 37 38 # Test MCP connection 39 encoded_arn = config[\u0026#39;agentcore_runtime_arn\u0026#39;].replace(\u0026#39;:\u0026#39;, \u0026#39;%3A\u0026#39;).replace(\u0026#39;/\u0026#39;, \u0026#39;%2F\u0026#39;) 40 mcp_url = f\u0026#34;https://bedrock-agentcore.{config[\u0026#39;agentcore_region\u0026#39;]}.amazonaws.com/runtimes/{encoded_arn}/invocations?qualifier=DEFAULT\u0026#34; 41 42 session = await test_mcp_connection(mcp_url, tokens[\u0026#39;access_token\u0026#39;]) 43 return session Practical Usage Patterns Tool Discovery and Invocation MCP enables dynamic discovery of server capabilities:\n1async def explore_server_capabilities(session): 2 # Discover available tools and resources 3 tools_response = await session.list_tools() 4 resources_response = await session.list_resources() 5 6 for tool in tools_response.tools: 7 print(f\u0026#34;Tool: {tool.name} - {tool.description}\u0026#34;) 8 9 for resource in resources_response.resources: 10 print(f\u0026#34;Resource: {resource.name} ({resource.mimeType})\u0026#34;) 11 12async def call_tool_dynamically(session, tool_name, **kwargs): 13 result = await session.call_tool(tool_name, kwargs) 14 return result.content Resource Access Access server resources with simple calls:\n1async def read_server_resource(session, resource_uri): 2 result = await session.read_resource(resource_uri) 3 return result.contents 4 5# Example usage 6contents = await read_server_resource(session, \u0026#34;file://config.json\u0026#34;) 7for content in contents: 8 print(f\u0026#34;{content.mimeType}: {content.text}\u0026#34;) Complete Working Example Here's a simplified production-ready example:\n1async def main(): 2 \u0026#34;\u0026#34;\u0026#34;Main function demonstrating MCP client connection.\u0026#34;\u0026#34;\u0026#34; 3 config = load_config() 4 5 print(\u0026#34;Select authentication mode:\u0026#34;) 6 print(\u0026#34;1. OAuth 2.0 (Manual)\u0026#34;) 7 print(\u0026#34;2. AWS Cognito (Quick)\u0026#34;) 8 print(\u0026#34;3. AWS SigV4\u0026#34;) 9 10 choice = input(\u0026#34;Choose (1/2/3): \u0026#34;) 11 12 if choice == \u0026#34;1\u0026#34;: 13 session = await test_oauth_flow(config) 14 elif choice == \u0026#34;2\u0026#34;: 15 token = await cognito_quick_mode( 16 config[\u0026#39;discovery_url\u0026#39;], 17 config[\u0026#39;client_id\u0026#39;], 18 config[\u0026#39;test_username\u0026#39;], 19 config[\u0026#39;test_password\u0026#39;] 20 ) 21 session = await test_mcp_connection(config[\u0026#39;mcp_server_url\u0026#39;], token) 22 elif choice == \u0026#34;3\u0026#34;: 23 client = SigV4AgentCoreMCPClient(config[\u0026#39;agentcore_runtime_arn\u0026#39;]) 24 session = await client.connect() 25 26 # Use the session 27 tools = await session.list_tools() 28 print(f\u0026#34;Connected! Found {len(tools.tools)} tools available.\u0026#34;) 29 30# Environment setup example 31def setup_environment(): 32 \u0026#34;\u0026#34;\u0026#34;Required environment variables.\u0026#34;\u0026#34;\u0026#34; 33 env_vars = { 34 \u0026#39;OAUTH_DISCOVERY_URL\u0026#39;: \u0026#39;https://cognito-idp.us-east-1.amazonaws.com/.../openid-configuration\u0026#39;, 35 \u0026#39;OAUTH_CLIENT_ID\u0026#39;: \u0026#39;your-cognito-client-id\u0026#39;, 36 \u0026#39;AGENTCORE_RUNTIME_ARN\u0026#39;: \u0026#39;arn:aws:bedrock-agentcore:us-west-2:123456789012:runtime/my-server\u0026#39;, 37 \u0026#39;OAUTH_TEST_USERNAME\u0026#39;: \u0026#39;testuser@example.com\u0026#39;, 38 \u0026#39;OAUTH_TEST_PASSWORD\u0026#39;: \u0026#39;your-password\u0026#39; 39 } 40 41 for key, example in env_vars.items(): 42 print(f\u0026#34;export {key}=\u0026#39;{example}\u0026#39;\u0026#34;) 43 44if __name__ == \u0026#34;__main__\u0026#34;: 45 asyncio.run(main()) Update: Improved OAuth Client Provider Approach Update: For a more robust, production-ready solution using the native MCP SDK's built-in OAuth Client Provider with automatic token management, M2M authentication support, and AgentCore-specific compatibility fixes, see: Leveraging MCP Client's OAuthClientProvider for Seamless AWS AgentCore Authentication.\nKey Learnings 1. Authentication Complexity The biggest lesson from working with MCP servers on AgentCore is that authentication setup is often the most complex part. Whether you're using OAuth with Cognito, Azure AD, or other providers, getting the client credentials and discovery URLs right is crucial. I recommend starting with the manual OAuth mode for testing before moving to automated flows.\n2. Connection Lifecycle Management Unlike local MCP servers where you might maintain persistent connections, AgentCore-hosted servers require careful attention to connection lifecycle. The platform provides session isolation, but you need to handle reconnection gracefully in your client code.\n3. Error Handling is Critical Remote MCP servers introduce network-related failure modes that don't exist with local servers. Building robust retry logic and graceful degradation from the start saves significant debugging time later.\n4. Tool Discovery Enables Dynamic Behavior One of MCP's most powerful features is tool discovery. Rather than hard-coding tool names and parameters, building clients that dynamically discover and adapt to server capabilities makes your code much more resilient to server updates.\nConclusion Working with MCP servers hosted on AWS AgentCore opens up exciting possibilities for building distributed AI agent systems. The combination of MCP's flexible protocol with AgentCore's managed infrastructure provides a powerful foundation for enterprise AI applications.\nThe key to success lies in understanding the authentication flows, building robust connection management, and embracing MCP's dynamic discovery capabilities. While there are complexity challenges, particularly around authentication and error handling, the benefits of managed hosting and automatic scaling make this approach very compelling for production deployments.\nAs the ecosystem continues to mature, I expect we'll see more standardized client libraries and simplified authentication flows that make this integration even more accessible to developers.\nResources AWS AgentCore Documentation Model Context Protocol Specification Amazon Bedrock AgentCore MCP Guide MCP Inspector Tool Sample Implementation ","link":"https://kane.mx/posts/2025/invoke-mcp-hosted-on-aws-agentcore/","section":"posts","tags":["MCP","MCP Client","AWS AgentCore Runtime","AWS AgentCore Gateway","OAuth","Authentication"],"title":"How invoking remote MCP servers hosted on AWS AgentCore"},{"body":"","link":"https://kane.mx/tags/amazon-bedrock/","section":"tags","tags":null,"title":"Amazon Bedrock"},{"body":"","link":"https://kane.mx/tags/aws-amplify/","section":"tags","tags":null,"title":"AWS Amplify"},{"body":"","link":"https://kane.mx/tags/bedrock-knowledgebase/","section":"tags","tags":null,"title":"Bedrock Knowledgebase"},{"body":"Overview In this article, I'll share my experience building an agentic chatbot on AWS using Amazon Bedrock, Amplify Gen2, and Amplify AI kit. This project, called Industry Assistant Portal, serves as an internal industry assistant that provides industry-specific AWS solutions guidance. The chatbot leverages Amazon Bedrock's powerful foundation models and knowledge base capabilities to deliver contextually relevant information about AWS industry solutions.\nThe journey of building this chatbot taught me valuable lessons about implementing agentic AI systems that can reason, plan, and execute complex tasks while maintaining context awareness. I'll cover the architecture, implementation details, challenges faced, and key learnings from this project.\nArchitecture The Portal is built with a modern tech stack:\nFrontend: Next.js 14 with Amplify UI React components (including AIConversation component) Backend: AWS Amplify Gen2 with GraphQL API AI Services: Amazon Bedrock (Claude Sonnet 3.5 v2 and Haiku 3.5) Knowledge Base: Amazon Bedrock Knowledge Base Vector Search: Amazon OpenSearch Serverless's vector search capabilities are used to retrieve relevant documents Authentication: Amazon Cognito flowchart TD Client[\u0026#34;Next.js Client\u0026#34;] Auth[\u0026#34;Amplify UI Authenticator + Cognito\u0026#34;] OIDCProvider[\u0026#34;Corp SSO\u0026#34;] API[\u0026#34;AWS AppSync (GraphQL) + Lambda\u0026#34;] AI[\u0026#34;AWS Bedrock (Claude 3.5 \u0026amp; Haiku 3.5)\u0026#34;] KB[\u0026#34;Bedrock Knowledge Base\u0026#34;] DB[\u0026#34;DynamoDB\u0026#34;] VectorDB[\u0026#34;Amazon OpenSearch Serverless\u0026#34;] DocExporter[\u0026#34;Doc Exporter Lambda\u0026#34;] DocProcessor[\u0026#34;Doc Sheet Processor Lambda\u0026#34;] KBSync[\u0026#34;Knowledge Base Sync Workflow\u0026#34;] S3[\u0026#34;S3 Buckets\u0026#34;] Client \u0026lt;--\u0026gt; Auth Auth \u0026lt;--\u0026gt; OIDCProvider Client \u0026lt;--\u0026gt; API API \u0026lt;--\u0026gt; AI API \u0026lt;--\u0026gt; DB AI \u0026lt;--\u0026gt; KB KB \u0026lt;--\u0026gt; VectorDB DocExporter --\u0026gt; S3 S3 --\u0026gt; DocProcessor DocProcessor --\u0026gt; S3 KBSync --\u0026gt; KB The architecture follows a serverless approach, with the frontend hosted on Amplify Hosting and the backend services managed through Amplify Gen2. The chatbot's intelligence comes from Amazon Bedrock's foundation models, particularly Claude Sonnet 3.5 v2, enhanced with a custom knowledge base containing industry-specific information.\nKey Components 1. Agentic Conversation Flow The heart of the system is an agentic conversation flow that follows a structured approach to understanding and responding to user queries:\nIntention Validation: Every user query is first analyzed to determine if it falls within the chatbot's domain of expertise (AWS solutions and guidance for specific industries). Sequential Intent Identification: The chatbot follows a strict sequence to identify the industry, solution area, and specific use case the user is interested in. Knowledge Base Integration: Relevant information is retrieved from the knowledge base to provide accurate and up-to-date responses. Response Generation: The chatbot generates comprehensive responses with proper citations and relevant industry contacts. sequenceDiagram User-\u0026gt;\u0026gt;+Frontend: Send message Frontend-\u0026gt;\u0026gt;+API: Send message to AI API-\u0026gt;\u0026gt;+AI Service: Process with Claude Sonnet AI Service-\u0026gt;\u0026gt;+Knowledge Base: Query for relevant info via searchDocumentation tool Knowledge Base-\u0026gt;\u0026gt;+Vector DB: Initial vector search (30 results) Vector DB-\u0026gt;\u0026gt;-Knowledge Base: Return initial chunks Knowledge Base-\u0026gt;\u0026gt;+Reranking: Process with Cohere Rerank Reranking-\u0026gt;\u0026gt;-Knowledge Base: Return top 10 reranked results Knowledge Base--\u0026gt;\u0026gt;-AI Service: Return documentation with source URIs AI Service--\u0026gt;\u0026gt;-API: Generate response with citations API--\u0026gt;\u0026gt;-Frontend: Return AI response Frontend--\u0026gt;\u0026gt;-User: Display response with citations and expert contacts This structured approach ensures that the chatbot provides relevant and accurate information while guiding users through a logical conversation flow.\n2. Knowledge Base Integration One of the most powerful features of the chatbot is its integration with Amazon Bedrock Knowledge Base. The knowledge base contains curated information about AWS industry solutions, best practices, and partner offerings.\nThe integration is implemented through a custom resolver that:\nAccepts user queries as input Performs vector search against the knowledge base Applies filtering based on metadata (e.g., updated date, status) Uses reranking to improve relevance of results Returns the most relevant documents to inform the chatbot's responses sequenceDiagram AI Service-\u0026gt;\u0026gt;+Custom Resolver: Call searchDocumentation tool Custom Resolver-\u0026gt;\u0026gt;+Bedrock API: POST to /knowledgebases/{id}/retrieve Bedrock API-\u0026gt;\u0026gt;+Vector DB: Perform vector search Vector DB-\u0026gt;\u0026gt;-Bedrock API: Return 30 most similar chunks Bedrock API-\u0026gt;\u0026gt;+Reranking Model: Process with Cohere Rerank v3.5 Reranking Model-\u0026gt;\u0026gt;-Bedrock API: Return top 10 reranked results Bedrock API-\u0026gt;\u0026gt;-Custom Resolver: Return documentation with metadata Custom Resolver-\u0026gt;\u0026gt;-AI Service: Return formatted results 1export function request(ctx) { 2 const { input } = ctx.args; 3 const { KNOWLEDGE_BASE_ID, RERANK_MODEL_ARN } = ctx.env; 4 5 return { 6 resourcePath: `/knowledgebases/${KNOWLEDGE_BASE_ID}/retrieve`, 7 method: \u0026#34;POST\u0026#34;, 8 params: { 9 headers: { 10 \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, 11 }, 12 body: JSON.stringify({ 13 retrievalQuery: { 14 text: input, 15 }, 16 retrievalConfiguration: { 17 vectorSearchConfiguration: { 18 numberOfResults: 30, 19 filter: { 20 orAll: [ 21 { 22 greaterThan: { 23 key: \u0026#39;updated_date\u0026#39;, 24 value: \u0026lt;relative date\u0026gt;, 25 }, 26 }, 27 { 28 equals: { 29 key: \u0026#39;status\u0026#39;, 30 value: \u0026lt;status\u0026gt;, 31 } 32 } 33 ] 34 }, 35 rerankingConfiguration: { 36 type: \u0026#39;BEDROCK_RERANKING_MODEL\u0026#39;, 37 bedrockRerankingConfiguration: { 38 modelConfiguration: { 39 modelArn: RERANK_MODEL_ARN, 40 }, 41 numberOfRerankedResults: 10 42 } 43 } 44 } 45 }, 46 }), 47 }, 48 }; 49} This approach ensures that the chatbot has access to the most relevant and up-to-date information when responding to user queries.\n3. User Interface The chatbot's user interface is built using Next.js and Amplify UI React components (especially \u0026lt;AIConversation\u0026gt;), providing a clean and intuitive chat experience. Key features include:\nMarkdown rendering with support for code blocks, tables, and lists Custom rendering for special content types (contacts, user choices) generated by LLM Message feedback mechanism for continuous improvement Responsive design for desktop and mobile devices The UI is designed to handle various response formats, including:\nText responses with proper formatting User choice prompts for guided conversations Contact information for industry experts Error messages with appropriate styling Implementation Highlights Amplify Gen2 Configuration The project leverages Amplify Gen2's TypeScript-based configuration for defining backend resources. This approach provides type safety and better developer experience compared to traditional YAML or JSON configurations.\n1export const schema = a.schema({ 2 knowledgeBase: CONFIG.knowledgeBaseId 3 ? a 4 .query() 5 .arguments({ 6 input: a.string(), 7 }) 8 .handler( 9 a.handler.custom({ 10 dataSource: \u0026#39;KnowledgeBaseDataSource\u0026#39;, 11 entry: \u0026#39;./resolvers/kbResolver.js\u0026#39;, 12 }) 13 ) 14 .returns(a.string()) 15 .authorization((allow) =\u0026gt; allow.authenticated()) 16 : a.customType({ 17 input: a.string(), 18 kbId: a.string(), 19 }), 20 21 chat: a.conversation({ 22 aiModel: modelId, 23 systemPrompt: `You are industry assistant, a highly skilled AWS Industry solutions expert...`, 24 tools: [ 25 ...(CONFIG.knowledgeBaseId ? [ 26 a.ai.dataTool({ 27 name: \u0026#39;searchDocumentation\u0026#39;, 28 description: `Performs a similarity search over the documentation...`, 29 query: a.ref(\u0026#39;knowledgeBase\u0026#39;), 30 }), 31 ] : []), 32 a.ai.dataTool({ 33 name: \u0026#39;intentionCheck\u0026#39;, 34 description: `Analyzes the user\u0026#39;s intention...`, 35 query: a.ref(\u0026#39;chatIntention\u0026#39;), 36 }), 37 ], 38 inferenceConfiguration: { 39 maxTokens: 3600, 40 temperature: 0.1, 41 topP: 0.6, 42 }, 43 }).authorization((allow) =\u0026gt; allow.owner()), 44}); This configuration defines the GraphQL schema, AI conversation resources, and authorization rules in a concise and maintainable way.\nAgentic Behavior with Function Calling The chatbot's agentic behavior is implemented using Amazon Bedrock's function calling capabilities. Two main functions are defined:\nsearchDocumentation: Performs similarity search over the knowledge base to retrieve relevant information. intentionCheck: Analyzes the user's intention to determine if it meets the criteria for appropriate responses. These functions allow the chatbot to:\nRetrieve relevant information from the knowledge base Validate user queries against its domain of expertise Guide users through a structured conversation flow Provide accurate and contextually relevant responses Challenges and Solutions Challenge 1: Structured Conversation Flow Challenge: Implementing a structured conversation flow that guides users through industry selection, solution area identification, and use case specification without feeling rigid or unnatural.\nSolution: I designed a sequential intent identification framework that analyzes user queries at multiple levels:\nFirst checking if the query relates to supported industries Then identifying specific solution areas within that industry Finally determining the specific use case the user is interested in This approach allows the chatbot to maintain context while guiding users through a logical conversation flow. The implementation uses a combination of knowledge base retrieval and intent analysis to make this process feel natural.\nChallenge 2: Knowledge Base Integration Challenge: Integrating the knowledge base effectively to provide accurate and relevant information while handling the limitations of retrieval-augmented generation.\nSolution: The solution involved two key aspects:\nData Preprocessing and Conversion:\nConverting various internal resources (PPT, PDF, HTML, etc.) into Markdown format for consistent processing Implementing effective chunking strategies to ensure chunks aren't too large but contain sufficient context Preserving document structure and relationships between sections during chunking Adding metadata to chunks to enable effective filtering and retrieval Optimized Retrieval Implementation:\nImplementing a custom resolver that performs vector search with a relatively high number of results (30) Applying metadata filtering to focus on recent or important documents Using reranking to improve relevance of results Returning a manageable number of reranked results (10) Knowledge Base Sync Workflow:\nImplementing a daily sync process to keep the knowledge base up-to-date Monitoring ingestion jobs to ensure data quality Publishing metrics to CloudWatch for observability This comprehensive approach ensures that the chatbot has access to the most relevant and up-to-date information while avoiding context window limitations. The careful preprocessing of knowledge base data proved to be just as important as the retrieval mechanism itself, as it directly impacts the quality and relevance of the information available to the model.\nChallenge 3: Response Quality and Citations Challenge: Ensuring response quality with proper citations while maintaining a natural conversation flow.\nSolution: I implemented a comprehensive response formatting system that:\nStructures responses in a natural conversation flow Includes citations when providing answers based on knowledge base retrieval Formats citations as footnotes with proper links Includes a references section at the end of responses This approach ensures that responses are both informative and trustworthy, with clear attribution to source materials.\nKey Learnings 1. The Power of Structured Prompting One of the most important lessons from this project is the power of structured prompting. By designing a clear and logical framework for the chatbot's behavior, I was able to create a system that feels intelligent and helpful without going off-track.\nThe sequential intent identification framework ensures that the chatbot:\nStays within its domain of expertise Guides users through a logical conversation flow Provides relevant and accurate information This structured approach is essential for creating agentic AI systems that can handle complex tasks while maintaining context awareness.\n2. Knowledge Base Design Matters The design of the knowledge base significantly impacts the quality of the chatbot's responses. Key considerations include:\nDocument Chunking: Finding the right balance between document size and context preservation Metadata Enrichment: Adding relevant metadata to enable effective filtering Regular Updates: Keeping the knowledge base up-to-date with the latest information Quality Control: Ensuring that the knowledge base contains accurate and relevant information Investing time in knowledge base design pays dividends in the quality of the chatbot's responses.\n3. User Experience Considerations Creating a good user experience for an agentic chatbot involves more than just accurate responses. Important considerations include:\nResponse Formatting: Using markdown, tables, and other formatting to improve readability Guided Options: Providing clear choices when users need to select from multiple options Error Handling: Gracefully handling errors and providing helpful guidance Feedback Mechanisms: Allowing users to provide feedback on responses for continuous improvement These considerations help create a chatbot that feels helpful and intuitive to use.\nFuture Improvements While the current implementation is functional and useful, there are several areas for future improvement:\nMulti-modal Support: Adding support for image and document upload to enable more complex interactions Personalization: Tailoring responses based on user preferences and history Improved Analytics: Implementing more comprehensive analytics to track usage patterns and identify areas for improvement Enhanced Knowledge Base: Expanding the knowledge base with more industry-specific information and use cases Multi-language Support: Adding support for multiple languages to serve a global audience Conclusion Building an agentic chatbot on AWS using Amazon Bedrock and Amplify Gen2 has been a rewarding experience. The combination of powerful foundation models, knowledge base integration, and a well-designed conversation flow creates a system that can provide valuable industry-specific guidance.\nThe key to success lies in the structured approach to conversation design, effective knowledge base integration, and attention to user experience details. By following these principles, you can create agentic AI systems that are both powerful and user-friendly.\nAs AI technology continues to evolve, the possibilities for agentic chatbots will only expand. I'm excited to see how this field develops and how these systems can be further enhanced to provide even more value to users.\nResources Amazon Bedrock Documentation Amplify Gen2 Documentation Next.js Documentation Amplify UI React Documentation Amplify AI kit ","link":"https://kane.mx/posts/2025/build-agentic-chatbot-on-aws/","section":"posts","tags":["AWS Amplify","Amazon Bedrock","AWS","Bedrock Knowledgebase","Agentic AI","Chatbot","Claude","Next.js"],"title":"Build Agentic Chatbot on AWS with Amazon Bedrock"},{"body":"","link":"https://kane.mx/tags/chatbot/","section":"tags","tags":null,"title":"Chatbot"},{"body":"","link":"https://kane.mx/tags/claude/","section":"tags","tags":null,"title":"Claude"},{"body":"","link":"https://kane.mx/tags/next.js/","section":"tags","tags":null,"title":"Next.js"},{"body":"Overview This comprehensive benchmark evaluates the capabilities of 10 leading AI-powered developer tools and IDEs. The focus is on their ability to autonomously complete real-world programming tasks through natural language conversations, minimizing the need for manual coding. The evaluation excludes common features like code explanation, completion, unit testing, and documentation generation to focus on advanced AI capabilities.\nTesting Period: Late December 2024 to Mid-January 2025\nTools Tested: 10 major AI development assistants\nTasks Evaluated: 3 real-world programming scenarios\nNote: Several tools were tested across multiple versions during this period.\nTest Methodology The evaluation focuses on real-world programming scenarios, measuring each tool's ability to act as an autonomous developer through natural language conversations. We employed two distinct testing approaches based on the tool's capabilities:\n1. AI Agentic Approach Applied to: Tools with full agentic capabilities (Cursor, Cline, Continue, Windsurf)\nKey Characteristics:\nComplete codebase context awareness Autonomous exploration and decision-making Multi-tool utilization (file system, terminal, etc.) Minimal human intervention (only approving changes) Performance measured by autonomous work quality 2. Multi-Round Conversation Approach Applied to: Tools without full agentic support\nKey Characteristics:\nTask decomposition into manageable steps Iterative instruction and feedback cycles Active human developer guidance Focus on code generation and modifications Performance measured by code quality and iteration count Evaluation Criteria Each tool was evaluated across four key dimensions:\nTask Completion Accuracy\nSolution correctness Requirement adherence Code functionality Code Quality \u0026amp; Maintainability\nCode structure and organization Documentation quality Best practices adherence Human Intervention Requirements\nNumber of guidance instances needed Complexity of required interventions Error resolution assistance Efficiency Metrics\nTime to completion Resource utilization Cost considerations Test Cases Test Environment Background The web application used for testing is built with the following technologies:\nAmplify Gen2: Latest version of AWS Amplify for frontend and backend development Frontend: Next.js 14.x with MUI (Material-UI) and Amplify UI components Backend: AWS AppSync for GraphQL API management and DynamoDB for data storage Amplify AI: Integrated for GenAI capabilities and chat conversations Test Environment Application Stack The benchmark was conducted using a modern web application built with the following technology stack:\nFrontend Architecture Framework: Next.js 14.x UI Components: Material-UI (MUI) for core components Amplify UI for AWS service integrations Custom themed components State Management: React Context and Hooks Styling: Tailwind CSS with custom theming Backend Services API Layer: AWS AppSync (GraphQL) Database: Amazon DynamoDB Authentication: AWS Cognito AI Services: Amplify AI for GenAI features Development Platform Infrastructure: AWS Amplify Gen2 CI/CD: Amplify Hosting This modern stack was chosen to evaluate the AI tools' capabilities across various technologies and integration points, providing a realistic enterprise development scenario.\nTools and IDEs Tested AI-Powered IDEs Cursor\nStandalone AI-powered IDE built on VSCode Uses Claude 3.5 Sonnet v2 as primary model Built for AI-first development (https://cursor.sh) Windsurf\nStandalone AI-powered editor built on VSCode Features Cascade for deep contextual awareness Powered by advanced AI models Includes inline commands, codelenses, and terminal integration Available at https://codeium.com/windsurf VSCode Extensions Cline (Open Source)\nCommunity-driven AI assistant with CLI and editor integration VSCode extension with human-in-the-loop GUI Features include file editing, terminal execution, browser automation Supports multiple models via OpenRouter, Anthropic, OpenAI, Amazon Bedrock Active open-source community Available at https://github.com/cline/cline Continue (Open Source)\nOpen-source VSCode extension for AI pair programming Supports multiple AI models with flexible configuration Community-driven development Learn more at https://continue.dev GitHub Copilot\nMicrosoft and GitHub's AI pair programmer Multi-IDE integration Available at https://github.com/features/copilot Amazon Q Developer\nAWS's AI coding assistant VS Code and JetBrains IDE integration Access via https://aws.amazon.com/q/developers MarsCode\nByteDance's Douyin AI coding assistant Features code completion, testing, explanation, and error fixing Available as VSCode and JetBrains plugins Focus on data security and privacy Try at https://marscode.cn Tongyi\nAlibaba's AI coding assistant Features real-time completion, multi-file editing, testing IDE integration with VS Code, Visual Studio, JetBrains Enterprise knowledge base integration Access at https://lingma.aliyun.com/ Baidu Comate\nAI coding assistant powered by ERNIE model Features code completion, explanation, debugging Multi-file editing and task decomposition support R\u0026amp;D knowledge system integration Available at https://comate.baidu.com/en Task 1: Theme Management in Next.js Task Detail: Add a new theme, set it as default, and refactor it as a shared variable. Additionally, the web application has two different theme implementations. Based on the prompt, generate a new UI theme, add it to the existing implementation, and make it configurable.\nDifficulty: Easy\nExplanation: For human developers, this task is relatively straightforward. It involves copying and pasting the existing theme code multiple times, then making necessary adjustments to the color and style properties. The process is simple and does not require advanced programming skills, as it mainly focuses on duplicating and modifying existing code snippets to create a new theme.\nTool Result Cost Detailed Notes Cline (Claude 3.5 Sonnet v2) ‚úÖ Success $0.9929 Completed in a single attempt to add and configure the new theme. Used the additional prompts to refactor the code to shared variables. Cline (Deepseek v3) ‚úÖ Success $0.02/¬•0.15 Required 2-3 iterations for proper theme implementation and refactoring the code to shared variables. Cursor (Claude 3.5 Sonnet v2) ‚úÖ Success few fast requests in Pro subscription Excellent first-try implementation. Used the additional prompts to refactor the code to shared variables. Tongyi ‚úÖ Success - Clean implementation but needed help with icon integration. Provided additional optimization suggestions for theme switching. Successfully added the new theme and made it configurable. Windsurf (GPT-4o) ‚úÖ Success - Required iterations for proper CSS variable scope. Strong documentation output. Successfully added and configured the new theme. GitHub Copilot ‚ö†Ô∏è Partial - Successfully implemented theme but struggled with refactoring to shared variables. Continue ‚ùå Failed - The generated code looked good, but the tool failed to apply the diff changes to source files. Looked like it was a bug of applying the diff changes to source files. Amazon Q ‚ùå Failed - Generated the wrong code. Do not support applying the diff changes to source files. MarsCode ‚ùå Failed - Multiple syntax errors in generated code. Poor understanding of Next.js theme architecture. Do not support applying the diff changes to source files. Task 2: Amplify AI Function Calling Integration Task Detail: Implement function calling capabilities in Amplify AI conversations to fetch football scores and standings using the external api-football API. The application already has a chat dialog powered by Amplify AI conversations, but it is currently limited to model training knowledge and cannot provide refreshed game results or information. The task involves extending the existing chat functionality to fetch and display up-to-date football data through function calling.\nDifficulty: Medium\nExplanation: This task requires deep understanding of Amplify AI's conversation tools and function calling features. The main challenge lies in correctly implementing the tools specification and handling external API integration, as most LLMs lack comprehensive knowledge about Amplify AI's latest features.\nTool Result Cost Detailed Notes Cline (Claude 3.5 Sonnet v2) ‚úÖ Success $2.3679 Completed with minimal iterations. Required manual update to standings tool description for season parameter clarity. Demonstrated strong understanding of Amplify AI tools implementation via learning the online documentation. Cursor (Claude 3.5 Sonnet v2) ‚úÖ Success ~5 fast requests in Pro subscription Generated correct implementation after providing documentation links and sample code. Lambda function for standings worked perfectly on first attempt. Required minor manual fixes for season parameter description. Cline (Deepseek v3) ‚ùå Failed $0.049/¬•0.36 Hit auto-approved API request limit (30 requests for a task). Struggled with Amplify Gen2 code comprehension despite existing references. Accidentally deleted crucial code (later restored). Continue ‚ùå Failed - Failed to index external documentation despite configuration. Struggled with Amplify AI tools implementation across both Sonnet 3.5 and Deepseek v3. Multiple bugs in editor functionality. Amazon Q ‚ùå Failed - Unable to generate correct tools definition. Limited knowledge of AWS Amplify Gen2. Poor source file integration from chat window, it could not apply the diff changes to source files. MarsCode ‚ùå Failed - Lacked Amplify Gen2 knowledge. No support for external documentation fetching. Unable to reference Amplify and api-football documentation. Baidu Comate ‚ùå Failed - Required extensive human assistance. Poor source file integration from chat interface. GitHub Copilot ‚ùå Failed - Generated incorrect code despite being provided sample code for Amplify AI conversation tools. Tongyi ‚ùå Failed - Failed to generate correct code despite detailed prompts with sample code. Unable to utilize IDE lint outputs for error resolution. Generated hallucinated responses. Windsurf ‚ùå Failed - No support for external documentation fetching. Generated code had multiple compilation errors in both Amplify Gen2 resources and Lambda functions. Task 3: Next.js 15 and React 19 Migration Task Detail: Upgrade the web application from Next.js 14.x to Next.js 15.x, which includes upgrading to React from 18.x to 19.x accordingly. There is an implicit requirement to upgrade other UI dependencies to support React 19.x, including Amplify UI React, MUI 5.x, and react-draggable.\nDifficulty: Hard\nExplanation: This task requires exploring the implicit dependencies between UI components and React 19. The AI developer needs to understand these dependencies through internet searches and codebase analysis. Another challenge is handling compatibility issues between Next.js 15, React 19, and existing code usage, updating routing patterns, and adapting to new Next.js or React APIs.\nNote: For this challenging migration test case, we only evaluated tools/IDEs that demonstrated strong performance in the previous test cases. This focused approach allowed us to better assess the capabilities of the most promising solutions when handling complex migration tasks.\nTool Result Cost Detailed Notes Cursor (Claude 3.5 Sonnet v2) ‚úÖ Success ~25 fast requests in Pro subscription Successfully upgraded core Next.js and React versions but required significant human assistance for dependency resolution. Struggled with conflicts between React 19 and UI libraries (amplify-ui-react, mui 5.x, react-draggable). Successfully implemented code changes after being provided with specific resolution steps. Successfully resolved the errors after upgrading to newer React and Next.js with providing the online migration guidelines. Cline (Claude 3.5 Sonnet v2) ‚úÖ Success $25+ Successfully completed version upgrades for Next.js, React, MUI, and Amplify UI React with detailed human guidance. Generated correct code to mitigate errors in Next.js based on migration guides and external documentation (like GitHub issues). However, showed inconsistent performance when handling migration-related errors - a same task was resolved quickly for under $1, while another attempt required multiple requests costing over $12. Cline (Deepseek V3) ‚ö†Ô∏è Partial $0.055/¬•0.4 Successfully upgraded Next.js, React, MUI, and Amplify UI React versions with human guidance. Generated correct fixes for some migration errors in newer Next.js and React versions, but couldn't resolve all issues. The Deepseek API demonstrated stability issues, often encountering unexpected errors during processing. Key Takeaways Model Performance Differences Claude 3.5 Sonnet v2 demonstrated consistent excellence Tool implementation significantly impacts model performance Amazon Q showed limited AWS service knowledge AI Agentic Capabilities Matter Tools with agentic capabilities (Cursor, Cline) consistently outperformed traditional AI assistants The ability to autonomously explore codebases and make decisions significantly reduced human intervention Agentic tools showed better understanding of complex project structures and dependencies Web Content Access Critical Tools with web access capabilities performed significantly better Access to up-to-date documentation, GitHub issues, and Stack Overflow was crucial for complex tasks Cost Efficiency Monthly subscription-based tools (like Cursor Pro) proved more cost-effective for programming tasks, offering a low price rate for each request Pay-per-request tools (like Cline/Continue) showed suboptimal cost efficiency due to high API costs for programming tasks requiring multiple iterations Emerging models like Deepseek demonstrated potential for significantly reducing costs while maintaining reasonable performance, though reliability needs improvement Open Source vs Commercial Tools Open-source tools (Cline, Continue) demonstrated several advantages: Faster feature development and community-driven improvements Greater flexibility in model selection and configuration Transparent operation and customizable workflows Cost advantages through flexible API provider choices Commercial products showed no absolute advantage even when using the same underlying models Open source solutions offered better cost control and feature customization Community contributions led to innovative features like MCP protocol support and expanded agentic capabilities Additional Resources LLM Rankings on OpenRouter OpenRouter provides a unified API for accessing multiple LLM providers, similar to AWS Bedrock's InvokeModel API. The platform maintains comprehensive rankings of LLMs based on performance metrics and capabilities across different use cases.\nAccording to OpenRouter's public statistics, weekly LLM request volume has shown exponential growth over the past year, increasing from approximately 14 billion to over 460 billion tokens per week.\nNotable findings from the rankings:\nAI development tools dominate the top 3 LLM usage positions, including Cline, Roo Cline, Aide, and Aider DeepSeek v3, an open-source LLM from China, maintains a strong position in the top 6 Usage patterns indicate growing adoption of AI-assisted development workflows Roo Cline: Community-Driven Innovation Roo Cline represents a significant community contribution to AI-assisted development. This open-source fork of Cline extends the original platform with:\nChat Modes: choose between different prompts for Roo Cline to better suit your workflow Chat Mode Prompt Customization \u0026amp; Prompt Enhancements With over 25,000 installations in the VS Code marketplace, Roo Cline has become a popular choice for developers seeking customizable AI assistance in their workflow. Its specialized chat modes have sparked significant interest in the developer community, with some seeing it as a revolutionary step in AI-assisted development.\n","link":"https://kane.mx/posts/2025/ai-developer-tools-benchmark-comparison/","section":"posts","tags":["AI Development Tools","IDE Comparison","Cursor IDE","Cline","GitHub Copilot","Amazon Q","MarsCode","Tongyi","Claude","Deepseek","GPT-4","Next.js","React","AWS Amplify"],"title":"2025 AI Developer Tools Benchmark: Comprehensive IDE \u0026 Assistant Comparison"},{"body":"","link":"https://kane.mx/tags/ai-development-tools/","section":"tags","tags":null,"title":"AI Development Tools"},{"body":"","link":"https://kane.mx/tags/amazon-q/","section":"tags","tags":null,"title":"Amazon Q"},{"body":"","link":"https://kane.mx/tags/cline/","section":"tags","tags":null,"title":"Cline"},{"body":"","link":"https://kane.mx/tags/cursor-ide/","section":"tags","tags":null,"title":"Cursor IDE"},{"body":"","link":"https://kane.mx/tags/deepseek/","section":"tags","tags":null,"title":"Deepseek"},{"body":"","link":"https://kane.mx/tags/github-copilot/","section":"tags","tags":null,"title":"GitHub Copilot"},{"body":"","link":"https://kane.mx/tags/gpt-4/","section":"tags","tags":null,"title":"GPT-4"},{"body":"","link":"https://kane.mx/tags/ide-comparison/","section":"tags","tags":null,"title":"IDE Comparison"},{"body":"","link":"https://kane.mx/tags/marscode/","section":"tags","tags":null,"title":"MarsCode"},{"body":"","link":"https://kane.mx/tags/react/","section":"tags","tags":null,"title":"React"},{"body":"","link":"https://kane.mx/tags/tongyi/","section":"tags","tags":null,"title":"Tongyi"},{"body":"","link":"https://kane.mx/tags/amazon-nova/","section":"tags","tags":null,"title":"Amazon Nova"},{"body":"","link":"https://kane.mx/tags/amazon-nova-canvas/","section":"tags","tags":null,"title":"Amazon Nova Canvas"},{"body":"","link":"https://kane.mx/tags/anthropic/","section":"tags","tags":null,"title":"Anthropic"},{"body":"","link":"https://kane.mx/tags/claude-desktop/","section":"tags","tags":null,"title":"Claude Desktop"},{"body":"Ever wished you could conjure up the perfect images for your blog posts or articles without leaving your editor? Wouldn't it be amazing to generate professional-quality visuals with just a few keystrokes while you're in your creative flow?\nWell, grab your virtual paintbrush because we're about to dive into how you can do exactly that using GenAI's image generation capabilities with Model Context Protocol (MCP) server!\nThe Magic Behind the Scenes: Model Context Protocol (MCP) Think of Model Context Protocol (MCP) as the universal translator for AI applications. Just like how USB-C lets you plug anything into anything (well, almost), MCP is the cool new standard that helps Large Language Models (LLMs) talk to all sorts of data sources and tools.\nflowchart LR subgraph \u0026#34;Your Computer\u0026#34; Host[\u0026#34;Host with MCP Client\\n(Claude, IDEs, Tools)\u0026#34;] S1[\u0026#34;MCP Server A\u0026#34;] S2[\u0026#34;MCP Server B\u0026#34;] S3[\u0026#34;MCP Server C\u0026#34;] Host \u0026lt;--\u0026gt;|\u0026#34;MCP Protocol\u0026#34;| S1 Host \u0026lt;--\u0026gt;|\u0026#34;MCP Protocol\u0026#34;| S2 Host \u0026lt;--\u0026gt;|\u0026#34;MCP Protocol\u0026#34;| S3 S1 \u0026lt;--\u0026gt; D1[(\u0026#34;Local\\nData Source A\u0026#34;)] S2 \u0026lt;--\u0026gt; D2[(\u0026#34;Local\\nData Source B\u0026#34;)] end subgraph \u0026#34;Internet\u0026#34; S3 \u0026lt;--\u0026gt;|\u0026#34;Web APIs\u0026#34;| D3[(\u0026#34;Remote\\nService C\u0026#34;)] end The elegant simplicity of MCP Architecture\nHere's how this clever system works:\nMCP Hosts: Your favorite apps like Claude Desktop and IDEs that want to tap into the AI goodness MCP Clients: The friendly middlemen ensuring smooth conversations between apps and servers MCP Servers: The specialized workers that make the magic happen Local Data Sources: Your computer's treasure trove of files and services Remote Services: The vast world of internet services at your fingertips What makes MCP really shine is its ability to:\nPlug and play with a growing collection of pre-built tools Switch between different AI providers as easily as changing TV channels Keep your data safe and sound within your own setup Amazon Nova Canvas: Your AI Art Studio Amazon Nova Canvas is like having a professional artist at your beck and call. This cutting-edge image generation model from AWS turns your ideas into stunning visuals, whether you describe them in words or show it reference images.\nWhat's in the toolbox?\nCreative Control: Want to tweak colors or adjust layouts? Just tell it what you want! Magic Wand for Images: Need to swap backgrounds or remove objects? Nova's got your back Safety First: Built-in watermarking and content checks keep things professional and traceable Let's see it in action! Here's what happened when I asked Nova to create \u0026quot;a dinosaur sitting in a teacup\u0026quot;:\nWho knew dinosaurs could be so adorable at teatime?\nPretty neat, right? Nova took this whimsical idea and turned it into something that's both charming and surprisingly believable. The way it handled the size difference between our tea-loving dino and its delicate perch is just chef's kiss.\nMaking It Work: MCP Server for Amazon Bedrock Nova Canvas Ready to connect your favorite editor to this image-generating wonderland? Enter the Amazon Bedrock MCP Server - your bridge to Nova Canvas's creative powers.\nThis open-source gem comes packed with:\nText-to-image generation that actually works Fine-tuning through negative prompts (for when you want to say \u0026quot;but definitely not like that!\u0026quot;) Flexible size and quality settings Consistent results with seed control Rock-solid error handling (because we all need a safety net) Playing Nice with Others: MCP Client Integration The beauty of MCP is how well it plays with others. Here are some of your soon-to-be favorite companions:\nClaude Desktop App: The full package with all the bells and whistles Continue: Your open-source coding buddy that speaks MCP fluently Cline: A VS Code extension that makes AI feel like a natural part of your workflow Setting Up Claude Desktop Want to start generating images in Claude Desktop? Here's your quick setup guide:\nGet your AWS credentials sorted (with Bedrock permissions) Add a sprinkle of MCP server config to Claude Desktop settings Here's what the magic looks like in action:\nNova Canvas bringing ideas to life in Claude Desktop\nWrapping Up As we roll into the New Year, it's incredible to see tools like Nova Canvas and MCP making creative AI so accessible and fun to use. Whether you're a developer, writer, or creative soul, generating professional images is now as easy as describing what's in your imagination.\nThe future is looking bright (and beautifully illustrated) with these tools at our fingertips. As we step into 2025, we can expect even more exciting developments in the world of AI-assisted creativity.\nHere's to a New Year filled with endless creative possibilities! May your prompts be inspired and your generations be spectacular. üé®‚ú®\n","link":"https://kane.mx/posts/2024/ai-image-generation-with-amazon-nova/","section":"posts","tags":["Model Context Protocol","Amazon Nova","Amazon Nova Canvas","Image Generation","GenAI","AWS","Anthropic","Claude Desktop","Cline"],"title":"Create Amazing Images with Amazon Nova and Model Context Protocol"},{"body":"","link":"https://kane.mx/tags/genai/","section":"tags","tags":null,"title":"GenAI"},{"body":"","link":"https://kane.mx/tags/image-generation/","section":"tags","tags":null,"title":"Image Generation"},{"body":"","link":"https://kane.mx/tags/model-context-protocol/","section":"tags","tags":null,"title":"Model Context Protocol"},{"body":"","link":"https://kane.mx/tags/aws-appsync/","section":"tags","tags":null,"title":"AWS AppSync"},{"body":"","link":"https://kane.mx/tags/aws-cognito/","section":"tags","tags":null,"title":"AWS Cognito"},{"body":"","link":"https://kane.mx/tags/fullstack/","section":"tags","tags":null,"title":"Fullstack"},{"body":"","link":"https://kane.mx/tags/llm/","section":"tags","tags":null,"title":"LLM"},{"body":"AWS Amplify is a powerful set of tools and services for developing, hosting, and managing serverless applications. With the recent launch of Amplify Gen 212, the platform has evolved significantly to enhance the developer experience. In this guide, we'll explore nine essential tips that will help you maximize your productivity with AWS Amplify, covering everything from authentication and infrastructure management to AI integration and deployment.\nUnderstanding Amplify Gen 2 Before diving into the tips, let's understand what makes Amplify Gen 2 special. It introduces a code-first developer experience that enables building fullstack applications using TypeScript. Key benefits include:\nTypeScript-first backend development Faster local development with cloud sandbox environments Improved team workflows with fullstack Git branches Unified management console Enhanced integration with AWS CDK Tip 1: Implementing Third-Party Authentication AWS Amplify provides seamless integration with popular authentication providers like Google, Facebook, and Amazon. You can also leverage any service supporting industry-standard protocols like OpenID Connect (OIDC) or SAML. While the built-in Authenticator component doesn't directly support third-party provider customization, you can achieve this through Header and Footer customization.\n1\u0026lt;Authenticator 2 components={{ 3 Header: SignInHeader, 4 SignIn: { 5 Header() { 6 return ( 7 \u0026lt;div className=\u0026#34;px-8 py-2\u0026#34;\u0026gt; 8 \u0026lt;Flex direction=\u0026#34;column\u0026#34; 9 className=\u0026#34;federated-sign-in-container\u0026#34;\u0026gt; 10 \u0026lt;Button 11 onClick={async () =\u0026gt; { 12 await signInWithRedirect({ 13 provider: { 14 custom: \u0026#39;OIDC-Provider\u0026#39; // OIDC Provider name created in Cognito User Pool 15 } 16 }); 17 }} 18 className=\u0026#34;federated-sign-in-button\u0026#34; 19 gap=\u0026#34;1rem\u0026#34; 20 \u0026gt; 21 \u0026lt;svg 22 xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; 23 fill=\u0026#34;#000\u0026#34; 24 version=\u0026#34;1.1\u0026#34; 25 viewBox=\u0026#34;0 0 32 32\u0026#34; 26 xmlSpace=\u0026#34;preserve\u0026#34; 27 className=\u0026#34;amplify-icon federated-sign-in-icon\u0026#34; 28 \u0026gt; 29 \u0026lt;path 30 d=\u0026#34;M31 31.36H1v-.72h30v.72zm0-7H1A.36.36 0 01.64 24V1A.36.36 0 011 .64h30a.36.36 0 01.36.36v23a.36.36 0 01-.36.36zm-29.64-.72h29.28V1.36H1.36v22.28zm7.304-7.476c-.672 0-1.234-.128-1.687-.385s-.842-.6-1.169-1.029l.798-.644c.28.355.593.628.938.819.345.191.747.287 1.204.287.476 0 .847-.103 1.113-.308.266-.206.399-.495.399-.868 0-.28-.091-.52-.273-.721-.182-.201-.511-.338-.987-.414l-.574-.084a4.741 4.741 0 01-.924-.217c-.28-.098-.525-.229-.735-.392s-.374-.366-.49-.609a1.983 1.983 0 01-.175-.868c0-.354.065-.665.196-.931.13-.266.31-.488.539-.665s.501-.311.819-.399a3.769 3.769 0 011.022-.133c.588 0 1.08.103 1.477.308.396.206.744.49 1.043.854l-.742.672c-.159-.224-.392-.427-.7-.609-.308-.182-.695-.272-1.162-.272s-.819.1-1.057.3c-.238.201-.357.474-.357.819 0 .354.119.611.357.77.238.159.581.275 1.029.35l.56.084c.803.122 1.372.353 1.708.693.336.341.504.786.504 1.337 0 .7-.238 1.251-.714 1.652-.476.402-1.13.603-1.96.603zm6.733 0c-.672 0-1.234-.128-1.687-.385s-.842-.6-1.169-1.029l.798-.644c.28.355.593.628.938.819.345.191.747.287 1.204.287.476 0 .847-.103 1.113-.308.266-.206.399-.495.399-.868 0-.28-.091-.52-.273-.721-.182-.201-.511-.338-.987-.413l-.574-.084c-.336-.046-.644-.119-.924-.217s-.525-.229-.735-.392-.374-.366-.49-.609a1.983 1.983 0 01-.175-.868c0-.354.065-.665.196-.931.13-.266.31-.488.539-.665.229-.177.501-.311.819-.399a3.769 3.769 0 011.022-.133c.588 0 1.08.103 1.477.308.396.206.744.49 1.043.854l-.742.672c-.158-.224-.392-.427-.7-.609s-.695-.273-1.162-.273-.819.101-1.057.301c-.238.201-.357.474-.357.819 0 .354.119.611.357.77s.581.275 1.029.35l.56.084c.803.122 1.372.353 1.708.693.337.341.505.786.505 1.337 0 .7-.238 1.251-.715 1.652-.475.401-1.129.602-1.96.602zm7.378 0c-.485 0-.929-.089-1.33-.266s-.744-.432-1.028-.763a3.584 3.584 0 01-.665-1.19 4.778 4.778 0 01-.238-1.561c0-.569.079-1.087.238-1.554a3.56 3.56 0 01.665-1.197c.284-.332.627-.586 1.028-.763s.845-.266 1.33-.266.927.089 1.323.266.739.432 1.029.763c.289.331.513.73.672 1.197.158.467.238.985.238 1.554 0 .579-.08 1.099-.238 1.561a3.546 3.546 0 01-.672 1.19c-.29.331-.633.585-1.029.763a3.19 3.19 0 01-1.323.266zm0-.995c.606 0 1.102-.187 1.484-.56.383-.373.574-.942.574-1.708v-1.036c0-.765-.191-1.334-.574-1.708s-.878-.56-1.484-.56-1.102.187-1.483.56c-.383.374-.574.943-.574 1.708v1.036c0 .766.191 1.335.574 1.708.382.374.877.56 1.483.56z\u0026#34;\u0026gt;\u0026lt;/path\u0026gt; 31 \u0026lt;path fill=\u0026#34;none\u0026#34; d=\u0026#34;M0 0H32V32H0z\u0026#34;\u0026gt;\u0026lt;/path\u0026gt; 32 \u0026lt;/svg\u0026gt; 33 \u0026lt;span style={{color: \u0026#34;white !important\u0026#34;}}\u0026gt;Sign In with My OIDC Provider\u0026lt;/span\u0026gt; 34 \u0026lt;/Button\u0026gt; 35 \u0026lt;Divider label=\u0026#34;or\u0026#34; size=\u0026#34;small\u0026#34;/\u0026gt; 36 \u0026lt;/Flex\u0026gt; 37 \u0026lt;/div\u0026gt; 38 ); 39 } 40 } 41 }} 42 loginMechanisms={[\u0026#39;email\u0026#39;]} 43 signUpAttributes={[\u0026#39;email\u0026#39;]} 44 initialState=\u0026#34;signIn\u0026#34; 45 hideSignUp={true} 46/\u0026gt; Tip 2: Building Passwordless Authentication Amazon Cognito now supports passwordless authentication, including sign-in with passkeys, email, and text messages. While the Authenticator component doesn't natively support these features, you can create a custom authentication experience using the Amplify JS library.\n1import { useState } from \u0026#39;react\u0026#39;; 2import { useRouter } from \u0026#39;next/navigation\u0026#39;; 3import { signIn, confirmSignIn, fetchUserAttributes } from \u0026#39;aws-amplify/auth\u0026#39;; 4import { TextField, Button, CircularProgress, Alert } from \u0026#39;@mui/material\u0026#39;; 5 6export default function Home() { 7 const [email, setEmail] = useState(\u0026#39;\u0026#39;); 8 const [code, setCode] = useState(\u0026#39;\u0026#39;); 9 const [loading, setLoading] = useState(false); 10 const [error, setError] = useState(\u0026#39;\u0026#39;); 11 const [showConfirmation, setShowConfirmation] = useState(false); 12 const router = useRouter(); 13 14 const handleSignIn = async (e: React.FormEvent) =\u0026gt; { 15 e.preventDefault(); 16 setLoading(true); 17 setError(\u0026#39;\u0026#39;); 18 19 try { 20 const { nextStep } = await signIn({ 21 username: email, 22 options: { 23 authFlowType: \u0026#39;USER_AUTH\u0026#39;, 24 preferredChallenge: \u0026#39;EMAIL_OTP\u0026#39;, 25 }, 26 }); 27 if (nextStep.signInStep === \u0026#39;CONFIRM_SIGN_IN_WITH_EMAIL_CODE\u0026#39; || 28 nextStep.signInStep === \u0026#39;CONTINUE_SIGN_IN_WITH_FIRST_FACTOR_SELECTION\u0026#39; 29 ) { 30 setShowConfirmation(true); 31 } 32 } catch (err) { 33 setError(err instanceof Error ? err.message : \u0026#39;Sign in failed\u0026#39;); 34 } finally { 35 setLoading(false); 36 } 37 }; 38 39 const handleConfirmSignIn = async (e: React.FormEvent) =\u0026gt; { 40 e.preventDefault(); 41 setLoading(true); 42 setError(\u0026#39;\u0026#39;); 43 44 try { 45 const { nextStep: confirmSignInNextStep } = await confirmSignIn({ challengeResponse: code }); 46 47 if (confirmSignInNextStep.signInStep === \u0026#39;DONE\u0026#39;) { 48 const attributes = await fetchUserAttributes(); 49 if (attributes.email) { 50 router.push(\u0026#39;/home\u0026#39;); 51 } 52 } 53 } catch (err) { 54 setError(err instanceof Error ? err.message : \u0026#39;Confirmation failed\u0026#39;); 55 } finally { 56 setLoading(false); 57 } 58 }; 59 60 return ( 61 \u0026lt;div className=\u0026#34;flex items-center justify-center min-h-screen\u0026#34;\u0026gt; 62 \u0026lt;div className=\u0026#34;w-full max-w-md p-6\u0026#34;\u0026gt; 63 \u0026lt;div className=\u0026#34;text-center mb-8\u0026#34;\u0026gt; 64 \u0026lt;h1 className=\u0026#34;text-2xl font-bold mb-2\u0026#34;\u0026gt;Sign in to My App\u0026lt;/h1\u0026gt; 65 \u0026lt;p className=\u0026#34;text-gray-600\u0026#34;\u0026gt; 66 {showConfirmation ? \u0026#39;Enter the code sent to your email\u0026#39; : \u0026#39;Enter your email to receive a code\u0026#39;} 67 \u0026lt;/p\u0026gt; 68 \u0026lt;/div\u0026gt; 69 70 {error \u0026amp;\u0026amp; ( 71 \u0026lt;Alert severity=\u0026#34;error\u0026#34; className=\u0026#34;mb-4\u0026#34;\u0026gt; 72 {error} 73 \u0026lt;/Alert\u0026gt; 74 )} 75 76 {!showConfirmation ? ( 77 \u0026lt;form onSubmit={handleSignIn}\u0026gt; 78 \u0026lt;TextField 79 fullWidth 80 label=\u0026#34;Email\u0026#34; 81 type=\u0026#34;email\u0026#34; 82 value={email} 83 onChange={(e) =\u0026gt; setEmail(e.target.value)} 84 disabled={loading} 85 required 86 className=\u0026#34;mb-4\u0026#34; 87 /\u0026gt; 88 \u0026lt;Button 89 fullWidth 90 variant=\u0026#34;contained\u0026#34; 91 type=\u0026#34;submit\u0026#34; 92 disabled={loading} 93 className=\u0026#34;mt-2\u0026#34; 94 \u0026gt; 95 {loading ? \u0026lt;CircularProgress size={24} /\u0026gt; : \u0026#39;Continue\u0026#39;} 96 \u0026lt;/Button\u0026gt; 97 \u0026lt;/form\u0026gt; 98 ) : ( 99 \u0026lt;form onSubmit={handleConfirmSignIn}\u0026gt; 100 \u0026lt;TextField 101 fullWidth 102 label=\u0026#34;Verification Code\u0026#34; 103 value={code} 104 onChange={(e) =\u0026gt; setCode(e.target.value)} 105 disabled={loading} 106 required 107 className=\u0026#34;mb-4\u0026#34; 108 /\u0026gt; 109 \u0026lt;Button 110 fullWidth 111 variant=\u0026#34;contained\u0026#34; 112 type=\u0026#34;submit\u0026#34; 113 disabled={loading} 114 className=\u0026#34;mt-2\u0026#34; 115 \u0026gt; 116 {loading ? \u0026lt;CircularProgress size={24} /\u0026gt; : \u0026#39;Verify\u0026#39;} 117 \u0026lt;/Button\u0026gt; 118 \u0026lt;/form\u0026gt; 119 )} 120 \u0026lt;/div\u0026gt; 121 \u0026lt;/div\u0026gt; 122 ); 123} Tip 3: Managing Backend Access with ID Tokens When working with authenticated users, proper token management is crucial. While Amplify automatically handles access tokens for data API requests, some scenarios require manual token management for accessing user attributes in your backend services.\n1import { fetchAuthSession } from \u0026#39;aws-amplify/auth\u0026#39;; 2 3const session = await fetchAuthSession(); 4if (!session.tokens?.idToken) throw new Error(\u0026#39;User not signed in\u0026#39;); 5 6await client.mutations.action({ 7 ...formData, 8}, { 9 authMode: \u0026#39;userPool\u0026#39;, 10 headers: { 11 \u0026#39;Authorization\u0026#39;: session.tokens.idToken.toString(), 12 } 13}); In the backend, you can use the email attribute of the user like below if you are using AppSync JS resolver: 1import { util } from \u0026#39;@aws-appsync/utils\u0026#39;; 2 3export function request(ctx) { 4 const owner = ctx.identity.claims.email || ctx.identity.username; 5} 6 7export function response(ctx) { 8 return ctx.result; 9} Tip 4: Mastering UI Development Amplify UI provides a rich set of components designed for seamless integration. Learn how to maintain a consistent look and feel when combining Amplify UI with other popular libraries like Material-UI (MUI).\n1import { ThemeProvider, createTheme, defaultDarkModeOverride } from \u0026#39;@aws-amplify/ui-react\u0026#39;; 2import { styled, ThemeProvider as MUIThemeProvider, createTheme } from \u0026#39;@mui/material/styles\u0026#39;; 3 4const theme = createTheme({ 5 name: \u0026#39;christmas-theme\u0026#39;, 6 tokens: { 7 colors: { 8 background: { 9 primary: { value: \u0026#39;#FFFFFF\u0026#39; }, // Snow white background 10 secondary: { value: \u0026#39;#165B33\u0026#39; }, // Christmas green 11 }, 12 }, 13 components: { 14 button: { 15 primary: { 16 backgroundColor: { value: \u0026#39;#CC231E\u0026#39; }, 17 color: { value: \u0026#39;#FFFFFF\u0026#39; }, 18 _hover: { 19 backgroundColor: { value: \u0026#39;#165B33\u0026#39; }, 20 }, 21 }, 22 }, 23 }, 24 }, 25 overrides: [defaultDarkModeOverride] 26}); 27 28const muiTheme = createTheme({ 29 palette: { 30 primary: { 31 main: theme.tokens.colors.font.interactive.value, 32 }, 33 }, 34}); 35 36export default function RootLayout({ 37 children, 38}: { 39 children: React.ReactNode; 40}) { 41 return ( 42 \u0026lt;html lang=\u0026#34;en\u0026#34; className={inter.className}\u0026gt; 43 \u0026lt;head\u0026gt; 44 \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1\u0026#34; /\u0026gt; 45 \u0026lt;/head\u0026gt; 46 \u0026lt;body\u0026gt; 47 \u0026lt;ThemeProvider theme={theme}\u0026gt; 48 \u0026lt;AmplifyProvider\u0026gt; 49 \u0026lt;main className=\u0026#34;min-h-screen\u0026#34;\u0026gt; 50 \u0026lt;MUIThemeProvider theme={muiTheme}\u0026gt; 51 {children} 52 \u0026lt;/MUIThemeProvider\u0026gt; 53 \u0026lt;/main\u0026gt; 54 \u0026lt;/AmplifyProvider\u0026gt; 55 \u0026lt;/ThemeProvider\u0026gt; 56 \u0026lt;/body\u0026gt; 57 \u0026lt;/html\u0026gt; 58 ); 59} Tip 5: Extending Infrastructure with CDK For complex backend requirements, AWS CDK enables powerful customization of your Amplify backend. This allows you to manage all types of AWS resources while benefiting from the extensive CDK construct ecosystem.\nExample 1: Customizing Lambda Logging You might want to customize the logging configuration of a Lambda function in your Amplify backend. Here's how you can achieve this using CDK Interoperability:\n1(backend.leagueHandler.resources.lambda.node.defaultChild as CfnFunction).addPropertyOverride(\u0026#39;LoggingConfig\u0026#39;, { 2 LogFormat: \u0026#39;JSON\u0026#39;, 3 ApplicationLogLevel: process.env.PRODUCTION ? \u0026#39;WARN\u0026#39; : \u0026#39;TRACE\u0026#39;, 4 SystemLogLevel: \u0026#39;INFO\u0026#39;, 5}); Example 2: Use environment variables in AppSync JS resolvers When using AppSync resolvers, you might need to access different external resources in different stags. You can use environment variables of AppSync to achieve this. Here's how you can set them in your Amplify backend and access them in a resolver:\nFilename: amplify/backend.ts 1const { cfnResources } = backend.data.resources; 2cfnResources.cfnGraphqlApi.xrayEnabled = true; 3cfnResources.cfnGraphqlApi.environmentVariables = { 4 ...(config.data.knowledgeBaseId ? { KNOWLEDGE_BASE_ID: config.data.knowledgeBaseId } : {}), 5 RERANK_MODEL_ID: \u0026#39;cohere.rerank-v3-5:0\u0026#39;, 6}; Filename: amplify/data/resolvers/kbResolver.js 1export function request(ctx) { 2 const { input } = ctx.args; 3 const { REGION, KNOWLEDGE_BASE_ID, RERANK_MODEL_ID } = ctx.env; 4 return { 5 resourcePath: `/knowledgebases/${KNOWLEDGE_BASE_ID}/retrieve`, 6 method: \u0026#34;POST\u0026#34;, 7 params: { 8 headers: { 9 \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, 10 }, 11 body: JSON.stringify({ 12 retrievalQuery: { 13 text: input, 14 }, 15 vectorSearchConfiguration: { 16 numberOfResults: 30, 17 rerankingConfiguration: { 18 type: \u0026#39;BEDROCK\u0026#39;, 19 bedrockRerankingConfiguration: { 20 modelConfiguration: { 21 modelArn: `arn:aws:bedrock:${REGION}::foundation-model/${RERANK_MODEL_ID}`, 22 additionalModelRequestFields: { 23 topK: 20 24 } 25 }, 26 numberOfRerankedResults: 10 27 } 28 } 29 }, 30 }), 31 }, 32 }; 33} 34 35export function response(ctx) { 36 return JSON.stringify(ctx.result.body); 37} Tip 6: Optimizing DynamoDB Access Learn how to handle common challenges like circular dependencies when accessing DynamoDB tables from Lambda resolvers in your Amplify-generated AppSync API. You can access user identity information in your resolvers using AppSync identity context.\n1import { defineBackend } from \u0026#39;@aws-amplify/backend\u0026#39;; 2import { auth } from \u0026#39;./auth/resource\u0026#39;; 3import { data, leagueHandler } from \u0026#39;./data/resource\u0026#39;; 4export const backend = defineBackend({ 5 auth, 6 data, 7 leagueHandler, 8}); 9 10const externalTableStack = backend.createStack(\u0026#39;ExternalTableStack\u0026#39;); 11 12const leagueTable = new Table(externalTableStack, \u0026#39;League\u0026#39;, { 13 partitionKey: { 14 name: \u0026#39;id\u0026#39;, 15 type: AttributeType.STRING 16 }, 17 billingMode: BillingMode.PAY_PER_REQUEST, 18 removalPolicy: RemovalPolicy.DESTROY, 19}); 20 21backend.data.addDynamoDbDataSource( 22 \u0026#34;ExternalLeagueTableDataSource\u0026#34;, 23 leagueTable as any 24); 25 26leagueTable.grantReadWriteData(backend.leagueHandler.resources.lambda); 27(backend.leagueHandler.resources.lambda as NodejsFunction).addEnvironment(\u0026#39;LEAGUE_TABLE_NAME\u0026#39;, leagueTable.tableName); declare the DynamoDB table outside of generated Amplify stack in amplify/backend.ts 1const schema = a.schema({ 2 League: a.customType({ 3 id: a.string().required(), 4 leagueCountry: a.ref(\u0026#39;LeagueCountry\u0026#39;), 5 teams: a.ref(\u0026#39;Team\u0026#39;).array(), 6 season: a.integer(), 7 }), 8}); Declare the DynamoDB schema as a custom type for AppSync in amplify/data/resource.ts, see here for more details.\nTip 7: Building Resilient AI Features Improve your application's reliability by implementing cross-region model inference with the Amplify AI Kit. While not supported out-of-the-box, you can achieve this using CDK Interoperability.\nHack the role of Lambda function for conversation and AppSync resolver role for generate in amplify/backend.ts 1function createBedrockPolicyStatement(currentRegion: string, accountId: string, modelId: string, crossRegionModel: string) { 2 return new PolicyStatement({ 3 resources: [ 4 `arn:aws:bedrock:*::foundation-model/${modelId}`, 5 `arn:aws:bedrock:${currentRegion}:${accountId}:inference-profile/${crossRegionModel}`, 6 ], 7 actions: [\u0026#39;bedrock:InvokeModel*\u0026#39;], 8 }); 9} 10 11if (CROSS_REGION_INFERENCE \u0026amp;\u0026amp; CUSTOM_MODEL_ID) { 12 const currentRegion = getCurrentRegion(backend.stack); 13 const crossRegionModel = getCrossRegionModelId(currentRegion, CUSTOM_MODEL_ID); 14 15 // [chat converstation] 16 const chatStack = backend.data.resources.nestedStacks?.[\u0026#39;ChatConversationDirectiveLambdaStack\u0026#39;]; 17 if (chatStack) { 18 const conversationFunc = chatStack.node.findAll() 19 .find(child =\u0026gt; child.node.id === \u0026#39;conversationHandlerFunction\u0026#39;) as IFunction; 20 21 if (conversationFunc) { 22 conversationFunc.addToRolePolicy( 23 createBedrockPolicyStatement(currentRegion, backend.stack.account, CUSTOM_MODEL_ID, crossRegionModel) 24 ); 25 } 26 } 27 28 // [insights generation] 29 const insightsStack = backend.data.resources.nestedStacks?.[\u0026#39;GenerationBedrockDataSourceGenerateInsightsStack\u0026#39;]; 30 if (insightsStack) { 31 const dataSourceRole = insightsStack.node.findChild(\u0026#39;GenerationBedrockDataSourceGenerateInsightsIAMRole\u0026#39;) as IRole; 32 if (dataSourceRole) { 33 dataSourceRole.attachInlinePolicy( 34 new Policy(insightsStack, \u0026#39;CrossRegionInferencePolicy\u0026#39;, { 35 statements: [ 36 createBedrockPolicyStatement(currentRegion, backend.stack.account, CUSTOM_MODEL_ID, crossRegionModel) 37 ], 38 }), 39 ); 40 } 41 } 42}\nSpecify the model ID in amplify/data/resource.ts 1const schema = a.schema({ 2 generateInsights: a.generation({ 3 aiModel: CROSS_REGION_INFERENCE ? { 4 resourcePath: getCrossRegionModelId(getCurrentRegion(undefined), CUSTOM_MODEL_ID!), 5 } : a.ai.model(LLM_MODEL), 6 systemPrompt: LLM_SYSTEM_PROMPT, 7 inferenceConfiguration: { 8 maxTokens: 1000, 9 temperature: 0.65, 10 }, 11 }) 12 .arguments({ 13 requirement: a.string().required(), 14 }) 15 .returns(a.customType({ 16 insights: a.string().required(), 17 })) 18 .authorization(allow =\u0026gt; [allow.authenticated()]), 19 20 chat: a.conversation({ 21 aiModel: CROSS_REGION_INFERENCE ? { 22 resourcePath: getCrossRegionModelId(getCurrentRegion(undefined), CUSTOM_MODEL_ID!), 23 } : a.ai.model(LLM_MODEL), 24 systemPrompt: FOOTBALL_SYSTEM_PROMPT, 25 }).authorization(allow =\u0026gt; allow.owner()), 26});\nTip 8: Creating Sophisticated Chat Interfaces The AIConversation component provides a flexible foundation for building chat applications. Master state management and user context handling for multiple conversations.\n1import { useState } from \u0026#39;react\u0026#39;; 2import { Fab, Paper, IconButton, Box, Tooltip, Typography } from \u0026#39;@mui/material\u0026#39;; 3import { AIConversation } from \u0026#39;@aws-amplify/ui-react-ai\u0026#39;; 4import { Avatar } from \u0026#39;@aws-amplify/ui-react\u0026#39;; 5import \u0026#39;@aws-amplify/ui-react/styles.css\u0026#39;; 6import { generateClient } from \u0026#39;aws-amplify/data\u0026#39;; 7import { createAIHooks } from \u0026#39;@aws-amplify/ui-react-ai\u0026#39;; 8import { type Schema } from \u0026#39;../../amplify/data/resource\u0026#39;; 9import ReactMarkdown from \u0026#39;react-markdown\u0026#39;; 10 11const client = generateClient\u0026lt;Schema\u0026gt;({ authMode: \u0026#39;userPool\u0026#39; }); 12const { useAIConversation } = createAIHooks(client); 13 14interface ChatBotProps { 15 chatId?: string; 16 refreshKey: number; 17 onStartNewChat: () =\u0026gt; void; 18 onLoadConversations: () =\u0026gt; void; 19 isLoading: boolean; 20} 21 22export default function ChatBot({ 23 chatId, 24 refreshKey, 25 onStartNewChat, 26 onLoadConversations, 27 isLoading 28}: ChatBotProps) { 29 const [open, setOpen] = useState(refreshKey \u0026gt; 0); 30 const [position, setPosition] = useState({ x: 0, y: 0 }); 31 32 const conversation = useAIConversation(\u0026#39;chat\u0026#39;, { 33 id: chatId, 34 }); 35 const [{ data: { messages }, isLoading: isLoadingChat }, sendMessage] = conversation; 36 37 const handleOpen = () =\u0026gt; { 38 setOpen(true); 39 onLoadConversations(); 40 }; 41 42 const handleClose = () =\u0026gt; setOpen(false); 43 44 const handleNewChat = () =\u0026gt; { 45 // Reset conversation and create new chat 46 onStartNewChat(); 47 }; 48 49 return ( 50\u0026lt;Box sx={{ flexGrow: 1, overflow: \u0026#39;hidden\u0026#39; }}\u0026gt; 51 \u0026lt;AIConversation 52 key={chatId} 53 allowAttachments 54 messages={messages} 55 handleSendMessage={sendMessage} 56 isLoading={isLoadingChat || isLoading} 57 avatars={{ 58 user: { 59 avatar: \u0026lt;Avatar size=\u0026#34;small\u0026#34; alt={email} /\u0026gt;, 60 username: \u0026#39;People\u0026#39; 61 }, 62 ai: { 63 avatar: \u0026lt;Avatar size=\u0026#34;small\u0026#34; alt=\u0026#34;AI\u0026#34; /\u0026gt;, 64 username: \u0026#39;Chat Bot\u0026#39; 65 } 66 }} 67 messageRenderer={{ 68 text: ({ text }) =\u0026gt; \u0026lt;ReactMarkdown\u0026gt;{text}\u0026lt;/ReactMarkdown\u0026gt;, 69 }} 70 /\u0026gt; 71\u0026lt;/Box\u0026gt; 72 ); 73} Tip 9: Streamlining Deployment Debugging When troubleshooting deployment issues in Amplify Hosting, leverage the --debug flag for deeper insights into pipeline failures, especially when code works in sandbox but fails in production.\n1version: 1 2backend: 3 phases: 4 build: 5 commands: 6 - nvm install 20 7 - nvm use 20 8 - npm ci --cache .npm --prefer-offline 9 - npx ampx pipeline-deploy --branch $AWS_BRANCH --app-id $AWS_APP_ID --debug 10frontend: 11 phases: 12 preBuild: 13 commands: 14 - nvm install 20 15 - nvm use 20 16 build: 17 commands: 18 - npm run build 19 artifacts: 20 baseDirectory: .next 21 files: 22 - \u0026#39;**/*\u0026#39; 23 cache: 24 paths: 25 - .next/cache/**/* 26 - .npm/**/* Conclusion AWS Amplify Gen 2 represents a significant evolution in fullstack development on AWS, offering a developer experience comparable to platforms like Vercel with Next.js. These tips will help you leverage Amplify's generated services alongside CDK's powerful constructs to build sophisticated serverless applications efficiently. The platform's seamless integration with the AWS ecosystem makes it an excellent choice for teams looking to accelerate their development process while maintaining enterprise-grade quality and scalability.\nIntroducing the Next Generation of AWS Amplify's Fullstack Development Experience\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFullstack TypeScript: Reintroducing AWS Amplify\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","link":"https://kane.mx/posts/2024/aws-amplify-you-need-to-know/","section":"posts","tags":["AWS","AWS Amplify","Amazon Bedrock","AWS AppSync","AWS Cognito","LLM","Claude","Serverless","Fullstack","GenAI"],"title":"Nine Essential Tips of AWS Amplify for Boosting Development Productivity"},{"body":"","link":"https://kane.mx/categories/serverless-computing/","section":"categories","tags":null,"title":"Serverless-Computing"},{"body":"So, Who Am I? Hey there! üëã I'm a proud dad of three awesome boys who keep me on my toes (and occasionally drive me crazy üòÖ). Currently calling Beijing my home, where I juggle parenting with my tech adventures.\nSpeaking of tech - I'm one of those *nix nerds who gets excited about terminal commands and clean code. By day, I wear multiple hats as a full-stack software engineer, but I also geek out over DevOps and all things cloud native especially Serverless technologies. Think of me as a digital Swiss Army knife - I love tinkering with different technologies and making things work seamlessly together!\nWhen I'm not chasing my kids around or diving deep into code, you might find me obsessing over the latest tech trends or trying to automate yet another part of my life (because who doesn't love a good automation script, right? ü§ñ).\nWant to know more about my adventures in tech and parenting? Feel free to stalk me (professionally, of course!) through the social links above. I promise my feed is a fun mix of code snippets, tech thoughts, and occasional dad jokes! üòâ\n","link":"https://kane.mx/about/","section":"","tags":null,"title":"About"},{"body":"Â¶ÇÊûú‰Ω†Âπ≥Êó∂ÂÖ≥Ê≥®ÁßëÊäÄÂúàÁöÑÂä®ÊÄÅÔºåÈÇ£‰πà‰Ω†‰∏ÄÂÆöÂú®ÊúãÂèãÂúàÊàñÁõ¥Êí≠‰∏≠ÁúãÂà∞ËøáËøôÊ†∑ÁöÑÂê∏ÁùõËØùÈ¢òÔºö\nÁºñÁ®ãÂ∞èÁôΩÁî® AI ÁºñÁ®ãÔºå‰∫∫‰∫∫ÈÉΩËÉΩÊàê‰∏∫ÁºñÁ®ãÈ´òÊâã Èõ∂Âü∫Á°Ä‰πüËÉΩË∑®ÁïåÂºÄÂèë ÁºñÁ®ãÂ∞èÁôΩÁöÑÈÄÜË¢≠ÔºöÂÄüÂä© AI ÊâìÈÄ† xxx Âú®Ëá™Â™í‰ΩìÁöÑÊé®Ê≥¢Âä©Êæú‰∏ãÔºåAI ÁºñÁ®ãÂ∑≤Êàê‰∏∫‰∏ÄÁßçÊΩÆÊµÅÔºåÁîöËá≥Ë¢´ÂåÖË£ÖÊàê‰∫∫‰∫∫ÈÉΩÂèØÊéåÊè°ÁöÑÊäÄËÉΩ„ÄÇÂΩìÁÑ∂ÔºåËøôËÉåÂêé‰πüÂ∞ë‰∏ç‰∫ÜÂüπËÆ≠ÂÖ¨Âè∏ÂÄüÊú∫Ë¥©ÂçñÁÑ¶ËôëÁöÑÊé®Âä®„ÄÇ\nÈöèÁùÄÂ§ßÊ®°ÂûãËÉΩÂäõÁöÑÊåÅÁª≠Â¢ûÂº∫ÔºåÁâπÂà´ÊòØ Claude Sonnet 3.5 v2 ÁöÑÂèëÂ∏ÉÔºåAI ÁºñÁ®ãÁöÑËÉΩÂäõÂÜçÊ¨°ÂæóÂà∞ÊòæËëóÊèêÂçá„ÄÇÂêÑÁßç AI ÁºñÁ®ãÂ∑•ÂÖ∑Âíå AI IDE Â±ÇÂá∫‰∏çÁ©∑ÔºåÊØîÂ¶Ç ClineÔºàVS Code Êèí‰ª∂Ôºâ„ÄÅAiderÔºà‰∏ªË¶ÅÊîØÊåÅ terminalÔºâ„ÄÅContinueÔºàVS Code Âíå JetBrains Êèí‰ª∂Ôºâ„ÄÅCursorÔºàÂü∫‰∫é VS Code ÁöÑËá™ÂÆö‰πâ IDEÔºâ„ÄÅAmazon Q DeveloperÔºàVS Code Âíå JetBrains Êèí‰ª∂Ôºâ‰ª•Âèä GitHub CopilotÔºàVS Code Êèí‰ª∂ÔºâÁ≠â„ÄÇËøô‰∫õÂ∑•ÂÖ∑Â¶ÇÈõ®ÂêéÊò•Á¨ãËà¨Ê∂åÁé∞ÔºåÁ°ÆÂÆûËØ¥Êòé LLM Âú®ÁºñÁ®ãÈ¢ÜÂüüÁöÑËÉΩÂäõÂ∑≤ÁªèÂæóÂà∞‰∫ÜÊòæËëóÊèêÂçáÔºåÂÖ∑Â§á‰∫ÜÂÆûÈôÖÂ∫îÁî®ÁöÑÂú∫ÊôØÂíåËÉΩÂäõ„ÄÇ\nÊúÄËøëÔºåÊàëÂú®‰∏Ä‰∏™‰∏™‰∫∫È°πÁõÆ‰∏≠ÂÆåÊï¥‰ΩìÈ™å‰∫Ü Cursor ÁöÑ AI ËæÖÂä©ÁºñÁ®ãËÉΩÂäõÔºå‰∏ãÈù¢ÊàëÂ∞±‰ª•Ëøô‰∏™È°πÁõÆ‰∏∫‰æãÔºåÂàÜ‰∫´ÊàëÁöÑ‰ΩøÁî®‰ΩìÈ™å„ÄÇ\nËøôÊòØ‰∏Ä‰∏™Âü∫‰∫é Next.js Âíå Material UI ÂÆûÁé∞ÁöÑ Web Â∫îÁî®ÔºåÁî®‰∫éËã±ËØ≠ÂçïËØçÂê¨ÂÜôÁªÉ‰π†ÔºåÂêåÊó∂ÂèØ‰ª•Â∏ÆÂä©ÁîµËÑëÂàùÂ≠¶ËÄÖÁªüËÆ°ÈîÆÁõòËæìÂÖ•ÊïàÁéá„ÄÇËôΩÁÑ∂ÊàëÊúâÂ§öÂπ¥ÁöÑ JavaScript/TypeScript ÂºÄÂèëÁªèÈ™åÔºå‰ΩÜËøôÊòØÊàëÈ¶ñÊ¨°‰ΩøÁî® React„ÄÅNext.js„ÄÅMaterial UI Á≠âÊ°ÜÊû∂ÂºÄÂèë Web Â∫îÁî®„ÄÇÊï¥‰∏™Â∫îÁî®‰ªéÈõ∂ÂºÄÂßãÔºåÈÄöËøá Chat Ê®°Âºè‰∏é LLMÔºà‰∏ªË¶ÅÊòØ Claude Sonnet 3.5 v2ÔºâÂçè‰ΩúÂàõÂª∫È°πÁõÆÈ™®Êû∂ÔºåÂπ∂ÈÄêÊ≠•ÂÆåÂñÑÂäüËÉΩÂèäÈ°µÈù¢Â±ïÁ§∫„ÄÇÊúÄÁªàÈÄöËøá Vercel Âíå AWS Amplify ÂÆåÊàêÈÉ®ÁΩ≤‰∏äÁ∫øÔºàVercel ÁâàÊú¨„ÄÅAWS ÁâàÊú¨Ôºâ„ÄÇ\nÂú®ÂºÄÂèëËøáÁ®ã‰∏≠ÔºåÊàë‰∏ªË¶ÅÈÄöËøá Cursor ÁöÑ Chat ÂäüËÉΩÊù•ÊèèËø∞ÈúÄÊ±ÇÔºåËÆ© Cursor ÊçÆÊ≠§ÁîüÊàê‰ª£Á†Å„ÄÇÊàëÁöÑÂ∑•‰Ωú‰∏ªË¶ÅÊòØÂÆ°Êü•ÁîüÊàêÁöÑ‰ª£Á†ÅÔºåÂÜ≥ÂÆöÊé•ÂèóÊàñÊãíÁªùÔºåÂπ∂Ê†πÊçÆÊñ∞ÁöÑÂèçÈ¶àËÆ© Cursor ÁªßÁª≠‰ºòÂåñ„ÄÇ‰∏é‰º†ÁªüÂºÄÂèëÊ®°Âºè‰∏çÂêåÔºåÊàë‰∏çÂÜçÈúÄË¶ÅÂú®ÁºñËæëÂô®‰∏≠Â§ßÈáèËæìÂÖ•‰ª£Á†ÅÔºåËÄåÊòØÈÄöËøáÂØπËØùÊ®°Âºè‰∏é Cursor ‰∫§ÊµÅÔºåÊèèËø∞ÈúÄÊ±ÇÂπ∂Ëé∑ÂèñÁõ∏Â∫îÁöÑ‰ª£Á†ÅÂÆûÁé∞„ÄÇÂÄºÂæó‰∏ÄÊèêÁöÑÊòØÔºåCursor ËÉΩÂ§üÈòÖËØªÊï¥‰∏™È°πÁõÆÁöÑ‰ª£Á†ÅÂ∫ìÔºåÂü∫‰∫éÁé∞Êúâ‰ª£Á†ÅÁîüÊàêÊñ∞ÁöÑ‰ª£Á†ÅÁâáÊÆµÔºåÂπ∂‰∏îÊîØÊåÅÂêåÊó∂‰∏∫Â§ö‰∏™Êñá‰ª∂ÁîüÊàê‰ª£Á†Å„ÄÇÂÆÉËøòÊîØÊåÅÂºïÁî®Â§ñÈÉ®ÊñáÊ°£ÂíåÈìæÊé•ÔºåÈÄöËøá RAG ËÉΩÂäõÊúâÊïàÂº•Ë°•‰∫Ü LLM Áü•ËØÜÂ∫ìÂèØËÉΩ‰∏çÂ§üÂÆåÊï¥ÁöÑÁü≠Êùø„ÄÇ\nÊ†πÊçÆÊàëÁöÑÂÆûË∑µÁªèÈ™åÔºåAI/LLM Âú®ÁºñÁ®ã‰∏≠ÁöÑ‰ºòÂäø‰∏ªË¶Å‰ΩìÁé∞Âú®‰ª•‰∏ãÂá†‰∏™ÊñπÈù¢Ôºö\nÂø´ÈÄüÁîüÊàêÈ°πÁõÆÈ™®Êû∂ Â∏ÆÂä©ÂºÄÂèëËÄÖËøÖÈÄü‰∏äÊâã‰∏çÁÜüÊÇâÁöÑÊäÄÊúØÊ†à Âø´ÈÄüÊü•ËØ¢Âπ∂Ëß£ÂÜ≥Â∏∏ËßÅÈîôËØØ È´òÊïàÊÄªÁªìÂíåËß£Èáä‰ª£Á†Å Âø´ÈÄüÁîüÊàêÊµãËØï‰ª£Á†Å Âø´ÈÄüÁîüÊàêÊñáÊ°£ ‰∏∫‰ª£Á†ÅÂÆûÁé∞Êèê‰æõÂ§öÊ†∑ÂåñÁöÑÊÄùË∑Ø ÁÑ∂ËÄåÔºåAI ÁºñÁ®ãÁõÆÂâç‰ªçÊó†Ê≥ïÂÆåÂÖ®Êõø‰ª£‰∫∫Á±ªÁºñÁ®ãÔºå‰∏ªË¶ÅÂ±ÄÈôêÂú®‰∫éÔºö\nÈ´òÊïàÁöÑ‰ª£Á†ÅËæÖÂä©ÁîüÊàêÈúÄË¶ÅÊâéÂÆûÁöÑÈ¢ÜÂüüÁü•ËØÜ„ÄÇÊâÄË∞ìÁöÑ\u0026quot;Â∞èÁôΩÁî®Êà∑\u0026quot;ÂæÄÂæÄÈöæ‰ª•Â∞ÜÁî®Êà∑ÈúÄÊ±ÇÂáÜÁ°ÆËΩ¨Âåñ‰∏∫ÁºñÁ®ãÈ¢ÜÂüüÁöÑ‰∏ì‰∏öÊ¶ÇÂøµÔºåÂõ†Ê≠§Êó†Ê≥ïÊèê‰æõÊúâÊïàÁöÑ Prompt Êù•ÁîüÊàêÊª°Ë∂≥‰∏öÂä°ÈúÄÊ±ÇÁöÑ‰ª£Á†Å„ÄÇ‰ª•ÊàëÁöÑÂÆûË∑µ‰∏∫‰æãÔºåÊúÄÂàùÊàëÂØπ MUI ÁöÑÁªÑ‰ª∂ÂÆåÂÖ®ÈôåÁîüÔºå‰∏ç‰∫ÜËß£ÁªÑ‰ª∂ÂêçÁß∞ÂèäÂÖ∂ÂèØËÉΩÂëàÁé∞ÁöÑËßÜËßâÊïàÊûú„ÄÇËøôÂØºËá¥ÊàëÊó†Ê≥ï‰ªÖÈÄöËøáÊèèËø∞È°µÈù¢Ê†∑ÂºèÈúÄÊ±ÇÊù•ÂÆûÁé∞È¢ÑÊúüÁöÑÈ°µÈù¢ÊïàÊûúÂíåÂ∏ÉÂ±Ä„ÄÇÈÄöËøáÊ∑±ÂÖ•Â≠¶‰π† MUI ÊñáÊ°£ÔºåÈÄêÊ≠•ÁÜüÊÇâÂÖ∂ÁªÑ‰ª∂‰ΩìÁ≥ªÂêéÔºåÊàëÂºÄÂßãÂú® Prompt ‰∏≠Á≤æÁ°ÆÊåáÂÆöÁªÑ‰ª∂ÂêçÁß∞„ÄÅÊ†∑ÂºèÁâπÊÄßÔºåÁîöËá≥ÈôÑ‰∏ä MUI ÁªÑ‰ª∂ÊñáÊ°£ÈìæÊé•ÔºåËøôÊâçÂÆûÁé∞‰∫ÜÁêÜÊÉ≥ÁöÑÈ°µÈù¢ÊïàÊûú„ÄÇ Âè¶‰∏Ä‰∏™ÂÖ∏ÂûãÊ°à‰æãÊòØÂú®ÂÆûÁé∞Âê¨ÂÜôÂäüËÉΩÊó∂ÔºåÊàëÈúÄË¶ÅÂÆûÁé∞‰∏Ä‰∏™Â§çÊùÇÁöÑÊí≠ÊîæÊéßÂà∂ÈÄªËæëÔºöÂçïËØçÂèØ‰ª•Ë¢´ËÆæÁΩÆ‰∏∫Èó¥Èöî‰∏ÄÊÆµÊó∂Èó¥ÂêéÈáçÂ§çÊí≠ÊîæÔºå‰ΩÜÂΩìÁî®Êà∑ÂÆåÊàêÂΩìÂâçÂçïËØçËæìÂÖ•ËΩ¨ÂÖ•‰∏ã‰∏ÄËØçÊó∂ÔºåÈúÄË¶Å‰∏≠Êñ≠‰πãÂâçÊú™ÂÆåÊàêÁöÑÊí≠ÊîæË∞ÉÂ∫¶„ÄÇ‰ªÖÈÄöËøáÊñáÂ≠óÊèèËø∞ËøôÊ†∑Â§çÊùÇÁöÑÈÄªËæëÔºåAI Èöæ‰ª•ÁîüÊàêÊª°ÊÑèÁöÑ‰ª£Á†Å„ÄÇËÄåÂØπ‰∫éÂÖ∑Â§áÁõ∏ÂÖ≥ÁºñÁ®ãÁü•ËØÜÂíåÊï∞ÊçÆÁªìÊûÑÂü∫Á°ÄÁöÑÂºÄÂèëËÄÖÊù•ËØ¥ÔºåÂæàÂÆπÊòìÊÉ≥Âà∞‰ΩøÁî®ÈòüÂàóÊù•ÁÆ°ÁêÜÂæÖÊí≠ÊîæÁöÑÂçïËØçÔºåÈÄöËøáÊ∏ÖÁêÜÈòüÂàóÊù•ÂÆûÁé∞Êí≠Êîæ‰∏≠Êñ≠ÁöÑÊéßÂà∂„ÄÇ\nLLM Ê®°ÂûãÊìÖÈïøÊ£ÄÁ¥¢ÂíåÂåπÈÖçÂ∑≤Êúâ‰ø°ÊÅØÔºåËÄå‰∏çÊòØÈÄöËøáÊé®ÁêÜÊé¢Á¥¢ÈóÆÈ¢òÁöÑÊ†πÊú¨ÂéüÂõ†„ÄÇËøôÊÑèÂë≥ÁùÄÂΩìÁî®Êà∑Âíå LLM ÈÉΩÁº∫‰πèÁõ∏ÂÖ≥Áü•ËØÜÊó∂ÔºåÂèØËÉΩ‰ºöÂú®ÈîôËØØÁöÑËß£ÂÜ≥ÊÄùË∑Ø‰∏äÂèçÂ§çÂ∞ùËØï„ÄÇ‰æãÂ¶ÇÔºåÂú®ÊàëÈÅáÂà∞ÁöÑ Next.js 15.0.x„ÄÅReact 18.x Âíå MUI 6.1.x ÁâàÊú¨‰∏çÂÖºÂÆπÂØºËá¥ÁöÑÈó¥Ê≠áÊÄß Bug ÂíåËøêË°åÊó∂Ë≠¶ÂëäÊó∂ÔºåÂç≥‰æø LLM ÂèÇËÄÉ‰∫ÜÊúÄÊñ∞ÊñáÊ°£Ôºå‰πüÂè™ËÉΩ‰∏çÊñ≠Êèê‰æõÊó†ÊïàÁöÑËß£ÂÜ≥ÊñπÊ°àÔºåËÄåÊ≤°ÊúâÊÑèËØÜÂà∞ÈóÆÈ¢òÁöÑÊú¨Ë¥®Âú®‰∫éËΩØ‰ª∂ÁâàÊú¨ÁöÑÂÖºÂÆπÊÄß„ÄÇ\nLLM ÁöÑÁü•ËØÜÂ∫ìÂèØËÉΩÂ≠òÂú®ÊªûÂêéÊÄßÔºåÂç≥‰ΩøÈÄöËøá RAG Ëé∑Âèñ‰∫ÜÊúÄÊñ∞ËµÑÊñôÔºåÂèóÈôê‰∫éÊ®°ÂûãËÆ≠ÁªÉÊï∞ÊçÆÁöÑÊó∂ÊïàÊÄßÔºå‰ªçÂèØËÉΩÂæóÂá∫ÈîôËØØÁöÑÁªìËÆ∫„ÄÇ‰∏çËøáÔºåÈöèÁùÄÊ®°ÂûãÁöÑÊåÅÁª≠ËøõÊ≠•Âíå RAG ËÉΩÂäõÁöÑÂ¢ûÂº∫ÔºåËøô‰∏™ÈóÆÈ¢òÊúâÊúõÂæóÂà∞ÈÄêÊ≠•ÊîπÂñÑ„ÄÇ\nÊàëÁöÑËßÇÁÇπÊòØÔºåAI ÁºñÁ®ãÁõÆÂâçËøò‰∏çËÉΩÂÆåÂÖ®Âèñ‰ª£‰∫∫Á±ªÁºñÁ®ãÔºå‰ΩÜÂ∑≤ÁªèÂèØ‰ª•‰Ωú‰∏∫Âº∫ÊúâÂäõÁöÑËæÖÂä©Â∑•ÂÖ∑„ÄÇÂØπ‰∫éÊñ∞ÊâãÊù•ËØ¥ÔºåAI ËæÖÂä©ÁºñÁ®ãËÉΩÂ∏ÆÂä©‰ªñ‰ª¨Êõ¥Âø´Âú∞‰∏äÊâãÈôåÁîüÁöÑÊäÄÊúØÊ†àÔºå‰ΩÜË¶ÅËß£ÂÜ≥Â§çÊùÇÊàñÈùûÊ†áÂáÜÁöÑÈóÆÈ¢òÔºå‰ªçÈúÄË¶ÅÂºÄÂèëËÄÖÂÖ∑Â§áÊâéÂÆûÁöÑ‰∏ì‰∏öÁü•ËØÜÔºõËÄåÂØπ‰∫éÁªèÈ™å‰∏∞ÂØåÁöÑÂºÄÂèëËÄÖËÄåË®ÄÔºåÂàôËÉΩÊòæËëóÊèêÂçáÂºÄÂèëÊïàÁéáÔºåÂø´ÈÄüÁîüÊàêÂèØÁî®ÁöÑ‰ª£Á†ÅÊ°ÜÊû∂ÔºåÂπ∂Êèê‰æõÂ§öÊ†∑ÂåñÁöÑÂÆûÁé∞ÊÄùË∑Ø„ÄÇÊúâÁªèÈ™åÁöÑÂºÄÂèëËÄÖÂèØ‰ª•ÁªìÂêàËá™Ë∫´ÁöÑ‰∏ì‰∏öÁü•ËØÜÂíåÊÄùÁª¥ËÉΩÂäõÔºåÈÖçÂêà AI ÁöÑËæÖÂä©ÂäüËÉΩÔºåÊõ¥Âø´ÈÄü„ÄÅÊõ¥È´òË¥®ÈáèÂú∞ÂÆåÊàêÂ§çÊùÇÁöÑÁºñÁ®ã‰ªªÂä°„ÄÇ\n","link":"https://kane.mx/posts/2024/ai-copilot-for-programming/","section":"posts","tags":["Programming","IDE","GenAI","Cline","Aider","Continue","Cursor","Amazon Q Developer","GitHub Copilot","Visual Studio Code","LLM","OpenAI","Amazon Bedrock","Anthropic Claude","Productivity","Next.js","Material UI","Vercel","AWS Amplify"],"title":"AI ÁúüËÉΩÁºñÁ®ã‰∫ÜÂêóÔºü"},{"body":"","link":"https://kane.mx/tags/aider/","section":"tags","tags":null,"title":"Aider"},{"body":"","link":"https://kane.mx/tags/amazon-q-developer/","section":"tags","tags":null,"title":"Amazon Q Developer"},{"body":"","link":"https://kane.mx/tags/anthropic-claude/","section":"tags","tags":null,"title":"Anthropic Claude"},{"body":"","link":"https://kane.mx/tags/continue/","section":"tags","tags":null,"title":"Continue"},{"body":"","link":"https://kane.mx/tags/cursor/","section":"tags","tags":null,"title":"Cursor"},{"body":"","link":"https://kane.mx/tags/ide/","section":"tags","tags":null,"title":"IDE"},{"body":"","link":"https://kane.mx/tags/material-ui/","section":"tags","tags":null,"title":"Material UI"},{"body":"","link":"https://kane.mx/tags/openai/","section":"tags","tags":null,"title":"OpenAI"},{"body":"","link":"https://kane.mx/tags/productivity/","section":"tags","tags":null,"title":"Productivity"},{"body":"","link":"https://kane.mx/tags/programming/","section":"tags","tags":null,"title":"Programming"},{"body":"","link":"https://kane.mx/tags/vercel/","section":"tags","tags":null,"title":"Vercel"},{"body":"","link":"https://kane.mx/tags/visual-studio-code/","section":"tags","tags":null,"title":"Visual Studio Code"},{"body":"","link":"https://kane.mx/tags/clickstream-analytics/","section":"tags","tags":null,"title":"Clickstream Analytics"},{"body":"","link":"https://kane.mx/series/clickstream-analytics/","section":"series","tags":null,"title":"Clickstream-Analytics"},{"body":"In this post, we will explore the observability features of our clickstream solution. Observability is crucial for understanding the health of your data pipeline, identifying issues promptly, and ensuring optimal performance. We'll cover the monitoring, logging, and troubleshooting capabilities built into the solution.\nOverview The clickstream analytics solution incorporates several observability features to help users monitor and maintain their data pipelines:\nLogging Custom CloudWatch dashboards Automated alerting Pipeline health checks Troubleshooting tools Let's delve into each of these components.\nLogging The solution utilizes Amazon CloudWatch Logs and Amazon S3 to centralize logs from various components of the data pipeline and store them cost-effectively. This includes:\nData ingestion service logs, generated from the containers in ECS. You can also enable access logs for the Application Load Balancer when configuring the data pipeline. Data processing job logs, which use Amazon EMR Serverless to process the raw ingestion data and persist the job logs in an S3 bucket. Data modeling workflow logs, which use AWS Step Functions and AWS Lambda to orchestrate the workflow. All these logs are stored in CloudWatch Logs. Lambda functions now support configuring CloudWatch log groups, allowing all logs to be stored in a single CloudWatch log group, facilitating easy searching and analysis across the entire workflow. Web console application logs, which are stored in CloudWatch Logs. Furthermore, if you need to search and analyze logs across the entire data pipeline, consider implementing the AWS solution Centralized Logging with OpenSearch.\nCustom CloudWatch Dashboards The solution automatically creates custom CloudWatch dashboards for each data pipeline. These dashboards provide at-a-glance views of key metrics, including:\nIngestion service health and performance Data processing job health and key metrics statistics Data modeling workflow health and performance Redshift resource usage The challenge lies in the modular design of the data pipeline. The pipeline has different components based on user choices, and even the data modeling on Redshift can be either Redshift Serverless or provisioned Redshift. Therefore, the dashboard is not fixed before the customer creates the data pipeline. The implementation approach is as follows:\nA common metrics stack is deployed before all other components, monitoring the metrics configurations in AWS Systems Manager Parameter Store. Each module (stack) creates one or more metric configurations as standard parameters (which are free but have a 4KB size limit) with the name pattern /Clickstream/metrics/\u0026lt;project id\u0026gt;/\u0026lt;stack id prefix\u0026gt;/\u0026lt;stack id\u0026gt;/\u0026lt;index\u0026gt;. A function in the common metrics stack updates the dashboard based on the metrics specified in the parameters. Figure 1: Example CloudWatch Dashboard for the Data Pipeline with KDS as sink, enabling data processing and data modeling with Redshift Serverless Figure 2: Example CloudWatch Dashboard for the Data Pipeline with MSK as sink, enabling data processing only You can refer to the FAQ How do I monitor the health of the data pipeline for my project? for guidance on using these metrics to monitor the health of your data pipeline.\nAutomated Alerting To proactively notify users of potential issues, the solution sets up built-in CloudWatch Alarms for critical metrics. These alarms can trigger notifications via Amazon SNS, allowing users to respond quickly to problems. Some key alarms include:\nECS Cluster CPU Utilization, used for scaling out/in of the ingestion fleet ECS Pending Task Count, which alarms when the fleet is scaling out Data processing job failures Failures in the workflow of loading data into Redshift Abnormal situations where no data is loaded into Redshift Maximum file age exceeding the data processing interval threshold, which often indicates a failure in the data loading workflow or insufficient Redshift capacity Failures in the workflow of refreshing materialized views Failures in the workflow of scanning metadata Troubleshooting To aid in troubleshooting, consider the following steps:\nReview the Troubleshooting documentation to see if it addresses your issue.\nLog Queries: Query CloudWatch Logs for common troubleshooting scenarios. When encountering an \u0026quot;Internal Server Error\u0026quot; in the solution web console, collect verbose error logs of the Lambda function running the solution web console as follows:\nNavigate to the CloudFormation stack of the solution web console. Click the Resources tab to search for the Lambda function with Logical ID ApiFunction68. Click the link in the Physical ID column to open the Lambda function on the Function page. Choose Monitor and then select View logs in CloudWatch. Refer to Accessing logs with the console for more details. Error Pattern Detection: Use automated analysis of logs to identify common error patterns and suggest potential solutions.\nData Quality Checks: Implement regular checks on processed data to identify potential data quality issues early in the pipeline.\nPerformance Analyzer: Utilize tools to analyze and optimize query performance in Redshift, including recommendations for table design and query structure.\nContact Support: Get expert assistance with this solution if you have an active support plan.\nBest Practices for Pipeline Observability To make the most of these observability features, consider the following best practices:\nRegular Monitoring: Check the CloudWatch dashboards regularly to familiarize yourself with normal patterns and quickly spot anomalies.\nAlert Tuning: Adjust alert thresholds based on your specific use case to minimize false positives while ensuring you catch real issues. Add additional alerts as needed.\nLog Analysis: Use CloudWatch Logs Insights to perform ad-hoc analysis on your logs when troubleshooting specific issues.\nOn-call Integration: Integrate alert notifications with your preferred tools, such as PagerDuty or AWS Chatbot.\nContinuous Improvement: Use the insights gained from observability tools to continuously improve your data pipeline's performance and reliability.\nConclusion Observability is a critical aspect of maintaining a healthy and efficient clickstream analytics pipeline. By leveraging the built-in monitoring, logging, and troubleshooting capabilities of the solution, you can ensure that your data pipeline remains robust and performant, allowing you to focus on deriving insights from your clickstream data.\n","link":"https://kane.mx/posts/deep-dive-clickstream-analytics/pipeline-observability/","section":"posts","tags":["Clickstream Analytics","AWS","Observability","Monitoring","Logging","Troubleshooting"],"title":"Deep Dive Clickstream Analytics Series: Data Pipeline Observability"},{"body":"","link":"https://kane.mx/tags/logging/","section":"tags","tags":null,"title":"Logging"},{"body":"","link":"https://kane.mx/tags/monitoring/","section":"tags","tags":null,"title":"Monitoring"},{"body":"","link":"https://kane.mx/tags/observability/","section":"tags","tags":null,"title":"Observability"},{"body":"","link":"https://kane.mx/series/","section":"series","tags":null,"title":"Series"},{"body":"","link":"https://kane.mx/tags/troubleshooting/","section":"tags","tags":null,"title":"Troubleshooting"},{"body":"","link":"https://kane.mx/tags/amazon-quicksight/","section":"tags","tags":null,"title":"Amazon QuickSight"},{"body":"","link":"https://kane.mx/tags/data-visualization/","section":"tags","tags":null,"title":"Data Visualization"},{"body":"In this post, we will explore the reporting module of our clickstream solution. This module leverages Amazon QuickSight to provide powerful visualization and analysis capabilities for clickstream data, enabling users to gain valuable insights from their data.\nOverview The reporting module is engineered to be flexible, user-friendly, scalable, and highly customizable. It provides a comprehensive suite of capabilities designed to meet diverse analytical needs:\nIntegration with Amazon QuickSight for interactive data visualization and dashboard creation. Pre-built dashboards for common clickstream analytics use cases. A collection of advanced analytics models, including event analysis, funnel analysis, path analysis, retention analysis, and attribution analysis. Custom dashboard creation capabilities. Out-of-the-box Dashboards The clickstream solution comes with a set of pre-built dashboards that provide immediate value to users. These dashboards cover common analytics use cases and are designed to work with the standardized data schema produced by the data processing and data modeling modules.\nOverview Dashboard The out-of-the-box dashboards include:\nOverview: Provides a high-level summary of key metrics and trends. Acquisition: Focuses on user acquisition channels and their performance. Engagement: Analyzes user engagement patterns and metrics. Retention: Examines user retention rates and factors affecting them. Device: Provides insights into the devices, operating systems, and browsers used by your app or website users. Details: Views common and custom dimensions for individual events, and queries all user attributes for a specific user. These dashboards are automatically created and configured when a new application pipeline is registered, allowing users to start gaining insights immediately without any additional setup.\nThe solution leverages the CreateAnalysis API to create the analysis and dashboard programmatically.\nCustom Reporting While the out-of-the-box dashboards cover many common use cases, the solution also provides the flexibility for users to create custom reports tailored to their specific needs.\nThe custom reporting feature leverages Amazon QuickSight's capabilities, allowing users to:\nCreate new visualizations using the rich set of chart types available in QuickSight. Design custom dashboards by combining various visualizations. Use advanced features like parameters, controls, and calculated fields to create interactive and dynamic reports. Join other data sources outside of clickstream data for comprehensive analysis. To facilitate custom reporting, the solution creates a semantic layer in QuickSight, which includes:\nA dataset based on the clickstream_event_base_view table in Redshift. Predefined dimensions and measures derived from the clickstream data. Proper relationships between tables to enable easy joining of data. This semantic layer abstracts the complexity of the underlying data model, making it easier for business users to create reports without needing to write complex SQL queries.\nData Exploration For more advanced users or those who need to perform ad-hoc analysis, the solution provides a data exploration interface within the Analytics Studio.\nData Exploration Interface The data exploration feature allows users to:\nExplore event data with built-in analysis models without SQL and QuickSight knowledge. Create temporary visualizations based on query results. Save and share insights derived from exploratory analysis. This feature is particularly useful for:\nInvestigating specific user behaviors or event patterns. Validating hypotheses about user interactions. Identifying new metrics or dimensions for custom reporting. Performance Optimization To ensure optimal performance of the reporting module, especially when dealing with large volumes of clickstream data, the solution implements several optimization strategies:\nMaterialized Views: Frequently used aggregations and complex joins are pre-computed and stored as materialized views in Redshift, reducing query time for common reporting scenarios.\nPrecalculation: The metrics used in out-of-the-box dashboards are pre-calculated on a daily basis and stored in individual tables.\nCaching: QuickSight's SPICE engine is utilized to cache frequently accessed data, improving dashboard load times. See the FAQ How to speed up the loading of the default dashboard? to enable SPICE for the out-of-the-box dashboard.\nQuery Optimization: The semantic layer in QuickSight is designed with performance in mind, using appropriate aggregations and filters to minimize data scanned during queries.\nSecurity and Access Control The reporting module integrates with the solution's overall security model, ensuring that:\nOnly authorized users can access the dashboards and exploration interface. All data transfers between QuickSight and Redshift are encrypted in transit within the VPC via QuickSight's VPC connection. Conclusion The reporting module of the clickstream analytics solution provides a powerful and flexible way to derive insights from clickstream data. By combining out-of-the-box dashboards, custom reporting capabilities, and a data exploration interface, it caters to a wide range of analytics needs while maintaining performance and security.\n","link":"https://kane.mx/posts/deep-dive-clickstream-analytics/report/","section":"posts","tags":["Clickstream Analytics","AWS","Amazon QuickSight","Reporting","Data Visualization","Business Intelligence"],"title":"Deep Dive Clickstream Analytics Series: Reporting"},{"body":"","link":"https://kane.mx/tags/reporting/","section":"tags","tags":null,"title":"Reporting"},{"body":"","link":"https://kane.mx/tags/chatgpt/","section":"tags","tags":null,"title":"ChatGPT"},{"body":"","link":"https://kane.mx/tags/development-tools/","section":"tags","tags":null,"title":"Development Tools"},{"body":"","link":"https://kane.mx/tags/openai-api/","section":"tags","tags":null,"title":"OpenAI API"},{"body":"Cursor is a powerful AI-assisted programming Integrated Development Environment (IDE) that comes with built-in Large Language Model (LLM) capabilities to aid in coding. With the introduction of Amazon's Bedrock Claude 3.5 model, developers now have access to advanced alternatives. In this post, we'll explore how to set up a custom gateway for Amazon Bedrock with the Claude Sonnet 3.5 foundation model and use it in Cursor as a custom model provider. This approach allows us to harness the cutting-edge language model capabilities of Amazon Bedrock within the familiar Cursor environment, potentially offering enhanced performance and cost-effectiveness for AI-assisted coding tasks.\nSetting Up the Bedrock Access Gateway Follow the steps outlined in this post to set up your Bedrock gateway.\nConfiguring Cursor to Use Bedrock Gateway To configure Cursor to use our Bedrock gateway:\nOpen the full settings of Cursor Navigate to the Models section Input the Bedrock gateway endpoint URL and API key (specified in the previous step) into the OpenAI API Key field Configure Bedrock gateway as OpenAI API server and key Choose any GPT model as your Cursor model. The Bedrock gateway will forward your request to Claude or other LLMs you've configured in the Bedrock gateway. Testing the Integration Try out some prompts in Cursor to ensure everything is working correctly. You should now be using Claude Sonnet 3.5 models via Amazon Bedrock!\nConclusion By setting up this custom gateway, you can leverage the power of Amazon's Bedrock models (such as Anthropic's Claude or Meta's Llama) within your familiar Cursor environment. This approach offers flexibility, potentially lower costs, and the ability to keep your data within the AWS ecosystem.\nRemember to monitor your usage and costs, as Bedrock pricing follows a pay-as-you-go model. Happy coding!\n","link":"https://kane.mx/posts/2024/cursor-meets-bedrock/","section":"posts","tags":["Cursor","LLM","OpenAI API","GenAI","ChatGPT","Anthropic Claude","AWS","Development Tools"],"title":"Using Amazon Bedrock as a Custom OpenAI Server Alternative in Cursor"},{"body":"Many Alfred users enjoy the convenience of the OpenAI ChatGPT workflow for quick AI-powered assistance. However, with the introduction of Amazon's Bedrock Claude 3 and 3.5 models, some may want to leverage these powerful alternatives. In this post, we'll explore how to set up a custom gateway to access Bedrock Claude models instead of the OpenAI API for your Alfred OpenAI workflow.\nWhy Use Bedrock Claude? Potentially lower costs compared to OpenAI's API Access to the latest Claude models Data privacy considerations (your data stays within AWS) Integration with other AWS services Setting Up the Bedrock Access Gateway Bedrock Access Gateway provides OpenAI-compatible RESTful APIs for Amazon Bedrock. It supports streaming responses via server-sent events (SSE) and is compatible with various Bedrock model families, including Anthropic Claude 3 (Haiku/Sonnet/Opus), Claude 3.5 Sonnet, Meta Llama 3, Mistral, and more features.\nTo set up the Bedrock access gateway endpoint in your AWS account, follow the deployment guide. You can choose to deploy using either AWS Lambda or AWS Fargate for Amazon ECS as the compute resource based on your cost and performance trade-off. Once the deployment is complete, you can find the gateway endpoint in the Outputs tab of the CloudFormation stack, as shown below:\nProxy API Base URL aka OPENAI_API_BASE Configuring the Alfred ChatGPT Workflow Now, we need to configure the Alfred ChatGPT workflow to use our Bedrock gateway:\nOpen Alfred Preferences Install the the ChatGPT workflow then choose it in workflow tab Replace the OpenAI API endpoint with your API Gateway URL(NOTE: remove the trailing /v1 from the url got from the stack output) Configure ChatGPT workflow Specify the API key when depoloying the Bedrock Access Gateway as OpenAPI key Testing the Integration Try out some prompts in Alfred to ensure everything is working correctly. You should now be using Bedrock Claude models instead of OpenAI!\nConclusion By setting up this custom gateway, you can leverage the power of Amazon's Bedrock Claude models within your familiar Alfred OpenAI workflow. This approach offers flexibility, potentially lower costs, and the ability to keep your data within the AWS ecosystem.\nRemember to monitor your usage and costs, as Bedrock pricing may differ from OpenAI's. Happy prompting!\n","link":"https://kane.mx/posts/2024/alfred-integration-with-bedrock-claude/","section":"posts","tags":["Alfred","Alfred Workflow","LLM","GenAI","OpenAI API","ChatGPT","Amazon Bedrock","Anthropic Claude","AWS","Productivity"],"title":"Access Bedrock Claude 3/3.5 Models with Alfred OpenAI ChatGPT Workflow"},{"body":"","link":"https://kane.mx/tags/alfred/","section":"tags","tags":null,"title":"Alfred"},{"body":"","link":"https://kane.mx/tags/alfred-workflow/","section":"tags","tags":null,"title":"Alfred Workflow"},{"body":"","link":"https://kane.mx/tags/amazon-redshift/","section":"tags","tags":null,"title":"Amazon Redshift"},{"body":"","link":"https://kane.mx/tags/data-modeling/","section":"tags","tags":null,"title":"Data Modeling"},{"body":"In this post, we will delve into the data modeling module of our clickstream solution. This module is an optional component that creates data models in the Amazon Redshift data warehouse and calculates reporting dimensions based on the event, session, and user factor tables generated in the data processing module.\nOverview Architecture Overview architecture for data modeling module The overview architecture demonstrates how the solution orchestrates loading clickstream data into the Amazon Redshift data warehouse and triggers data modeling within Redshift.\nData Loading Workflow The first workflow in the architecture diagram above illustrates how factor event, session, item, and user records are loaded into the Redshift data warehouse for further processing. The sequence for loading clickstream data into Redshift is as follows:\nRaw event data is processed by Apache Spark, a distributed system that shards the data across multiple nodes and sinks the processed data to S3 without maintaining order. The best approach for processing this sink data is using an event-driven architecture. We subscribe to the ObjectCreated event of the sink S3 bucket in EventBridge, then record the file information in DynamoDB as a to-be-processed item.\nWhen the EMR job execution completes, the Job Success event of the EMR Serverless job execution is emitted via EventBridge. This triggers the load data workflow orchestrated by AWS Step Functions to load the items from the DynamoDB table into Redshift.\nThe workflow mentioned in the previous step parallelly triggers four sub-workflows to load records into the corresponding target tables (event, user, session, and item).\nEach sub-workflow follows the same procedures:\nIt scans the DynamoDB table to find a batch of files to be loaded into the target Redshift table. Once files are found, a manifest file required by the Redshift COPY command is created in the S3 bucket, which is the most efficient way to load batch data into Redshift tables. The workflow submits batch SQL statements to Redshift via the Redshift Data API, then periodically checks the execution status with backoff until completion. When a batch of files is loaded into Redshift, the workflow scans the DynamoDB table again to load another batch if available, continuing until all existing files are loaded. The statements to load data into Redshift tables follow these steps:\nCopy the batch files from S3 into a temporary staging table with the same schema as the target table. The COPY command removes any duplicate records in that batch of data. Use the MERGE command to upsert the records to the target table, avoiding duplicate items. A best practice for using Step Functions to orchestrate long-running workflows is to be aware of the hard limit for Maximum execution history size in standard state machines. To mitigate this limitation, extract steps as sub-workflows when orchestrating long-running processes.\nData Modeling Workflow Once a batch of data is loaded into the tables, the load data workflow asynchronously starts the data modeling workflow without waiting for its completion. The data modeling in the clickstream solution consists of materialized views, self-managed view tables, dimension tables, and stored procedures. The workflow triggers the refreshing of materialized views, incrementally populates data into the clickstream_base_view table (which joins event and session data), and executes stored procedures to calculate daily metric dimensions for out-of-the-box dashboards. For cost-effectiveness, materialized view refreshing and base view population are executed at four-hour intervals by default, even if data loading occurs more frequently. The daily metric calculation is executed on a daily basis by default.\nMetadata Scanning Workflow The final step of the data loading workflow asynchronously triggers another workflow to scan metadata of new events on a daily basis. It aggregates the keys of custom properties for events and users, collecting their top 20 values and counts, then persists this information to the database (DynamoDB) of the web console for further exploratory analysis and metadata management.\nData Expiration Workflow The solution is designed to keep only recent (hot) events in Redshift to reduce data volume during exploratory event analysis. A time-based event scheduler triggers a workflow to clean up aging events as specified by the user.\nRedshift Management The solution supports both provisioned Redshift clusters and Redshift Serverless. Users must provision the Redshift cluster outside of the solution and can choose an existing provisioned Redshift cluster in the same region and account when configuring the pipeline.\nRedshift Serverless is handled slightly differently. The solution provisions a new Redshift Serverless namespace and workgroup, using IAM roles to access Redshift Serverless without creating an administrator user with a password. For information on accessing Redshift Serverless in the query editor, see the question \u0026quot;I already enable data modeling on Redshift, so why can't I see the schema and tables created by this solution in the Redshift query editor?\u0026quot; in the FAQ.\nThe two types of Redshift offer different pricing models for various use scenarios:\nProvisioned Redshift has a fixed cost, suitable for long-running workloads such as streaming ingestion. Redshift Serverless uses a pay-as-you-go model, where you only pay for actual usage. It's ideal for varied workloads. See this post for a deep dive into Redshift Serverless costs and use cases. You can also combine Redshift Serverless and provisioned Redshift to share the same data for optimal performance and cost trade-offs. See the question \u0026quot;How to implement a dedicated Redshift for Analytics Studio?\u0026quot; in the FAQ for information on combining the two types of Redshift in clickstream analysis.\nRedshift Resource Management The solution uses the infrastructure-as-code (IaC) tool AWS CDK to manage cloud resources. After provisioning the data pipeline with the given configuration, users can immediately use the solution without additional manual configurations. This means that Redshift tables, views, materialized views, and other resources are created or updated when the data pipeline is created or updated. The challenge of updating existing materialized views or creating new ones based on large volumes of existing records (tens of billions or more) is time-consuming, potentially taking minutes to hours depending on data volume and overall Redshift cluster load. This could cause the entire pipeline provisioning process to timeout or fail on rollback if Redshift resource updates are synchronized steps in the pipeline creation or update phase.\nTo address this, the solution makes the job of updating Redshift resources an asynchronous workflow, which never blocks pipeline creation or updates. The workflow is designed to be idempotent, allowing safe resumption of the job if it fails due to timeout or other constraints.\nRedshift Schema Best Practices Properly Configure BACKUP Option for MATERIALIZED VIEW When creating materialized views in Redshift that are crucial for persisting data for business purposes, use the BACKUP YES option. This ensures that the materialized view is included in automated and manual cluster snapshots.\nFor provisioned Redshift clusters, this practice is particularly important. When a cluster is maintained due to underlying hardware failure, it typically involves dumping a snapshot and restoring a new cluster from that snapshot. Materialized views created with the BACKUP NO option would be lost in this process.\nUse Proper Sort Key Clickstream events are time-series data, with each event containing a mandatory event_timestamp field of type timestamp. This field serves as the sort key for the event table, distributing records across Redshift clusters. When querying the event table, using the event_timestamp field in filter conditions is crucial. Without specifying this field in query conditions, Redshift would perform a full table scan, significantly impacting performance.\nUse SUPER Type for Semi-structured Data Clickstream events often require the ability to store arbitrary key-value pairs as custom properties for events and users. This presents a challenge for traditional relational databases. A common approach is to create an event_prop table to store these dynamic key-value pairs, like so:\nuuid event_id key value value_type 123e4567-e89b-12d3-a456-426614174000 1 user_id 42 INTEGER 123e4567-e89b-12d3-a456-426614174001 1 session_id abc123 STRING 123e4567-e89b-12d3-a456-426614174002 1 event_type click STRING 123e4567-e89b-12d3-a456-426614174003 2 user_id 43 INTEGER 123e4567-e89b-12d3-a456-426614174004 2 session_id def456 STRING 123e4567-e89b-12d3-a456-426614174005 2 event_type view STRING 123e4567-e89b-12d3-a456-426614174006 2 custom_field my_field_value STRING However, querying custom properties of events becomes extremely slow when joining billions of event records with hundreds of billions of event property records (assuming an event has 10 or more properties).\nThe SUPER type in Redshift offers a solution to this problem. It can contain complex values such as arrays, nested structures, and other complex structures associated with serialization formats like JSON. The SUPER data type is a set of schemaless array and structure values that encompass all other scalar types in Amazon Redshift. Our solution utilizes a SUPER field to represent the custom properties of events and users, containing an object with arbitrary key-value pairs.\n","link":"https://kane.mx/posts/deep-dive-clickstream-analytics/data-modeling/","section":"posts","tags":["Clickstream Analytics","AWS","Amazon Redshift","Data Modeling"],"title":"Deep dive clickstream analytic series: Data Modeling"},{"body":"","link":"https://kane.mx/tags/amazon-emr/","section":"tags","tags":null,"title":"Amazon EMR"},{"body":"","link":"https://kane.mx/tags/apache-spark/","section":"tags","tags":null,"title":"Apache Spark"},{"body":"","link":"https://kane.mx/tags/data-processing/","section":"tags","tags":null,"title":"Data Processing"},{"body":"In this post, we will delve into the data processing module of our clickstream solution. This module is an optional component that normalizes raw clickstream events by cleaning, transforming, and enriching them to fit the standard clickstream data schema defined in the solution. It's designed for flexibility, reliability, high performance, and cost-effectiveness.\nOverview Architecture Overview architecture for data process module The data processing is designed for batch or micro-batch data processing to optimize performance and cost-effectiveness. It's primarily an Apache Spark application running on Amazon EMR Serverless to achieve a balance between high performance and cost. It offers the following capabilities:\nThe data processing job is triggered by a time-based Amazon EventBridge rule. The rule can be set to a fixed interval or any EventBridge rule-supported crontab expression. Users can tune the interval based on their business needs and budget; generally, more frequent processing intervals incur higher costs. A Lambda function is executed when the scheduler is triggered. This function acts as a job submitter, which submits a new job to the EMR Serverless application created in the data pipeline provisioning. It performs the following steps: Scans the S3 bucket storing the raw events collected by the data ingestion service. Estimates the minimum computing resources needed for processing the batch data based on the data volume. It uses the last modified timestamp of S3 objects, which is a strongly consistent system, to find new and previously unprocessed files. Does nothing if no new data files are found in the S3 bucket since the last run. This saves costs, especially in test environments with occasional events. Submits a new job execution to the EMR Serverless application with initial CPU cores, memory size, and timestamp range as the application's arguments. The data processing application, powered by Apache Spark, loads the given data files, cleans, transforms, and enriches the data, and finally writes the results back to the sink S3 bucket. Flexibility - A Pluggable Data Processing Implementation The solution provides flexibility to support multiple SDKs and even unknown third-party clients based on HTTP protocol, as mentioned in the data ingestion section. The data processing application has a pluggable implementation to support other data transform and data enrichment implementations. When introducing a new client that sends clickstream events, you can create a custom data transform implementation to clean and transform the data to the normalized data schema of the solution. You can also specify zero or multiple enrichment implementations to enhance the clickstream event.\nBy default, the solution provides three data transformers to support the official SDKs of the clickstream solution, Google Tag Manager for server-side tagging (you can follow the Guidance for Using Google Tag Manager for Server-Side Website Analytics on AWS to set up GTM server-side servers on AWS), and Sensors Data.\nAdditionally, the solution provides the following built-in enrichments for clickstream events:\nIP enrichment: Uses GeoLite2 Free Geolocation Data by MaxMind to enrich source IP with city, continent, and country information. UA enrichement: Uses ua_parser Java library to enrich User-Agent in the HTTP header with device and browser information. Traffic source enrichment: Uses page referrer, well-known UTM parameters, and configurable mediums to enrich the source, medium, campaign, and Click ID of the traffic for your websites and mobile applications. If you want to analyze your custom data with these built-in benefits, you can refer to this documentation to start developing your transform implementation and custom enrichment implementations.\nData Schema The data processing module processes raw clickstream events, which are mostly JSON data containing one or more client-side events, into the normalized data schema defined in the solution for further data modeling. The solution uses the following four tables to represent clickstream events:\nEvent: This table stores event data. Each record represents an individual event. The event records keep all common properties collected by the SDK and the custom properties specified by the user as a JSON object. Additionally, the data processing application appends a few JSON object columns to collect process info such as source IP, source file, etc. User: This table stores the latest user attributes. Each record represents a visitor (pseudonymous user). Item: This table stores event-item data. Each record represents an event that is associated with an item. Session: This table stores session data. Each record represents a session for each pseudonymous user. See detailed session definition in the SDK manual. Data Storage By default all processed data are stored in S3 bucket forever, you can use the lifecycle of S3 object to automatically deleting the out-dated files and save costs.\nAll processed files are saved in the data lake (S3 bucket) in Parquet format. The prefix of the object path contains the project id and app id for each application in the same project. The data processing module also creates a Glue table for each project when provisioning the pipeline. Users can use Amazon Athena to query the processed data with predefined partitions (app id, year, month, and day) to improve performance and save costs.\nBy default, all processed data is stored in the S3 bucket indefinitely. You can use S3 object lifecycle policies to automatically delete outdated files and save costs.\nCost-effectiveness Due to the nature of batch data processing, we use EMR Serverless to achieve a balance between performance and cost-effectiveness. The EMR Serverless application is not charged when no job is executing.\nSecondly, the job submitter intelligently allocates only the necessary compute resources for job execution to avoid using excessive resources and save costs.\nThird, the EMR Serverless application uses AWS Graviton architecture, providing at least 20% cost savings with better performance compared to using x86_64 architecture, without requiring any code changes!\nBased on the characteristics of batch data processing, larger data volumes and right-sized compute resources can achieve a better performance-cost ratio. In the benchmark conducted by the team, it costs $0.26 per GB when processing an appropriately sized data volume.\n","link":"https://kane.mx/posts/deep-dive-clickstream-analytics/data-processing/","section":"posts","tags":["Clickstream Analytics","AWS","Amazon EMR","Apache Spark","Data Processing"],"title":"Deep dive clickstream analytic series: Data Processing"},{"body":"","link":"https://kane.mx/tags/amazon-ecs/","section":"tags","tags":null,"title":"Amazon ECS"},{"body":"","link":"https://kane.mx/tags/aws-cloudformation/","section":"tags","tags":null,"title":"AWS CloudFormation"},{"body":"","link":"https://kane.mx/tags/container/","section":"tags","tags":null,"title":"Container"},{"body":"In this post, we will delve into the data ingestion service of our clickstream solution. This service is a vital part of the clickstream analytics system. It is designed to be reliable, resilient, high-performing, flexible, and cost-effective. It plays a key role in capturing clickstream data from various sources and delivering it to downstream processing and modeling components.\nOverview Architecture Overview architecture for data ingestion service The data ingestion service of our clickstream solution is a standard web service that can be used by a wide range of clients, including web servers, mobile apps, IoT devices, and third-party platforms. It offers the following capabilities:\nOptional use of AWS Global Accelerator for accelerating end-users' global footprints and fast failover for multi-Region architectures. Application Load Balancer for backend fleet load balancing, SSL certificate offload, basic payload verification, and authentication via OIDC provider. The backend service is hosted on Amazon Elastic Container Service (Amazon ECS), supporting either EC2 or serverless Fargate (in future releases) for container hosting. Two-service backend architecture: Apache Nginx service for processing CORS requests and forwarding incoming events. Worker service powered by open-source Vector for sending events to the configured sink service and enriching events with fields like ingest time and source IP. Support for three different sink services based on user preference: Amazon Kinesis or Apache Kafka for near real-time analysis with seconds latency. Amazon Managed Streaming for Apache Kafka (MSK) for those who prefer a fully managed Apache Kafka service, self-managed Kafka also is supported. Amazon S3 bucket for cost-effective sink when real-time analysis isn't required. Provisioning of MSK Connect instances or AWS Lambda functions to periodically synchronize events from upstream services to S3 bucket when using Kafka or Kinesis as sink destinations. Design Tenets Reliability Reliability is crucial for data ingestion, especially when handling large-scale events. Our service ensures reliability through:\nAuto Scaling groups with warm pools for rapid scaling during traffic spikes. Buffering mechanisms using Amazon Kinesis Data Streams, Amazon MSK, or Amazon S3 to handle sudden traffic spikes and potential downstream processing delays. All sink requests to Kafka and Kinesis are synchronized; the error HTTP status code (500 or greater) will be returned when failing to write an event to the downstream sink destination. All SDKs of clickstream will retry the event-sending request when receiving the error response code from the data ingestion service. However, using Amazon S3 as a sink destination is a bit different. The worker uses an in-memory cache to buffer the incoming events, then persists the batch events (use total event size or time interval as threshold) to the S3 bucket. It might lose a few events if the container or host instance is crashed or restarted before the batch events are persisted. Once the incoming events are persisted in streaming services (Kafka or Kinesis), the solution uses asynchronized jobs to write the events to the S3 bucket to make sure all events are well received. Resilience Our data ingestion service is designed to withstand and recover from various failures and disruptions:\nMulti-AZ deployment for enhanced fault tolerance and high availability. Retry mechanisms with exponential backoff for handling transient failures. High Performance at Scale Optimized for high performance, especially during large scaling events:\nAWS Global Accelerator integration for improved global availability and performance. High-performance ingestion service capable of handling 4000 to 6000 requests per second (RPS) per EC2 server, with horizontal scaling to serve massive data volumes. Due to the ingestion service being stateless, we could use a horizontal scaling policy to serve the massive data volumes. We successfully guided the customer to serve the 500, 000 RPS in a single ingestion fleet. Flexibility The service offers flexibility through:\nMultiple sink options: Amazon S3, Amazon Kinesis Data Streams, and Amazon MSK. Customizable authentication and compatibility with third-party clickstream SDKs or clients. Cost-Effectiveness We ensure cost-effectiveness by:\nLeveraging serverless technologies for a pay-per-use model. Implementing data lifecycle management for optimized storage costs. Utilizing auto-scaling fleet with built-in policies to scale in during low-load periods. Resources Management Resource management is handled through AWS CloudFormation and AWS CDK, with:\nSeparate CloudFormation nested stacks for different sink destinations. Conditional resource management to control sink destination configurations. Shared stack for managing replacement CloudFormation resources to minimize outages during service updates. In conclusion, the data ingestion service is a critical component of our clickstream analytics system, providing reliable, resilient, high-performance, flexible, and cost-effective capabilities for capturing and processing clickstream data. Its robust architecture makes it suitable for businesses of all sizes, from small startups to large enterprises handling massive data volumes.\nStay tuned for our next post, where we'll delve into the data processing module of the clickstream analytics system!\n","link":"https://kane.mx/posts/deep-dive-clickstream-analytics/data-ingestion/","section":"posts","tags":["Clickstream Analytics","AWS","Container","Amazon ECS","AWS CDK","AWS CloudFormation"],"title":"Deep dive clickstream analytic series: Data Ingestion"},{"body":"This post explores the web console module of the clickstream solution.\nThe web console allows users to create and manage projects with their data pipeline, which ingests, processes, analyzes, and visualizes clickstream data. In version 1.1, the Analytics Studio was introduced for business analysts, enabling them to view metrics dashboards, explore clickstream data, design customized dashboards, and manage metadata without requiring in-depth knowledge of data warehouses and SQL.\nOne code base for different architectures The web console is a web application built using AWS serverless technologies, as demonstrated in the Build serverless web application with AWS Serverless series.\nUse CloudFront, S3 and API Gateway for hosting web console Another use case is deploying the web console within a private network, such as a VPC, to meet compliance requirements. In this architecture, CloudFront, S3, and API Gateway are replaced by an internal application load balancer and Lambda functions running within the VPC.\nUse Application Load Balancer and Lambda for hosting web console privately When implementing those two deployment modes, using the AWS Lambda Web Adapter allows for sharing the same code base to gracefully process the events sent from both API Gateway and Application Load Balancer.\nAuthentication and Authorization The web console supports two deployment modes for authentication:\nIf the AWS region has Amazon Cognito User Pool available, the solution can automatically create a Cognito user pool as an OIDC provider. If the region does not have Cognito User Pool, or the user wants to use existing third-party OIDC providers like Okta or Keycloak, the solution allows specifying the OIDC provider information. For the API layer, a custom authorizer is used when the API is provided by API Gateway, and a middleware of Express framework is used when the API is provided by Application Load Balancer.\nThe backend code uses the Express framework to implement the authorization of the API, supporting both deployment modes.\nCentralized web console The clickstream tenet utilizes a single web console, which serves as the control plane, to manage one or more isolated data pipelines across any supported region.\nPipeline lifecycle management The web console manages the lifecycle of the project's data pipeline, which is composed of modular components managed by multiple AWS CloudFormation stacks. The web console uses Step Functions workflows to orchestrate the workflow for managing the lifecycle of those CloudFormation stacks. The workflows are abstracted as parallel execution, serial execution and stack execution.\nStack Orchestration - Parallel execution Stack Orchestration - Serial execution Stack Orchestration - Stack execution And the status change events of CloudFormation stacks are emitted to SNS topic via EventBridge, then the message is deliverred to same region or cross region SQS queue for meeting the centralized web console.\nUse SQS \u0026 SNS cross-regions messages Service availability checks The web console provides a wizard that allows users to provision a data pipeline on the cloud based on their requirements, such as pipeline region, VPC, sink type, and data processing interval. Since AWS services have varying regional availability and features, the web console needs to dynamically display the available options based on the service availability, which it checks through the CloudFormation registry, as the pipeline components are managed by CloudFormation.\nBelow is a sample code snippet for checking the availability of key services used in the solution.\n1import { CloudFormationClient, DescribeTypeCommand } from \u0026#39;@aws-sdk/client-cloudformation\u0026#39;; 2 3export const describeType = async (region: string, typeName: string) =\u0026gt; { 4 try { 5 const cloudFormationClient = new CloudFormationClient({ 6 ...aws_sdk_client_common_config, 7 region, 8 }); 9 const params: DescribeTypeCommand = new DescribeTypeCommand({ 10 Type: \u0026#39;RESOURCE\u0026#39;, 11 TypeName: typeName, 12 }); 13 return await cloudFormationClient.send(params); 14 } catch (error) { 15 logger.warn(\u0026#39;Describe AWS Resource Types Error\u0026#39;, { error }); 16 return undefined; 17 } 18}; 19 20export const pingServiceResource = async (region: string, service: string) =\u0026gt; { 21 let resourceName = \u0026#39;\u0026#39;; 22 switch (service) { 23 case \u0026#39;emr-serverless\u0026#39;: 24 resourceName = \u0026#39;AWS::EMRServerless::Application\u0026#39;; 25 break; 26 case \u0026#39;msk\u0026#39;: 27 resourceName = \u0026#39;AWS::KafkaConnect::Connector\u0026#39;; 28 break; 29 case \u0026#39;redshift-serverless\u0026#39;: 30 resourceName = \u0026#39;AWS::RedshiftServerless::Workgroup\u0026#39;; 31 break; 32 case \u0026#39;quicksight\u0026#39;: 33 resourceName = \u0026#39;AWS::QuickSight::Dashboard\u0026#39;; 34 break; 35 case \u0026#39;athena\u0026#39;: 36 resourceName = \u0026#39;AWS::Athena::WorkGroup\u0026#39;; 37 break; 38 case \u0026#39;global-accelerator\u0026#39;: 39 resourceName = \u0026#39;AWS::GlobalAccelerator::Accelerator\u0026#39;; 40 break; 41 case \u0026#39;flink\u0026#39;: 42 resourceName = \u0026#39;AWS::KinesisAnalyticsV2::Application\u0026#39;; 43 break; 44 default: 45 break; 46 }; 47 if (!resourceName) return false; 48 const resource = await describeType(region, resourceName); 49 return !!resource?.Arn; 50}; Optimize package size Node modules hell One crucial best practice when working with serverless functions like AWS Lambda is optimizing the package size. Just like in the analogy shown here, a smaller and more compact package results in faster performance, lower costs, and more efficient resource utilization. However, when we reach the node_modules folder, it's akin to a vast, sprawling structure, representing an unoptimized and bloated package size. This can lead to slower cold starts, higher compute costs, and potential issues with deployment limits. By following best practices such as minimizing dependencies, leveraging code bundling and tree-shaking techniques, and optimizing asset handling, you can achieve an optimized package size, ensuring your serverless functions are lean, efficient, and cost-effective.\nOriginally, the solution uses node_prune to remove useless dependencies in node_modules for reducing the lambda size. The package size was reduced to 120MB from 200MB. After introducing new dependencies, the original package size became 300MB. Node_prune could not reduce the size to meet the hard limit of Lambda package size 256 MB. The team introduced a new open-sourced tool vercel/ncc for bundling and tree-shaking the node_modules. Vercel/ncc bundled the solution code and dependencies as a single file, it only 20MB from original 300MB. It‚Äôs amazing!\nOptimized package size One more thing to keep in mind is to maintain the relative path of the configuration files used by your application.\nEmbedded QuickSight dashboards Analytics Studio is a component of the clickstream web console that integrates Amazon QuickSight dashboards into our web application.\nHigh level architecture of embedded QuickSight in web console Embedded QuickSight dashboard In the Exploration of Analytics Studio, the web console will automatically create temporary visuals and dashboards for each query owned by the QuickSight role/user used by the web console, ensuring these temporary resources remain invisible to other QuickSight users.\n","link":"https://kane.mx/posts/deep-dive-clickstream-analytics/web-console/","section":"posts","tags":["Clickstream Analytics","AWS","Serverless computing"],"title":"Deep dive clickstream analytic series: Serverless web console"},{"body":"","link":"https://kane.mx/tags/serverless-computing/","section":"tags","tags":null,"title":"Serverless Computing"},{"body":"In the last couple of months, I led a team to build a comprehensive and open-sourced solution that helps customers analyze clickstream events on the cloud. The solution provides data autonomy, allowing users full access to raw data, near real-time ingestion, flexible configurations, and cost-effectiveness. It is a system that utilizes serverless services to cater to various customers, whether small businesses or large-scale events with massive data volumes, offering fully managed services with minimal operational efforts or the flexibility to use preferred open-source technical stacks.\nThe clickstream analytics system typically consists of several modules: SDKs, Data Ingest, Data Processing, Data Modeling and Visualization.\nGeneral clickstream analytics system architecture Building a well-architected and secure cloud-native system with modular, resilient, and cost-effective components is always challenging. The solution provides a production-ready and decoupled implementation, with most modules being optional.\nI will thoroughly explore all modules to deeply dive how to build a cloud-native system and implement a system that supports multiple technical variations and components.\nServerless Web Console Data Ingestion Data Processing Data Modeling Reporting Data Pipeline Observability ","link":"https://kane.mx/posts/deep-dive-clickstream-analytics/preface/","section":"posts","tags":["Clickstream Analytics","AWS","Serverless computing"],"title":"How to build a clickstream analytic system for small businesses to large-scale events"},{"body":"","link":"https://kane.mx/tags/amazon-athena/","section":"tags","tags":null,"title":"Amazon Athena"},{"body":"","link":"https://kane.mx/tags/analytics/","section":"tags","tags":null,"title":"Analytics"},{"body":"In today's digital age, businesses are constantly seeking ways to understand and analyze user behavior on their websites. Clickstream events provide valuable insights into how users interact with a website, and analyzing this data can help businesses make informed decisions to improve user experience and drive conversions.\nClickstream Analytics on AWS collects, ingests, analyzes, and visualizes clickstream events from your websites and mobile applications. The solution manages an ingestion endpoint to receive clickstream events, which are multiple events in a batch sent by the solution‚Äòs SDKs.\nOnce the ingestion endpoint receives the events, they are stored in an Amazon S3 bucket without additional processing. The bucket path is configured as a Glue table in the solution's AWS Glue Data Catalog. So the data is available for analysis using Amazon Athena.\nOne use case is to query and analyze the raw clickstream data to gain immediate insights after the data is stored in the S3 bucket. For example, the operators can debug the clickstream events without waiting for the data to be processed. However, the challenges of querying the raw data are:\nthe clickstream events are compressed by SDKs, so the data is not easily query-able reach the Lambda payload limitation Response payload size exceeded maximum allowed payload size (6291556 bytes) when using Athena UDF to extract the events In this post, I will show you how to use Amazon Athena UDFs to query the raw clickstream data to overcome the challenges.\nThe steps are:\nclone repo: https://github.com/zxkane/aws-athena-query-federation Follow the steps to build and deploy the UDFs as Lambda function. After completing the deployment, find the ARN of Lambda function. Let‚Äôs say it as clickstream-udfs. Go to the console of Glue. Run the below query to load the latest partitions of raw data. 1msck repair table \u0026lt;your project id\u0026gt;.ingestion_events; Run below sample query to view compressed data. 1-- view compressed data 2USING EXTERNAL FUNCTION decompress_clickstream_common_fields(col1 VARCHAR) RETURNS VARCHAR LAMBDA \u0026#39;\u0026lt;your lambda arn\u0026gt;\u0026#39;, 3 EXTERNAL FUNCTION decompress_clickstream_attribute_fields(col1 VARCHAR) RETURNS VARCHAR LAMBDA \u0026#39;\u0026lt;your lambda arn\u0026gt;\u0026#39;, 4 EXTERNAL FUNCTION decompress_clickstream_user_fields(col1 VARCHAR) RETURNS VARCHAR LAMBDA \u0026#39;\u0026lt;your lambda arn\u0026gt;\u0026#39; 5SELECT 6 json_parse(decompress_clickstream_user_fields(data)), 7 json_parse(decompress_clickstream_common_fields(data)), 8 json_parse(decompress_clickstream_attribute_fields(data)) 9FROM \u0026#34;\u0026lt;your project id\u0026gt;\u0026#34;.\u0026#34;ingestion_events\u0026#34; 10WHERE year=\u0026#39;2024\u0026#39; and month=\u0026#39;06\u0026#39; and day=\u0026#39;20\u0026#39; and hour=\u0026#39;02\u0026#39; 11limit 10; 12 13-- count the received raw events 14USING EXTERNAL FUNCTION decompress_clickstream_common_fields(col1 VARCHAR) RETURNS VARCHAR LAMBDA \u0026#39;\u0026lt;your lambda arn\u0026gt;\u0026#39;, 15 EXTERNAL FUNCTION decompress_clickstream_attribute_fields(col1 VARCHAR) RETURNS VARCHAR LAMBDA \u0026#39;\u0026lt;your lambda arn\u0026gt;\u0026#39;, 16 EXTERNAL FUNCTION decompress_clickstream_user_fields(col1 VARCHAR) RETURNS VARCHAR LAMBDA \u0026#39;\u0026lt;your lambda arn\u0026gt;\u0026#39; 17SELECT 18 sum(json_array_length(json_parse(decompress_clickstream_common_fields(data)))) 19FROM \u0026#34;\u0026lt;your project id\u0026gt;\u0026#34;.\u0026#34;ingestion_events\u0026#34; 20WHERE year=\u0026#39;2024\u0026#39; and month=\u0026#39;06\u0026#39; and day=\u0026#39;20\u0026#39; and hour=\u0026#39;02\u0026#39;; This conclusion summarizes the key benefits of using Amazon Athena UDFs for querying raw clickstream data, provides some final thoughts and considerations.\nImmediate access to data: You can analyze clickstream events as soon as they're stored in the S3 bucket, without waiting for additional processing. Debugging capabilities: Operators can quickly debug clickstream events by directly querying the raw data. Overcoming compression challenges: The UDFs allow you to decompress and parse the data on-the-fly, making it easily queryable. Avoiding Lambda payload limitations: By using separate UDFs for different parts of the data, you can circumvent the Lambda payload size restrictions. ","link":"https://kane.mx/posts/2024/analyzing-clickstream-events-using-amazon-athena-udfs/","section":"posts","tags":["Amazon Athena","Analytics","Athena UDF","Clickstream Analytics","AWS","AWS Lambda"],"title":"Analyzing Clickstream Events Using Amazon Athena UDFs"},{"body":"","link":"https://kane.mx/tags/athena-udf/","section":"tags","tags":null,"title":"Athena UDF"},{"body":"","link":"https://kane.mx/tags/aws-lambda/","section":"tags","tags":null,"title":"AWS Lambda"},{"body":"","link":"https://kane.mx/tags/ci/","section":"tags","tags":null,"title":"CI"},{"body":"","link":"https://kane.mx/tags/clean-code/","section":"tags","tags":null,"title":"Clean Code"},{"body":"","link":"https://kane.mx/tags/devops/","section":"tags","tags":null,"title":"DevOps"},{"body":"","link":"https://kane.mx/tags/github-actions/","section":"tags","tags":null,"title":"Github Actions"},{"body":"As developers, we all know the importance of maintaining high code quality standards. One powerful tool that can help us achieve this is SonarQube, a renowned platform for continuous code quality inspection. However, setting up and maintaining a dedicated SonarQube instance can be a cumbersome task, requiring significant resources and ongoing maintenance.\nFortunately, GitHub Actions offers a convenient solution by allowing us to spin up an ephemeral (short-lived) SonarQube instance directly within our workflow. This approach streamlines the process, eliminating the need for a permanent SonarQube server while still reaping the benefits of its code analysis capabilities.\nWhy Ephemeral SonarQube? Using an ephemeral SonarQube instance in your GitHub Actions workflow provides several advantages:\nNo Infrastructure Overhead: With ephemeral SonarQube, you don't need to worry about provisioning and maintaining a dedicated server or virtual machine for SonarQube. This reduces infrastructure costs and management overhead. Scalability: Ephemeral instances can be easily spun up and torn down as needed, making the process highly scalable and adaptable to your project's requirements. Consistent Environment: By running SonarQube within the GitHub Actions environment, you ensure a consistent and reproducible analysis environment across all your builds. For open-source projects, it validates every Pull Request without requiring a SonarQube instance and permissions for contributors. Secure and Isolated: Each ephemeral SonarQube instance is isolated and secure, reducing the risk of cross-contamination or security vulnerabilities. Setting up Ephemeral SonarQube in GitHub Actions Setting up an ephemeral SonarQube instance in your GitHub Actions workflow is a straightforward process. Here's a high-level overview of the steps involved:\nDefine a GitHub Actions Workflow: Create a new workflow file (e.g., .github/workflows/sonar-check.yml) in your repository. Configure SonarQube Instance as a service: Run SonarQube instance as a service container. Specify the instance details, including the version and edition (community or enterprise) you want to use. Then, configure the custom quality profiles and quality gate for meeting the code quality standards of your project. Build Your Project and Run Tests with Reports: Build your code and run tests for coverage reports, if applicable. Trigger Code Analysis: Use the sonarsource/sonarqube-scanner-action or another community action using SonarQube Scanner CLI to run the SonarQube Scanner against your codebase, configuring any necessary analysis properties or exclusions. Check the Analysis Result: After the analysis is complete, check if the result meets the quality gate or not. Update Analysis Result to Pull Request: Write the SonarQube analysis result to the Pull Request as a new comment. It would be ideal to comment on the new code for any new finding issues. Here's an example of how your GitHub Actions workflow might look:\n1name: code scans 2on: 3 pull_request: {} 4 workflow_dispatch: {} 5jobs: 6 sonarqube: 7 name: sonarqube scan 8 runs-on: \u0026#39;ubuntu-latest\u0026#39; 9 services: 10 sonarqube: 11 image: public.ecr.aws/docker/library/sonarqube:10-community 12 ports: 13 - 9000:9000 14 steps: 15 - uses: actions/checkout@v4 16 - name: Setup Node.js 17 uses: actions/setup-node@v4 18 with: 19 node-version: 20.x 20 - name: Install dependencies 21 run: yarn install --check-files \u0026amp;\u0026amp; yarn --cwd example/ install --check-files 22 - name: Run build and unit tests 23 run: npx projen compile \u0026amp;\u0026amp; npx projen test 24 - name: Configure sonarqube 25 env: 26 SONARQUBE_URL: http://localhost:9000 27 SONARQUBE_ADMIN_PASSWORD: ${{ secrets.SONARQUBE_ADMIN_PASSWORD }} 28 run: | 29 bash .github/workflows/sonarqube/sonar-configure.sh 30 - name: SonarQube Scan 31 uses: sonarsource/sonarqube-scan-action@master 32 env: 33 SONAR_HOST_URL: http://sonarqube:9000 34 SONAR_TOKEN: ${{ env.SONARQUBE_TOKEN }} 35 with: 36 args: \u0026gt; 37 -Dsonar.projectKey=pr-${{ github.event.pull_request.number }} 38 # Check the Quality Gate status. 39 - name: SonarQube Quality Gate check 40 id: sonarqube-quality-gate-check 41 uses: sonarsource/sonarqube-quality-gate-action@master 42 # Force to fail step after specific time. 43 timeout-minutes: 5 44 env: 45 SONAR_TOKEN: ${{ env.SONARQUBE_TOKEN }} 46 SONAR_HOST_URL: http://localhost:9000 47 - uses: phwt/sonarqube-quality-gate-action@v1 48 id: quality-gate-check 49 if: always() 50 with: 51 sonar-project-key: pr-${{ github.event.pull_request.number }} 52 sonar-host-url: http://sonarqube:9000 53 sonar-token: ${{ env.SONARQUBE_TOKEN }} 54 github-token: ${{ secrets.PROJEN_GITHUB_TOKEN }} 55 - name: Comment results and findings on Pull Request 56 uses: zxkane/sonar-quality-gate@master 57 if: always() 58 env: 59 DEBUG: true 60 GITHUB_TOKEN: ${{ secrets.PROJEN_GITHUB_TOKEN }} 61 GIT_URL: \u0026#34;https://api.github.com\u0026#34; 62 GIT_TOKEN: ${{ secrets.PROJEN_GITHUB_TOKEN }} 63 SONAR_URL: http://sonarqube:9000 64 SONAR_TOKEN: ${{ env.SONARQUBE_TOKEN }} 65 SONAR_PROJECT_KEY: pr-${{ github.event.pull_request.number }} 66 with: 67 login: ${{ env.SONARQUBE_TOKEN }} 68 skipScanner: true In this example, the workflow is triggered on pull request events to the main branch. The sonarsource/sonarqube-scanner-action is used to install the SonarQube Scanner and perform the code analysis. The SONAR_TOKEN environment variables is used to authenticate with the ephemeral SonarQube instance and its URL is specified as localhost:9000 or sonarqube:9000 which only could be accessed from the workflow runtime, respectively.\nAfter the analysis is complete, the sonarsource/sonarqube-quality-gate-action is used to check if the code meets the defined quality gate criteria. The customize quality gate is configured in the Configure sonarqube step.\nThe final step comments the results on the Pull Request and adds finding issues as inline comments too.\nYou can check the complete sample in this repo.\nConclusion Incorporating ephemeral SonarQube into your GitHub Actions workflow streamlines the process of continuous code quality inspection. By leveraging the power of SonarQube without the overhead of maintaining a dedicated instance, you can ensure that your codebase adheres to high quality standards with minimal effort.\nGive ephemeral SonarQube a try in your next project and experience the benefits of seamless code analysis and quality assurance within your GitHub Actions workflows.\n","link":"https://kane.mx/posts/2024/scan-your-code-with-ephemeral-sonarqube-in-github-actions/","section":"posts","tags":["Clean Code","SonarQube","Github Actions","CI","DevOps"],"title":"Scan Your Code with Ephemeral SonarQube in GitHub Actions"},{"body":"","link":"https://kane.mx/tags/sonarqube/","section":"tags","tags":null,"title":"SonarQube"},{"body":"","link":"https://kane.mx/tags/amazon-dynamodb/","section":"tags","tags":null,"title":"Amazon DynamoDB"},{"body":"","link":"https://kane.mx/tags/amazon-vpc/","section":"tags","tags":null,"title":"Amazon VPC"},{"body":"Amazon DynamoDB now supports AWS PrivateLink as of March 19, 2024. This feature allows you to securely access DynamoDB from your Amazon Virtual Private Cloud (VPC) without exposing your traffic to the public internet.\nHowever, unlike VPC endpoints for other AWS managed services, the AWS PrivateLink for Amazon DynamoDB does not support the Private DNS feature. This means that if your subnets are configured with only a DynamoDB Interface VPC endpoint, the public DNS name of the DynamoDB service (e.g., dynamodb.us-east-1.amazonaws.com in the us-east-1 region) cannot be resolved in those subnets.\nAs a result, you cannot share the same code to connect to the DynamoDB endpoint via the internet or a Gateway VPC endpoint when using Interface VPC endpoints. Instead, when you create an interface endpoint, DynamoDB generates two types of endpoint-specific DNS names: Regional and zonal. You must specify your own endpoint information when creating the DynamoDB client.\n1# replace the Region us-east-1 and VPC endpoint ID https://vpce-1a2b3c4d-5e6f.dynamodb.us-east-1.vpce.amazonaws.com with your own information. 2ddb_client = session.client( 3service_name=\u0026#39;dynamodb\u0026#39;, 4region_name=\u0026#39;us-east-1\u0026#39;, 5endpoint_url=\u0026#39;https://vpce-1a2b3c4d-5e6f.dynamodb.us-east-1.vpce.amazonaws.com\u0026#39; 6) As an experienced AWS developer, it's easy to assume that the newly launched DynamoDB Interface VPC endpoint behaves like other AWS managed services, allowing you to continue using existing code to initialize the DynamoDB client in isolated subnets. However, this assumption would be incorrect and could lead to issues.üòÇüòÇüòÇ\nMake sure to update your application code to use the endpoint-specific DNS names or the endpoint URL when working with DynamoDB Interface VPC endpoints. You can find more examples in the AWS documentation.\n","link":"https://kane.mx/posts/2024/dynamodb-interface-vpc-endpoint/","section":"posts","tags":["AWS","Amazon DynamoDB","Amazon VPC","Tip"],"title":"Avoiding Pitfalls When Using Amazon DynamoDB Interface VPC Endpoints"},{"body":"","link":"https://kane.mx/tags/tip/","section":"tags","tags":null,"title":"Tip"},{"body":"","link":"https://kane.mx/series/effective-cloud-computing/","section":"series","tags":null,"title":"Effective-Cloud-Computing"},{"body":"Serverless computing is all the rage, promising pay-as-you-go magic and freedom from infrastructure woes. But what about serverless for data warehouses? Let's delve into the fascinating (and sometimes confusing) world of Redshift Serverless: its cost structure, ideal use cases, and situations where it might not be the best fit.\nCost Breakdown: Beyond the Illusion of Free Redshift Serverless offers a compelling promise: only pay for what you use. But like any good magic trick, there's more to the story. Here's the primary cost breakdown:\nCompute Units (RPUs): You're charged per second for used compute capacity. This is fantastic for burst workloads, but beware of idle charges. Even when your warehouse is inactive, the base capacity incurs costs. It's with a 60-second minimum charge, even just one query is executed in a second in the charge period. Storage: Redshift Managed Storage (RMS) charges apply to the data you store, regardless of serverless or provisioned clusters. Data Transfer: Cross-region data sharing or accessing data from other AWS services like S3, Glue, etc outside the region attract data transfer charges. By breaking down the cost of Redshift serverless, the RPUs usage majorly impacts the cost. Let's see a few examples of how to analyze the cost of your Redshift serverless.\nRedshift serverless uses SYS_SERVERLESS_USAGE to view details of Amazon Redshift serverless usage of resources. After selecting some rows from the tables, it looks like below,\nstart_time end_time compute_seconds compute_capacity data_storage cross_region_transferred_data charged_seconds 2024-02-24 16:34:00 2024-02-24 16:35:00 62 8 31224 0 480 2024-02-24 16:33:00 2024-02-24 16:34:00 48 8 31218 0 0 2024-02-24 16:30:00 2024-02-24 16:31:00 0 0 31218 0 480 2024-02-24 16:29:00 2024-02-24 16:30:00 13 8 31217 0 0 2024-02-24 16:28:00 2024-02-24 16:29:00 0 0 31217 0 480 2024-02-24 16:27:00 2024-02-24 16:28:00 29 8 31210 0 480 From the above records, we know the Redshift serverless is configured with 8 RPUs (minimum RPUs). Every minute for the active Redshift serverless, 480 seconds (60 * 8 RPUs) are charged for the compute units of the Redshift serverless, though the actual usage of compute-seconds is small!\nYou can use the below query to view the percentage of actual computed seconds for your queries in the charged seconds.\n1-- query actual compute usage vs charged usage per day for Redshift Serverless running in us-west-2 2select 3 DATE_TRUNC(\u0026#39;day\u0026#39;, start_time) AS query_day, 4 sum(compute_seconds) as used_compute_seconds, sum(charged_seconds) as charged_seconds, 2/3 as utility_percentage, 5 sum(charged_seconds)*0.360/3600 as cost 6from sys_serverless_usage where CAST(start_time AS TIMESTAMP) \u0026gt;= CURRENT_DATE - INTERVAL \u0026#39;7 days\u0026#39; 7GROUP BY 1; If you want to which queries are charged in a specific period, leverage SYS_QUERY_HISTORY to view details of user queries. You could join those two tables to see the relationship like below example,\n1with query_info as ( 2 select 3 start_time, 4 end_time, 5 (\u0026#39;[ user_id: \u0026#39; || user_id || \u0026#39; query_id: \u0026#39; || query_id || \u0026#39; transaction_id: \u0026#39; || transaction_id || \u0026#39; session_id: \u0026#39; || session_id ||\u0026#39; - queries: \u0026#39; || SUBSTRING(btrim(query_text), 1, 100) || \u0026#39; ]\u0026#39;) as per_query_info 6 from sys_query_history where start_time ilike \u0026#39;%2024-02-24%\u0026#39; 7 order by start_time 8) 9select 10 syu.start_time, 11 syu.end_time, 12 compute_capacity, 13 charged_seconds, 14 listagg(per_query_info, \u0026#39;,\u0026#39;) as queries_within 15from sys_serverless_usage syu 16inner join query_info sqh 17on sqh.end_time \u0026lt;= syu.end_time 18and sqh.end_time \u0026gt;= syu.start_time 19group by 1,2,3,4; By analyzing the queries, you can evaluate the performance of Redshift serverless and identify the most expensive queries for optimization.\nUse Cases: Where Serverless Shines Redshift Serverless shines in specific scenarios:\nThe intensive queries in a short period (like massive BI queries in few hours). Ad-hoc analytics: Need to run quick queries on your data without spinning up a cluster? Serverless is perfect. Dev/test environments: Test your data pipeline and queries without managing infrastructure. Unpredictable workloads: For workloads with variable demand, serverless scales automatically, saving you from overprovisioning costs. When to Say No: Serverless Isn't for Everyone While tempting, serverless isn't always the answer. Consider these situations:\nLong-running queries: Serverless charges per second, making it less cost-effective for long-running queries compared to provisioned clusters. For example, streaming ingestion from Kinesis data stream or Kafka. Cost-sensitive workloads: If strict budget control is crucial, the base capacity charge and potential idle costs might outweigh the benefits. Conclusion: Choose Wisely Redshift Serverless offers a powerful, flexible option for specific data warehouse needs. However, understanding its cost structure and ideal use cases is crucial to avoid surprises. Carefully evaluate your workload characteristics and budget constraints before diving in. Remember, the magic of serverless lies in using it wisely!\nBonus Tip: Explore hybrid approaches, combining serverless for ad-hoc queries with provisioned clusters for predictable workloads via data sharing feature.\nI hope this blog post helps you navigate the world of Redshift Serverless! Do you have any questions or experiences to share? Let's discuss in the comments!\n","link":"https://kane.mx/posts/2024/redshift-serverless-cost-deep-dive/","section":"posts","tags":["AWS","Amazon Redshift","Serverless Computing"],"title":"Redshift Serverless: Cost Deep Dive and Use Cases"},{"body":"AWS CDK accelerates cloud development using common programming languages to model your applications. I had a series of posts using CDK to demonstrate Building serverless web applications with AWS Serverless. Because CDK uses a programming language to model your application, you can encapsulate your library via Constructs, and then reuse it crossing the entire application.\nMeanwhile, you can create your own constructs to encapsulate the compliance requirements to simplify the code. For example, in our solution, I used the construct SolutionFunction to force using the same Node.js version(18.x), architecture(ARM64), Lambda logging configuration(JSON log), environment variables for Powertools Logger and so on crossing all NodejsFunction. In addition, using Aspects and escape hatches to make sure the application meets the compliance requirements.\nLet's deep dive into how to make all Nodejs Lambda functions compliant with the above requirements.\nFirstly, define the SolutionFunction for making a generic configuration of solutions's Nodejs Lambda,\n1export class SolutionNodejsFunction extends NodejsFunction { 2 3 constructor(scope: Construct, id: string, props?: NodejsFunctionProps) { 4 super(scope, id, { 5 ...props, 6 bundling: props?.bundling ? { 7 ...props.bundling, 8 externalModules: props.bundling.externalModules?.filter(p =\u0026gt; p === \u0026#39;@aws-sdk/*\u0026#39;) ?? [], 9 } : { 10 externalModules: [], 11 }, 12 runtime: Runtime.NODEJS_18_X, 13 architecture: Architecture.ARM_64, 14 environment: { 15 ...POWERTOOLS_ENVS, 16 ...(props?.environment ?? {}), 17 }, 18 logRetention: props?.logRetention ?? RetentionDays.ONE_MONTH, 19 logFormat: \u0026#39;JSON\u0026#39;, 20 applicationLogLevel: props?.applicationLogLevel ?? \u0026#39;INFO\u0026#39;, 21 }); 22 } 23} Then, add an Aspect to the application to make sure the NodejsFunction functions are an instance of SolutionFunction.\n1class NodejsFunctionSanityAspect implements IAspect { 2 3 public visit(node: IConstruct): void { 4 if (node instanceof NodejsFunction) { 5 if (!(node instanceof SolutionNodejsFunction)) { 6 Annotations.of(node).addError(\u0026#39;Directly using NodejsFunction is not allowed in the solution. Use SolutionNodejsFunction instead.\u0026#39;); 7 } 8 if (node.runtime != Runtime.NODEJS_18_X) { 9 Annotations.of(node).addError(\u0026#39;You must use Nodejs 18.x runtime for Lambda with javascript in this solution.\u0026#39;); 10 } 11 } 12 } 13} 14Aspects.of(app).add(new NodejsFunctionSanityAspect()); The above code snippets help us to archive the compliance of Nodejs Lambda functions without modifying tens or hundreds of occurrences one by one.\nHowever, due to service availability, the ARM64 architect and JSON log Lambda function are not available in the AWS China partition. Also, using another Aspect with escape hatches to override the attributes with conditional values.\n1class CNLambdaFunctionAspect implements IAspect { 2 3 private conditionCache: { [key: string]: CfnCondition } = {}; 4 5 public visit(node: IConstruct): void { 6 if (node instanceof Function) { 7 const func = node.node.defaultChild as CfnFunction; 8 if (func.loggingConfig) { 9 func.addPropertyOverride(\u0026#39;LoggingConfig\u0026#39;, 10 Fn.conditionIf(this.awsChinaCondition(Stack.of(node)).logicalId, 11 Fn.ref(\u0026#39;AWS::NoValue\u0026#39;), { 12 LogFormat: (func.loggingConfig as CfnFunction.LoggingConfigProperty).logFormat, 13 ApplicationLogLevel: (func.loggingConfig as CfnFunction.LoggingConfigProperty).applicationLogLevel, 14 LogGroup: (func.loggingConfig as CfnFunction.LoggingConfigProperty).logGroup, 15 SystemLogLevel: (func.loggingConfig as CfnFunction.LoggingConfigProperty).systemLogLevel, 16 })); 17 } 18 if (func.architectures \u0026amp;\u0026amp; func.architectures[0] == Architecture.arm64) { 19 func.addPropertyOverride(\u0026#39;Architectures\u0026#39;, 20 Fn.conditionIf(this.awsChinaCondition(Stack.of(node)).logicalId, 21 Fn.ref(\u0026#39;AWS::NoValue\u0026#39;), func.architectures)); 22 } 23 } 24 } 25 26 private awsChinaCondition(stack: Stack): CfnCondition { 27 const conditionName = \u0026#39;AWSCNCondition\u0026#39;; 28 // Check if the resource already exists 29 const existingResource = this.conditionCache[stack.artifactId]; 30 31 if (existingResource) { 32 return existingResource; 33 } else { 34 const awsCNCondition = new CfnCondition(stack, conditionName, { 35 expression: Fn.conditionEquals(\u0026#39;aws-cn\u0026#39;, stack.partition), 36 }); 37 this.conditionCache[stack.artifactId] = awsCNCondition; 38 return awsCNCondition; 39 } 40 } 41} 42Aspects.of(app).add(new CNLambdaFunctionAspect()); Alright, using the above two aspects forces the solution to meet the compliance requirements of Lambda functions with the same runtime version, architecture, and logger configuration. \u0026#x1f929; \u0026#x1f604; \u0026#x1f929;\n","link":"https://kane.mx/posts/2024/custom-compliance-for-aws-cdk/","section":"posts","tags":["AWS","AWS CDK","AWS Lambda","Tips"],"title":"Custom compliance implementation in AWS CDK"},{"body":"","link":"https://kane.mx/tags/tips/","section":"tags","tags":null,"title":"Tips"},{"body":"","link":"https://kane.mx/tags/amazon-codewhisperer/","section":"tags","tags":null,"title":"Amazon CodeWhisperer"},{"body":" Disclaimer: the cover image was generated by Amazon Bedrock's Titan Image Generator G1.\nAWS CLI is a swiss knife for orchestrating the operations of AWS resources. Especially, the filter option could help your filter and transform the output then combine with other Linux commands together.\nThis post collects the CLI usages to resolve my AWS operation needs.\nDelete the legacy versions of a service catalog product AWS Service Catalog has default 100 versions per product. Below is a one line command to delete the legacy versions.\n1export PRODUCT_ID=\u0026lt;product-id\u0026gt; 2 3# query the version name starting with \u0026#39;v5.0.0\u0026#39; then show Id and Name only 4aws servicecatalog describe-product --no-paginate --id $PRODUCT_ID --query \u0026#39;ProvisioningArtifacts[?starts_with(Name, `v5.0.0`)].{Id:Id, Name:Name}\u0026#39; 5 6# query the version name contains \u0026#39;v5.0.0-beta\u0026#39; then delete them 7aws servicecatalog describe-product --no-paginate --id $PRODUCT_ID --query \u0026#39;ProvisioningArtifacts[?contains(Name, `v5.0.0-beta`)].Id\u0026#39; |jq -r \u0026#39;.[]\u0026#39; | xargs -I {} aws servicecatalog delete-provisioning-artifact --product-id $PRODUCT_ID --provisioning-artifact-id {} Public all S3 objects with specific prefix 1aws s3 ls s3://$name/$prefix --recursive | awk \u0026#39;{print $4}\u0026#39; | xargs -I {} -n 1 aws s3api put-object-acl --acl public-read --bucket $name --key {} Reset resource policy of CloudWatch logs You might encounter a CloudFormation stack deployment failure due to creating CloudWatch log group with an error message like the one below,\nCannot enable logging. Policy document length breaking Cloudwatch Logs Constraints, either \u0026lt; 1 or \u0026gt; 5120 (Service: AmazonApiGatewayV2; Status Code: 400; Error Code: BadRequestException; Request ID: xxx-yyy-zzz; Proxy: null)\nCloudWatch Logs resource policies are limited to 5120 characters. The remediation is merging or removing useless policies, then updating the resource policies of CloudWatch logs to reduce the number of policies.\nBelow is a sample command to reset resource policy of CloudWatch logs:\nPush Helm chart to all regional ECR repositories Import a local SSH key to all AWS regions Query latest amazon linux2 AMI Delete multiple CloudWatch Log groups Launch an EC2 within default VPC with default security group Add below script in your .zshrc, then run ec2-launch-amazon-linux in terminal to launch a new instance.\nAmazon CodeWhisperer for command line is a new set of capabilities and integrations for AI-powered productivity tool, Amazon CodeWhisperer, that makes software developers more productive in the command line. It can also assist you generating the CLI command based on your natural language inputs.\n","link":"https://kane.mx/posts/2024/awscli-collection/","section":"posts","tags":["AWS","AWS CLI","Amazon CodeWhisperer","Tips","Collections"],"title":"Awesome AWS CLI"},{"body":"","link":"https://kane.mx/tags/aws-cli/","section":"tags","tags":null,"title":"AWS CLI"},{"body":"","link":"https://kane.mx/tags/collections/","section":"tags","tags":null,"title":"Collections"},{"body":" Disclaimer: the cover image was generated by StableDiffusionXL with prompts 'cover image, spring boot, flask framework running in aws lambda'.\nWhen deploying and operating a web application on the cloud, you prefer to use your favorite programming language and web framework. Also, you want to benefit from Serverless technologies for stability, scalability, cost optimization, and operation excellence.\nAWS Lambda Web Adapter is a tool that perfectly meets your expectations. It lifts and shifts the web application based on your preferred language and web framework, including FastAPI, Flask, Django, Express.js, Next.js, Spring Boot, Nginx, PHP, Rust, Golang Gin, Laravel, ASP.NET, and so on! You don't have to change any code to migrate your application to Lambda runtime. It also supports WebSocket and streaming features that work well with your LLM applications.\nAnother use case is that you can orchestrate your cloud infrastructure to support different network topologies without changing any code. Assuming you are ISV, your customers want to deploy your services as both public service and private service. With lambda web adapter, you can share the source code of the service, just orchestrating the AWS services to meet the requirements.\nPublic service pattern: CloudFront + S3 + API Gateway + Lambda You can use CloudFront to publish your entire web service. Using S3 to host all static content of your site and API Gateway with Lambda integration serves as the backend API.\nThe pattern architect for hosting web application as public service Private service pattern: Application Load Balancer (ALB) + Lambda With lambda web adapter, you can deploy your web application with Amazon VPC without exposing it to the internet. In this pattern, we choose ALB as the gateway of network traffic, then forward the different requests to two Lambda functions running web frontend and backend correspondingly.\nThe pattern architect for hosting web application as private service without public access In Clickstream Analytics on AWS solution it applies the above patterns to deploy its web console for different network topologies without changing the code of the web application. Also, the solution implements the above pattern as CDK constructs for replication using,\nCloudFront + S3 + API Gateway + Lambda ALB + Lambda Learns While implementing the above patterns in the Clickstream solution, we learned the below tips for applying them on AWS.\nFor CloudFront + S3 + API Gateway + Lambda Put API Gateway behind CloudFront for the same origin Use CloudFront function to rewrite requests for React Browser Router Can not enable access log of CloudFront in the same region when deploying to opt-in regions For ALB + Lambda The payload size for Lambda behind ALB is 1MB Split the bundled JS into multiple chunks Handle with the authentication and authorization via your Web framework I presented this topic in AWS User Group Taiwan CDK Squad Meetup in Chinese. Below are the slides in the community sharing,\n","link":"https://kane.mx/posts/2023/build-serverless-web-application-with-aws-lambda-web-adapter/","section":"posts","tags":["AWS","AWS CDK","AWS Lambda","Lambda Web Adapter","Serverless Pattern","Serverless","CDK Construct"],"title":"Build serverless web application with AWS Lambda web adapter"},{"body":"","link":"https://kane.mx/series/build-serverless-application/","section":"series","tags":null,"title":"Build-Serverless-Application"},{"body":"","link":"https://kane.mx/tags/cdk-construct/","section":"tags","tags":null,"title":"CDK Construct"},{"body":"","link":"https://kane.mx/tags/lambda-web-adapter/","section":"tags","tags":null,"title":"Lambda Web Adapter"},{"body":"","link":"https://kane.mx/tags/serverless-pattern/","section":"tags","tags":null,"title":"Serverless Pattern"},{"body":"","link":"https://kane.mx/tags/aws-js-sdk/","section":"tags","tags":null,"title":"AWS JS SDK"},{"body":"When programming with the AWS SDK, developers sometimes want to debug a specific HTTP request when invoking an SDK API. Due to the poor documentation of AWS JS SDK v3, it takes a lot of work to find a way to print the verbose logging of AWS SDK by asking it to the LLMs.\nBelow is a practical tip for enabling verbose logging for AWS JS SDK v3.\nSolution 1 - specify a custom logger for AWS SDK clients 1import { DescribeParametersCommand, SSMClient } from \u0026#34;@aws-sdk/client-ssm\u0026#34;; 2import * as log4js from \u0026#34;log4js\u0026#34;; 3 4log4js.configure({ 5 appenders: { out: { type: \u0026#34;stdout\u0026#34; } }, 6 categories: { default: { appenders: [\u0026#34;out\u0026#34;], level: \u0026#34;debug\u0026#34; } }, 7}); 8 9const logger = log4js.getLogger(); 10 11const ssmClient = new SSMClient({ 12 logger: logger, 13}); Solution 2 - use middleware to hook the life cyele of request 1import { DescribeParametersCommand, SSMClient } from \u0026#34;@aws-sdk/client-ssm\u0026#34;; 2 3const logRequestMiddleware = (next: any, _context: any) =\u0026gt; async (args: any) =\u0026gt; { 4 console.log(\u0026#39;Request:\u0026#39;, args.request); 5 return next(args); 6}; 7 8const ssmClient = new SSMClient({ 9}); 10 11ssmClient.middlewareStack.add(logRequestMiddleware, { step: \u0026#39;finalizeRequest\u0026#39; }); See complete working example gist below,\n","link":"https://kane.mx/posts/2023/aws-js-sdk-v3-verbose-logging/","section":"posts","tags":["AWS JS SDK","Tip","AWS"],"title":"Verbose logging for AWS JS SDK v3"},{"body":"","link":"https://kane.mx/tags/amazon-api-gateway/","section":"tags","tags":null,"title":"Amazon API Gateway"},{"body":"","link":"https://kane.mx/tags/amazon-sqs/","section":"tags","tags":null,"title":"Amazon SQS"},{"body":"Application Programming Interfaces(APIs) is a critical part of the web service, Werner Vogel, the CTO of AWS had a great 6 Rules for Good API Design presentation in 2021 re:Invent keynote.\nIn AWS the developers could manage and proxy the APIs via Amazon API Gateway. The developers can use console, CLI, API or IaC code(for example, Terraform/CloudFormation/CDK) to provisioning the API resources on AWS. However some developers might flavor with using OpenAPI specification to define the APIs. It enables multiple services/tools to understand the APIs' specification, such as Postman. Amazon API Gateway supports this use case, you can import the existing OpenAPI definition as API.\nAmazon API Gateway offers two RESTful API products, REST API and HTTP API. Both of those two APIs support importing OpenAPI definition, but they might use different OpenAPI extensions to support different features.\nAnd below example will use infrastructure as code(AWS CDK) to import the OpenAPI definition to the API Gateway APIs. While importing OpenAPI definition, the most challenge is updating the OpenAPI definition with dynamic resources information(for example, IAM role for calling downstream resources of integration) before importing the OpenAPI definition. For AWS CDK(on top of AWS CloudFormation) uses the intrinsic functions of CloudFormation(Fn::Join) to archive it.\nREST API 1 const deployOptions = { 2 stageName: \u0026#39;\u0026#39;, 3 loggingLevel: MethodLoggingLevel.ERROR, 4 dataTraceEnabled: false, 5 metricsEnabled: true, 6 tracingEnabled: false, 7 }; 8 const restOpenAPISpec = this.resolve(Mustache.render( 9 fs.readFileSync(path.join(__dirname, \u0026#39;./rest-sqs.yaml\u0026#39;), \u0026#39;utf-8\u0026#39;), 10 variables)); 11 new SpecRestApi(this, \u0026#39;rest-to-sqs\u0026#39;, { 12 apiDefinition: ApiDefinition.fromInline(restOpenAPISpec), 13 endpointExportName: \u0026#39;APIEndpoint\u0026#39;, 14 deployOptions, 15 }); HTTP API But above solution does not work with HTTP API, because the CloudFormation of HTTP API does not support intrinsic functions of CFN. \u0026#x1f625; The workaround is putting the OpenAPI definition to Amazon S3 firstly, then import it from S3 bucket via CloudFormation. It involves putting the OpenAPI definition with dynamic resource information to S3 bucket before importing the OpenAPI definition from S3. Here I leveage the CDK built-in custom resource to call S3 API to put the OpenAPI definition file to S3.\n22/11/09 UPDATE: The Body of AWS::ApiGatewayV2::Api only supports the json object. It works after converting the Yaml OpenAPI definition to JSON!\n1const yaml = require(\u0026#39;js-yaml\u0026#39;); 2 3... 4 5 // import openapi as http api 6 const variables = { 7 integrationRoleArn: apiRole.roleArn, 8 queueName: bufferQueue.queueName, 9 queueUrl: bufferQueue.queueUrl, 10 }; 11 const openAPISpec = this.resolve(yaml.load(Mustache.render( 12 fs.readFileSync(path.join(__dirname, \u0026#39;./http-sqs.yaml\u0026#39;), \u0026#39;utf-8\u0026#39;), variables))); 13 14 const httpApi = new CfnApi(this, \u0026#39;http-api-to-sqs\u0026#39;, { 15 body: openAPISpec, 16 failOnWarnings: false, 17 }); The example code creates both REST API and HTTP API, both of them forwards the events to Amazon SQS queue that are sent by HTTP POST requests. See OpenAPI definition of HTTP to SQS, OpenAPI definition of REST to SQS or complete source for further reference.\n","link":"https://kane.mx/posts/2022/import-oas-as-api-on-aws/","section":"posts","tags":["Serverless","Amazon API Gateway","OpenAPI","OAS","Amazon SQS","AWS","AWS CDK"],"title":"Define your API via OpenAPI definition on AWS"},{"body":"","link":"https://kane.mx/tags/oas/","section":"tags","tags":null,"title":"OAS"},{"body":"","link":"https://kane.mx/tags/openapi/","section":"tags","tags":null,"title":"OpenAPI"},{"body":"","link":"https://kane.mx/tags/codepipeline/","section":"tags","tags":null,"title":"CodePipeline"},{"body":"","link":"https://kane.mx/tags/continuous-deployment/","section":"tags","tags":null,"title":"Continuous Deployment"},{"body":"DevOps pipeline is a key component of project operation, it helps you automate steps in your software delivery process.\nAmazon itself has rich expirence on DevOps with large scale services, it shares the lesson and learn from operating the Amazon's services. You can read this summary post written in Chinese.\nAlso AWS provides fully managed SaaS services for the lifecycle of software development, including AWS CodePipeline for automating continuous delivery pipelines, AWS CodeCommit for securely hosting highly scalable private Git repositories, AWS CodeArtifact for artifact management, AWS CodeBuild for building and testing code with continuous scaling and AWS CodeDeploy for automating code deployments to maintain application uptime.\nAWS Code series services are feasible to build the different DevOps pipelines to satisfy the customer's requirements. But it's required some work to assemble the building blocks to build the pipeline.\nCDK Pipeline is an abstract to simplify the builder experience to build DevOps pipeline for CDK application. It leveages the Infrastructure as Code and Construct to standarndize and customize the pipeline of CDK application.\nThe pipeline code just has few lines and looks like below,\n1 const connectArn = scope.node.tryGetContext(\u0026#39;SourceConnectionArn\u0026#39;); 2 if (!connectArn) {throw new Error(\u0026#39;Must specify the arn of source repo connection.\u0026#39;);} 3 const oidcSecret: string = scope.node.tryGetContext(\u0026#39;OIDCSerectArn\u0026#39;); 4 if (!oidcSecret) {throw new Error(\u0026#39;Must specify the context \u0026#34;OIDCSerectArn\u0026#34; for storing secret.\u0026#39;);} 5 6 const pipeline = new CodePipeline(this, \u0026#39;Pipeline\u0026#39;, { 7 synth: new ShellStep(\u0026#39;Synth\u0026#39;, { 8 input: CodePipelineSource.connection(\u0026#39;zxkane/cdk-collections\u0026#39;, \u0026#39;master\u0026#39;, { 9 connectionArn: connectArn, 10 codeBuildCloneOutput: true, 11 }), 12 installCommands: [ 13 \u0026#39;git submodule init \u0026amp;\u0026amp; git submodule update \u0026amp;\u0026amp; git submodule sync\u0026#39;, 14 \u0026#39;npm i --prefix serverlesstodo/frontend\u0026#39;, 15 \u0026#39;npm run build --prefix serverlesstodo/frontend\u0026#39;, 16 \u0026#39;yarn --cwd serverlesstodo install --check-files --frozen-lockfile\u0026#39;, 17 ], 18 commands: [ 19 \u0026#39;cd serverlesstodo\u0026#39;, 20 \u0026#39;npx projen\u0026#39;, 21 \u0026#39;npx projen test\u0026#39;, 22 `npx cdk synth serverlesstodo -c OIDCSerectArn=${oidcSecret} -c SourceConnectionArn=${connectArn} -c CognitoDomainPrefix=todolist-userpool-prod`, 23 ], 24 primaryOutputDirectory: \u0026#39;serverlesstodo/cdk.out/\u0026#39;, 25 }), 26 dockerEnabledForSynth: true, 27 codeBuildDefaults: { 28 cache: Cache.local(LocalCacheMode.SOURCE, LocalCacheMode.DOCKER_LAYER), 29 }, 30 synthCodeBuildDefaults: { 31 partialBuildSpec: BuildSpec.fromObject({ 32 version: \u0026#39;0.2\u0026#39;, 33 phases: { 34 install: { 35 \u0026#39;runtime-versions\u0026#39;: { 36 nodejs: 14, 37 }, 38 }, 39 }, 40 }), 41 }, 42 }); 43 44 pipeline.addStage(new TodolistApplication(this, \u0026#39;Prod\u0026#39;, { 45 env: { 46 account: process.env.CDK_DEFAULT_ACCOUNT, 47 region: process.env.CDK_DEFAULT_REGION, 48 }, 49 })); Some key points in above pipeline code snippet,\nthis example code hosts on Github, so using CodeStar connection to fetch code from Github synth of CodePipeline is the configuration of CodeBuild project, it installs the dependencies of project then build, test and generate the deployment artifacts(CloudFormation template), see docs of deploying from source the CDK pipeline has built-in mutation step to update pipeline itself before deploying the application After deploying the pipeline stack, the screenshot of pipeline looks like below, Todolist app pipeline As usual, all AWS resources are orchestrated by a AWS CDK project, it's easliy to be deployed to any account and any region of AWS!\nHappying continuously deploy your application \u0026#x1f680; \u0026#x1f606;\u0026#x1f606;\u0026#x1f606;\n","link":"https://kane.mx/posts/2022/build-serverless-app-on-aws/devops-pipeline/","section":"posts","tags":["Serverless","AWS","AWS CDK","CodePipeline","DevOps","Continuous Deployment"],"title":"Setup DevOps pipeline with few code"},{"body":"","link":"https://kane.mx/tags/amplify/","section":"tags","tags":null,"title":"Amplify"},{"body":"","link":"https://kane.mx/tags/cognito/","section":"tags","tags":null,"title":"Cognito"},{"body":"When working on either 2C application or 2B service, the customers do not want to or is not allowed to sign up the new account, they can login the application via existing IdP or enterprise SSO. So, building the application supports the federated OIDC login to address such requirements.\nThis post extends the capability of Todolist application protected by Amazon Cognito, using Auth0 as the third party OpenID Connect provider introduces the external user pool.\nThe application also uses the AWS Amplify to build the frontend capabilities(for example, authentication, invoke backend restful api), Amazon Cognito providing both federated OIDC login and self-managed users sign in/sign up, and Amazon API Gateway providing the backend API and validating the token with OIDC provider.\nBelow is the key procedures to add the federated OIDC login to the existing web application protected by Cognito,\n1. Update the authorizer of API Gateway to validate the token issued by OIDC providers.\nThe previous authorizer is using API Gateway Cognito authorizer, it only can validate the token issued by Cognito user pool. Cognito user pool also complies with the OIDC standard, using Lambda authorizer can implement to validate the tokens issued by either Cognito user pool and third party OIDC provider.\nThe CDK code creates a lambda function as Lambda Authorizer of API Gateway, which sets the supported OIDC issuers as environment,\n1 const authFunc = new NodejsFunction(this, `${resourceName}AuthFunc`, { 2 entry: path.join(__dirname, \u0026#39;./lambda.d/authorizer/index.ts\u0026#39;), 3 handler: \u0026#39;handler\u0026#39;, 4 architecture: Architecture.ARM_64, 5 timeout: Duration.seconds(5), 6 memorySize: 128, 7 runtime: Runtime.NODEJS_16_X, 8 tracing: Tracing.ACTIVE, 9 environment: { 10 ISSUERS: issuers, 11 RESOURCE_PREFIX: Arn.format({ 12 service: \u0026#39;execute-api\u0026#39;, 13 resource: api.restApiId, 14 }, Stack.of(this)), 15 }, 16 }); The custom lambda authorizer uses the Auth0's jwt-decode and AWS JWT Verify to verify the ID tokens issued by OIDC provider. See source for detail implementation.\n2. Add the third party OIDC provider to Cognito user pool. It involves the client information with secrets generated by OIDC provider, we use the AWS Secrets Manager to securely store the credentials.\nAs prerequisites of this step, you must create an application in your OIDC provider. For example, creating an application in Auth0, then configure the allowed callback URLs to the pool domain. The next saving the issuer domain, client id, client secret and name(will be readable string in UI) to a secret in Secrets Manager. Todolist app in Auth0 The code snippet of CDK creates the external OIDC provider looks like below,\n1 const oidcSecretArn = this.node.tryGetContext(\u0026#39;OIDCSerectArn\u0026#39;); 2 var oidcProvider: UserPoolIdentityProviderOidc | undefined; 3 if (oidcSecretArn) { 4 const secret = Secret.fromSecretAttributes(this, \u0026#39;OIDCSecret\u0026#39;, { 5 secretCompleteArn: oidcSecretArn, 6 }); 7 oidcProvider = new UserPoolIdentityProviderOidc(this, \u0026#39;FedarationOIDC\u0026#39;, { 8 clientId: secret.secretValueFromJson(\u0026#39;clientId\u0026#39;).toString(), 9 clientSecret: secret.secretValueFromJson(\u0026#39;clientSecret\u0026#39;).toString(), 10 issuerUrl: secret.secretValueFromJson(\u0026#39;issuerUrl\u0026#39;).toString(), 11 name: secret.secretValueFromJson(\u0026#39;name\u0026#39;).toString(), 12 userPool: userpool, 13 scopes: [ 14 \u0026#39;profile\u0026#39;, 15 \u0026#39;openid\u0026#39;, 16 \u0026#39;email\u0026#39;, 17 ], 18 }); 19 userpool.registerIdentityProvider(oidcProvider); 20 } 3. Update the amplify configuration file with OIDC provider information.\n1 const amplifyConfFile = \u0026#39;aws-exports.json\u0026#39;; 2 const body = 3`{ 4 \u0026#34;aws_project_region\u0026#34;: \u0026#34;${Aws.REGION}\u0026#34;, 5 \u0026#34;Auth\u0026#34;: { 6 \u0026#34;region\u0026#34;: \u0026#34;${Aws.REGION}\u0026#34;, 7 \u0026#34;userPoolId\u0026#34;: \u0026#34;${poolInfo.userpool.userPoolId}\u0026#34;, 8 \u0026#34;userPoolWebClientId\u0026#34;: \u0026#34;${poolInfo.client.userPoolClientId}\u0026#34;, 9 \u0026#34;authenticationFlowType\u0026#34;: \u0026#34;USER_SRP_AUTH\u0026#34;, 10 \u0026#34;oauth\u0026#34;: { 11 \u0026#34;name\u0026#34;: \u0026#34;${poolInfo.oidc.name}\u0026#34;, 12 \u0026#34;domain\u0026#34;: \u0026#34;${poolInfo.poolDomain.domainName}.auth.${Aws.REGION}.amazoncognito.com\u0026#34;, 13 \u0026#34;scope\u0026#34;: [\u0026#34;email\u0026#34;, \u0026#34;openid\u0026#34;, \u0026#34;aws.cognito.signin.user.admin\u0026#34;, \u0026#34;profile\u0026#34;], 14 \u0026#34;redirectSignIn\u0026#34;: \u0026#34;${poolInfo.oidc.signinUrl}\u0026#34;, 15 \u0026#34;redirectSignOut\u0026#34;: \u0026#34;${poolInfo.oidc.signinUrl}\u0026#34;, 16 \u0026#34;responseType\u0026#34;: \u0026#34;code\u0026#34; 17 } 18 }, 19 \u0026#34;API\u0026#34;: { 20 \u0026#34;endpoints\u0026#34;: [ 21 { 22 \u0026#34;name\u0026#34;: \u0026#34;backend-api\u0026#34;, 23 \u0026#34;endpoint\u0026#34;: \u0026#34;https://${cloudFrontS3.cloudFrontWebDistribution.distributionDomainName}/prod/\u0026#34; 24 } 25 ] 26 } 27}`; 4. Customize the Amplify's React Authenticator component to add the federated OIDC login entrance.\n1 SignIn: { 2 Footer() { 3 const { toResetPassword } = useAuthenticator(); 4 5 return ( 6 \u0026lt;View textAlign=\u0026#34;center\u0026#34;\u0026gt; 7 \u0026lt;Divider orientation=\u0026#34;horizontal\u0026#34; /\u0026gt; 8 \u0026lt;Text\u0026gt; 9 { 10 !isAuthenticated \u0026amp;\u0026amp; ( 11 \u0026lt;View 12 as=\u0026#34;div\u0026#34; 13 backgroundColor=\u0026#34;var(--amplify-colors-white)\u0026#34; 14 borderRadius=\u0026#34;6px\u0026#34; 15 color=\u0026#34;var(--amplify-colors-blue-60)\u0026#34; 16 height=\u0026#34;4rem\u0026#34; 17 maxWidth=\u0026#34;100%\u0026#34; 18 padding=\u0026#34;1rem\u0026#34; 19 \u0026gt; 20 \u0026lt;Button 21 variation=\u0026#34;primary\u0026#34; 22 onClick={ 23 () =\u0026gt; { 24 Auth.federatedSignIn({ customProvider: oidcProviderName }); 25 }} 26 \u0026gt; 27 Sign In with {oidcProviderName} 28 \u0026lt;/Button\u0026gt; 29 \u0026lt;/View\u0026gt; 30 ) 31 } 32 \u0026lt;/Text\u0026gt; 33 \u0026lt;/View\u0026gt; 34 ); 35 }, 36 }, The new look of Amplify's authoricator component looks like below with both self-managed user pool and federated OIDC login, Todolist federated OIDC login As usual, all AWS resources are orchestrated by a AWS CDK project, it's easliy to be deployed to any account and any region of AWS!\nHappying logging your website with externl OIDC provider \u0026#x1f512; \u0026#x1f606;\u0026#x1f606;\u0026#x1f606;\n","link":"https://kane.mx/posts/2022/build-serverless-app-on-aws/federated-oidc-login-with-cognito-and-amplify/","section":"posts","tags":["Serverless","AWS","AWS CDK","API Gateway","Cognito","Amplify","OpenID Connect","Authentication"],"title":"Federated OIDC login with Cognito and Amplify"},{"body":"","link":"https://kane.mx/tags/openid-connect/","section":"tags","tags":null,"title":"OpenID Connect"},{"body":"","link":"https://kane.mx/tags/authorization/","section":"tags","tags":null,"title":"Authorization"},{"body":"Previous post we demonstrated how distributing and securely deploying the website to global end users. The authentication and authorization are always mandatory features of web application. Amazon Cognito is a managed AWS serverless service helping the applications to implement AuthN and AuthZ, with Cognito the applications securely scales to millions of users(up to 50,000 free users) supporting identity and access management standards, such as OAuth 2.0, SAML 2.0, and OpenID Connect.\nThe web application uses AWS Amplify to integrate with AWS services, such as Cognito and API Gateway. Below the procedures how integrating Cognito as AuthN via Amplify in Todolist project,\nadd amplify JS libraries into your project's dependencies 1{ 2 \u0026#34;name\u0026#34;: \u0026#34;todo-list\u0026#34;, 3 \u0026#34;dependencies\u0026#34;: { 4 \u0026#34;@aws-amplify/ui-react\u0026#34;: \u0026#34;^3.5.0\u0026#34;, 5 \u0026#34;aws-amplify\u0026#34;: \u0026#34;^4.3.34\u0026#34;, 6 \u0026#34;axios\u0026#34;: \u0026#34;^0.27.2\u0026#34;, 7 \u0026#34;react\u0026#34;: \u0026#34;^18.2.0\u0026#34;, 8 \u0026#34;react-dom\u0026#34;: \u0026#34;^18.2.0\u0026#34;, 9 \u0026#34;react-icons\u0026#34;: \u0026#34;^4.4.0\u0026#34;, 10 \u0026#34;sweetalert2\u0026#34;: \u0026#34;^11.4.24\u0026#34;, 11 \u0026#34;uuid\u0026#34;: \u0026#34;^8.3.2\u0026#34; 12 } 13} load the configuration file from server side and configure the Amplify categories 1 useEffect(() =\u0026gt; { 2 setLoadingConfig(true); 3 Axios.get(\u0026#34;/aws-exports.json\u0026#34;).then((res) =\u0026gt; { 4 const configData = res.data; 5 const tokenHeader = async () =\u0026gt; { return { Authorization: `Bearer ${(await Auth.currentSession()).getIdToken().getJwtToken()}` }; }; 6 configData.API.endpoints[0].custom_header = tokenHeader; 7 Amplify.configure(configData); 8 apiEndpointName = configData.API.endpoints[0].name; 9 setApiEndpoint(configData.API.endpoints[0].name); 10 11 Hub.listen(\u0026#39;auth\u0026#39;, ({ payload }) =\u0026gt; { 12 const { event } = payload; 13 switch (event) { 14 case \u0026#39;signIn\u0026#39;: 15 case \u0026#39;signUp\u0026#39;: 16 case \u0026#39;autoSignIn\u0026#39;: 17 getTasks(); 18 break; 19 } 20 }); 21 22 getTasks(); 23 24 setLoadingConfig(false); 25 }); 26 }, []); use [Authenticator component][authenticator] adding complete authentication flows with minimal boilerplate 1 return ( 2 \u0026lt;Authenticator components={components} loginMechanisms={[\u0026#39;email\u0026#39;]}\u0026gt; 3 {({ signOut, user }) =\u0026gt; ( 4 \u0026lt;Flex 5 direction=\u0026#34;column\u0026#34; 6 justifyContent=\u0026#34;flex-start\u0026#34; 7 alignItems=\u0026#34;center\u0026#34; 8 alignContent=\u0026#34;flex-start\u0026#34; 9 wrap=\u0026#34;nowrap\u0026#34; 10 gap=\u0026#34;1rem\u0026#34; 11 textAlign=\u0026#34;center\u0026#34; 12 \u0026gt; 13 \u0026lt;View width=\u0026#34;100%\u0026#34;\u0026gt; 14 ... 15 \u0026lt;/View\u0026gt; 16 \u0026lt;/Flex\u0026gt; 17 )} 18 \u0026lt;/Authenticator\u0026gt; 19 ) update TODO CRUD methods to use Amplify's API catagory to make HTTP requests to API Gateway 1 const getTasks = async () =\u0026gt; { 2 const canEnter = await ionViewCanEnter(); 3 if (canEnter) { 4 try { 5 setLoadingData(true); 6 7 const initData = { 8 headers: { \u0026#34;content-type\u0026#34;: \u0026#34;application/json\u0026#34; }, // OPTIONAL 9 response: true, // OPTIONAL (return the entire Axios response object instead of only response.data) 10 }; 11 API 12 .get(apiEndpointName || apiEndpoint, \u0026#34;/todo\u0026#34;, initData) 13 .then(res =\u0026gt; { 14 setLoadingData(false); 15 const tasksData = res.data; 16 if ((typeof tasksData === \u0026#34;string\u0026#34;)) { 17 Swal.fire(\u0026#34;Ops..\u0026#34;, tasksData); 18 } else { 19 setTasks(tasksData); 20 } 21 }) 22 .catch(error =\u0026gt; { 23 setLoadingData(false); 24 console.error(error); 25 Swal.fire( 26 `${error.message}`, 27 `${error?.response?.data?.message}`, 28 undefined 29 ); 30 }); 31 } catch (error) { 32 console.info(error); 33 } 34 } 35 }; All above changes are implemented Cognito authN with the web react application.\nIn the server-side the Cognito user pool will be provisioned, the API Gateway endpoint is authorized by Cognito user pool authorizer. The Amplify configuration file aws-exports.json will be created on the air when provisioning the stack with the user pool and API information.\nAs usual, all AWS resources are orchestrated by AWS CDK project, it's easliy to be deployed to any account and any region of AWS!\nHappying protecting the website with Cognito \u0026#x1f512; \u0026#x1f606;\u0026#x1f606;\u0026#x1f606;\n","link":"https://kane.mx/posts/2022/build-serverless-app-on-aws/protect-website-with-cognito/","section":"posts","tags":["Serverless","AWS","AWS CDK","API Gateway","Cognito","Amplify","Authentication","Authorization"],"title":"Protect website with Cognito"},{"body":"","link":"https://kane.mx/tags/cloudfront/","section":"tags","tags":null,"title":"CloudFront"},{"body":"It's a well known pattern to distribute the website via CDN globally, it reduces the latency of the site and improve the availibity and security leveraging the infrastructure of cloud provider.\nUsing CDN service CloudFront and simple storage S3 on AWS hosts the static website. It well fits the SPA(single page application) framework technologies, for example, React, Vue and Angularjs. There are lots of existing project and code snippets to sharing this pattern, such as CloudFront to S3 and API Gateway and AWS S3 / React Website Pattern.\nIn the TODO application it reuses an existing project Todolist built by React. The original Todolist application is a pure client application without communicating the backend service. In this demo the Todolist application is updated to communicate with Restful TODO APIs created by Amazon API Gateway. Also the restful backend API is distributed by CDN CloudFront to reduce the latency and protect the origin service without crossing domain request.\nTodolist app The demo uses the aws-cloudfront-s3 construct from AWS Solutions Constructs to simplify orchestrating the CloudFront to S3/API Gateway pattern. And use AWS S3 Deployment Construct Library to publish the static web page to S3 bucket. See below code snippet how archive it in CDK.\nAs usual, all AWS resources are orchestrated by AWS CDK project, it's easliy to be deployed to any account and any region of AWS!\nHappying distributing the website \u0026#x1f310; \u0026#x1f606;\u0026#x1f606;\u0026#x1f606;\n","link":"https://kane.mx/posts/2022/build-serverless-app-on-aws/static-website/","section":"posts","tags":["Serverless","AWS","AWS CDK","CloudFront","S3"],"title":"Distribute the website globally"},{"body":"","link":"https://kane.mx/tags/s3/","section":"tags","tags":null,"title":"S3"},{"body":"Most web applications are using Restful APIs to interactive with the backend services. In the TODO application, it's the straight forward to get, update and delete the items from backend database. Amazon DynamoDB is a key-value database, it fits for this scenario with scalability and optimized pay-as-you-go cost. Also Amazon API Gateway has built-in integration with AWS serivces, the restful API can be transformed to the request to DynamoDB APIs. Using this combination you can provide the restful APIs only provisioning AWS resources without writing the CRUD code!\nLet's assume the TODO application having below model to represent the TODO items,\n1{ 2\u0026#34;subject\u0026#34;: \u0026#34;my-memo\u0026#34;, // some subject of TODO item 3\u0026#34;description\u0026#34;: \u0026#34;the great idea\u0026#34;, // some description for the TODO item 4\u0026#34;dueDate\u0026#34;: 1661926828, // the timestamp of sceonds for the due date of TODO item 5} Then define below restful APIs for list, fetch, update and delete TODO item/items.\nCreate new TODO item 1PUT /todo Update a TODO item 1POST /todo/\u0026lt;todo id\u0026gt; Delete a TODO item 1DELETE /todo/\u0026lt;todo id\u0026gt; List TODO items 1GET /todo All magic with no code restful API of API Gateway is setting up data transformations for REST API.\nBelos is using the Apache VTL to transform the request JSON payload to DynamoDB UpdateItem API request.\nAlso using API Gateway's transformation feature of the response of integration(DynamoDB API in this case) to shape the response like below,\nThere are few best practise of using API Gateway and AWS services integration to simplify the CRUD operations,\nuse request validator to validate the request payload use integration response to handle with the error cases of integration services. Below is an example checking the error message of DynamoDB API then reshape the error message 1#if($input.path(\u0026#39;$.__type\u0026#39;) == \u0026#34;com.amazonaws.dynamodb.v20120810#ConditionalCheckFailedException\u0026#34;) 2{ 3 \u0026#34;message\u0026#34;: \u0026#34;the todo id already exists.\u0026#34; 4} 5#end sanity all string inputs from client via API Gateway built-in $util method $util.escapeJavaScript() to avoid NoSQL injection attack response valid json if the string contains signle quotes(') 1\u0026#34;subject\u0026#34;: \u0026#34;$util.escapeJavaScript($input.path(\u0026#39;$.Attributes.subject.S\u0026#39;)).replaceAll(\\\u0026#34;\\\\\\\\\u0026#39;\\\u0026#34;,\\\u0026#34;\u0026#39;\\\u0026#34;)\u0026#34; As usual, all AWS resources are orchestrated by AWS CDK project, it's easliy to be deployed to any account and any region of AWS!\nHappying üë®‚Äçüíª API \u0026#x1f606;\u0026#x1f606;\u0026#x1f606;\n","link":"https://kane.mx/posts/2022/build-serverless-app-on-aws/restful-api/","section":"posts","tags":["Serverless","AWS","API Gateway","DynamoDB","AWS CDK"],"title":"Build no code restful HTTP API with API Gateway and DynamoDB"},{"body":"Building web application is a common use case, leveraging cloud services could accelerate the builders to develop and deploy the services. With AWS serverless services, the application can easily get the capabilities like security, highly availability, scalability, resiliency and cost optimized.\nThis is a series posts to demonstrate how building a serverless TODO web application on AWS with AWS serverless services and AWS CDK, it consists of,\nRestful HTTP APIs, use Amazon API Gateway and Amazon DynamoDB Securely and accelerately distribute the static website via Amazon CloudFront and Amazon S3 Authentication and Authorization via Amazon Cognito and AWS Amplify Federated OIDC authentication with Amazon Cognito CI/CD DevOps pipeline source code written by AWS CDK to archive above features ","link":"https://kane.mx/posts/2022/build-serverless-app-on-aws/intro/","section":"posts","tags":["Serverless","AWS"],"title":"Build serverless web application with AWS Serverless"},{"body":"","link":"https://kane.mx/tags/cd/","section":"tags","tags":null,"title":"CD"},{"body":"","link":"https://kane.mx/tags/continuous-delivery/","section":"tags","tags":null,"title":"Continuous Delivery"},{"body":"","link":"https://kane.mx/tags/debugging/","section":"tags","tags":null,"title":"Debugging"},{"body":"","link":"https://kane.mx/tags/flux/","section":"tags","tags":null,"title":"Flux"},{"body":"After enabling E2E testing of FluxCD powered GitOps continuous deployment, the feedback of new commits are quite slow. Because you have to wait for the E2E testing result, lots of time cost on setuping the environment and provisioning your development from scrath.\nInspired by E2E testing in Github actions, the DevOps engineers can build local debugging environment in Kind or minikube.\nBelow is a script how using Kind to provision FluxCD then reconciling the latest commits by FluxCD.\n","link":"https://kane.mx/posts/gitops/fluxcd-local-debug-tip/","section":"posts","tags":["Flux","GitOps","Kubernetes","Git","CD","Continuous Delivery","Debugging"],"title":"FluxCD GitOps debugging tip"},{"body":"","link":"https://kane.mx/tags/git/","section":"tags","tags":null,"title":"Git"},{"body":"","link":"https://kane.mx/series/gitops/","section":"series","tags":null,"title":"Gitops"},{"body":"","link":"https://kane.mx/tags/gitops/","section":"tags","tags":null,"title":"GitOps"},{"body":"","link":"https://kane.mx/categories/kubernetes/","section":"categories","tags":null,"title":"Kubernetes"},{"body":"","link":"https://kane.mx/tags/kubernetes/","section":"tags","tags":null,"title":"Kubernetes"},{"body":"","link":"https://kane.mx/tags/aws-secrets-manager/","section":"tags","tags":null,"title":"AWS Secrets Manager"},{"body":"","link":"https://kane.mx/tags/eks/","section":"tags","tags":null,"title":"EKS"},{"body":"","link":"https://kane.mx/tags/external-secrets-operator/","section":"tags","tags":null,"title":"External Secrets Operator"},{"body":"ËÉåÊôØ ÂØÜÈí•ÁöÑÁÆ°ÁêÜÂØπ‰∫é‰ΩøÁî® GitOps ÊñπÂºèÂÅöÊåÅÁª≠ÂèëÂ∏ÉÊòØ‰∏Ä‰∏™ÊåëÊàòÔºåÁâπÂà´ÊòØÂΩìÁõÆÊ†áÈÉ®ÁΩ≤Âπ≥Âè∞ÊòØ Kubernetes ÁöÑÊó∂ÂÄô„ÄÇ K8S ‰ΩøÁî®Â£∞ÊòéÂºèÈÖçÁΩÆÁÆ°ÁêÜÊúÄÁªàÁä∂ÊÄÅÔºåËÄåK8S‰∏≠ÁöÑÂØÜÈí•‰ªÖ‰ªÖÊòØÂ∞ÜÂØÜÈí•ÂÜÖÂÆπÂÅö‰∫Übase64Ê†ºÂºèÁöÑÁºñÁ†Å„ÄÇ Âú®Âü∫‰∫é Flux ÁöÑ GitOps ÂÆûÊàò‰ªãÁªç‰∫Ü‰ΩøÁî®Bitnami Sealed SecretsÂä†ÂØÜÂØÜÈí•ÂÜÖÂÆπÔºå ÂèØ‰ª•ÂÆâÂÖ®ÁöÑÂ∞ÜÂä†ÂØÜÂêéÁöÑKubernetes ManifestÊñá‰ª∂Êèê‰∫§Âà∞Git‰ª£Á†Å‰ªìÂ∫ìÔºåÁî±Sealed SecretsÂèëÁé∞Ëøô‰∫õSealedSecretÁöÑÂØÜÁ†ÅÔºå Âπ∂Ëß£ÂØÜÂêéÂä®ÊÄÅÁöÑÂàõÂª∫K8SÂéüÁîüSecretsÂØπË±°„ÄÇ\nSealedSecret Ëß£ÂÜ≥‰∫ÜÂ¶Ç‰ΩïÂú® Git ‰ª£Á†Å‰ªìÂ∫ì‰∏≠ÂÆâÂÖ®ÁöÑ‰øùÂ≠òÂØÜÈí•ÁöÑÁóõÁÇπÔºå‰ΩÜÊòØËØ•ÊñπÂºè‰ªçÁÑ∂ÈúÄË¶ÅÁ≥ªÁªüÁÆ°ÁêÜÂëòËá™Ë°åÁöÑÂ¶•ÂñÑ‰øùÂ≠ò SealedSecret ‰ΩøÁî®ÁöÑÁßÅÈí•Ôºå‰ª•ÂèäÂ¶Ç‰Ωï‰ªéÁÅæÈöæ‰∏≠ÊÅ¢Â§çÁöÑÂú∫ÊôØ„ÄÇÊ≠§Â§ñÔºåÊï¥‰∏™ÂØÜÈí•ÁöÑÁîüÂëΩÂë®ÊúüÁÆ°ÁêÜÂú®K8SÈõÜÁæ§ÂÜÖÈÉ®Ôºå Êó†Ê≥ïËÆ©ÈõÜÁæ§Â§ñÁöÑÂ∑•‰ΩúË¥üËΩΩÂÆâÂÖ®ÊúâÊïàÁöÑ‰ΩøÁî®Ëøô‰∫õÂØÜÈí•Ôºå‰æãÂ¶ÇÔºå‰∫ëÂéÇÂïÜ‰∏äÊâòÁÆ°ÁöÑ RDS Á±ªÂûãÊï∞ÊçÆÂ∫ì„ÄÇ\n‰ΩøÁî®Â§ñÈÉ®ÂØÜÈí•ÊúçÂä°ÁÆ°ÁêÜK8SÂØÜÈí• Âú® CNCF Âü∫Èáë‰ºöÂú®2021Âπ¥ÂÅöÁöÑ‰∏Ä‰ªΩÂÖ≥‰∫éÂØÜÈí•ÁÆ°ÁêÜÁöÑÊäÄÊúØÈõ∑ËææÊä•Âëä‰∏äÊåáÂá∫Ôºå AWS Secrets Manager, HashiCorp Vault Ë¢´Âàó‰∏∫ÊàêÁÜüÁöÑÂØÜÈí•ÁÆ°ÁêÜÊúçÂä°ÊàñÊñπÊ°à„ÄÇ Â¶ÇÊûúÂèØ‰ª•Âú® Kubernetes ‰∏≠‰ΩøÁî®Ëøô‰∫õÊàêÁÜüÁöÑÂØÜÈí•ÊúçÂä°ÊàñÊñπÊ°àÊù•ÁÆ°ÁêÜÂØÜÈí•Â∞ÜÂèØ‰ª•ÂêåÊó∂Ëé∑ÂæóÂØÜÈí•ÊúçÂä°ÂÆâÂÖ®ÂäüËÉΩÂº∫Â§ßÂíå Kubernetes ‰ªªÂä°ÁºñÊéíÁöÑÂ§öÈáçÊî∂Áõä„ÄÇ\nÂõæ1ÔºöCNCF End User Technology Radar, Secret Management, February 2021 External Secrets Operator(ESO) ÈíàÂØπ‰ª•‰∏ä‰∏çË∂≥‰πãÂ§ÑÔºåÊé•‰∏ãÊù•‰ªãÁªçÁöÑ External Secrets Operator Â∞ÜÊåâËøô‰∏™ÊÄùË∑ØËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢ò„ÄÇ\nExternal Secrets Operator ÊòØ‰∏Ä‰∏™ Kubernetes OperatorÔºåÂÆÉÈõÜÊàê‰∫ÜÂ§ñÈÉ®ÂØÜÈí•ÁÆ°ÁêÜÁ≥ªÁªüÔºå ‰æãÂ¶Ç AWS Secrets Manager„ÄÅHashiCorp Vault„ÄÅGoogle Secrets Manager„ÄÅAzure Key Vault Á≠âÁ≠â„ÄÇ ‰ªñ‰ΩøÁî®Â§ñÈÉ®ÂØÜÈí•ÁÆ°ÁêÜÊúçÂä°ÁöÑ API ËØªÂèñ‰ø°ÊÅØÂπ∂Ëá™Âä®Â∞ÜÂÄºÊ≥®ÂÖ• Kubernetes Secret„ÄÇ\n‰ª•‰∏äÊòØ External Secrets Operator ÁöÑÁÆÄ‰ªãÔºåÁúã‰∫Ü‰ª•ÂêéÊòØ‰∏çÊòØËßâÂæóÁâπÂà´ÁúºÁÜü„ÄÇ‰ªñË∑üÂêåÊó∂ CNCF ‰∏ãÂè¶‰∏Ä‰∏™ DNS Ëß£ÊûêÊúçÂä°External DNSÈùûÂ∏∏ÁöÑÁ±ª‰ººÔºå‰∏∫ Kubernetes ÂÜÖÁöÑÂüüÂêçËß£ÊûêÊ≥®ÂÜåÊèê‰æõÁªü‰∏ÄÁöÑÂÆûÁé∞‰ΩìÈ™åÔºå ÂêåÂÖ∂‰ªñ‰ºóÂ§öÁ¨¨‰∏âÊñπÊàêÁÜüÁöÑ DNS Ëß£ÊûêÈõÜÊàê„ÄÇ\n‰∏ãÈù¢Â∞Ü‰ªãÁªçÂ¶Ç‰ΩïÂú®‰ΩøÁî® FluxCD ÁÆ°ÁêÜ External Secrets OperatorÔºå‰ª•ÂèäÂú® EKS ‰∏≠‰ΩøÁî® AWS Secrets Manager ÁÆ°ÁêÜÁöÑÂØÜÈí•„ÄÇ\nFluxCD ÈÉ®ÁΩ≤ External Secrets Operator External Secrets Operator ÊîØÊåÅ‰ΩøÁî® Helm ÂÆâË£ÖÔºåFlux ÈÉ®ÁΩ≤ ESO ÂêåÂÆâË£ÖÂÖ∂‰ªñ Helm Chart Á±ª‰ºº„ÄÇ\nÂä†ÂÖ• ESO ÁöÑ Helm ‰ªìÂ∫ì 1apiVersion: source.toolkit.fluxcd.io/v1beta1 2kind: HelmRepository 3metadata: 4 name: external-secrets 5spec: 6 interval: 10m 7 url: https://charts.external-secrets.io ÈÄöËøá HelmRelease ÈÉ®ÁΩ≤ ESO 1apiVersion: helm.toolkit.fluxcd.io/v2beta1 2kind: HelmRelease 3metadata: 4 name: external-secrets 5spec: 6 # Override Release name to avoid the pattern Namespace-Release 7 # Ref: https://fluxcd.io/docs/components/helm/api/#helm.toolkit.fluxcd.io/v2beta1.HelmRelease 8 releaseName: external-secrets 9 targetNamespace: kube-system 10 interval: 10m 11 chart: 12 spec: 13 chart: external-secrets 14 sourceRef: 15 kind: HelmRepository 16 name: external-secrets 17 namespace: kube-system 18 values: 19 installCRDs: true 20 serviceAccountName: helm-controller 21 timeout: 5m 22 test: 23 enable: true 24 ignoreFailures: true 25 install: 26 crds: CreateReplace 27 remediation: 28 retries: 3 29 upgrade: 30 crds: CreateReplace 31 remediation: 32 remediateLastFailure: false ‰∏∫ ESO ÂàõÂª∫ÈÖçÁΩÆ IRSA EKS ÈÄöËøáIRSAÂ∞Ü K8S ÂÜÖ RBAC ÁöÑ ServiceAccount Âêå IAM role Áªü‰∏ÄÂú®‰∏ÄËµ∑Ôºå ÂèØ‰ª•ËÆ©K8SÂÜÖÁöÑÂ∑•‰ΩúË¥üËΩΩÈÄöËøáÂéüÁîüÁöÑ ServiceAccount ÁªëÂÆö IAM RoleÔºåÊó†ÈúÄÊòæÁ§∫ÁöÑÊåáÂÆö AccessKey/Secret Êù•ËÆøÈóÆ AWS API„ÄÇ\nÂõ†‰∏∫ ESO ÂøÖÈ°ªÈÄöËøá AWS API ËÆøÈóÆËØªÂèñ‰øùÂ≠òÂú® AWS Secrets Manager ‰∏≠ÁöÑÂØÜÈí•„ÄÇÊâÄ‰ª•ÈúÄË¶Å‰∏∫ ESO ÈÖçÁΩÆ AWS ËÆøÈóÆÂØÜÈí•Êàñ‰ΩøÁî® IRSA ÊîØÊåÅ„ÄÇ\nÊ†πÊçÆ ESO ÊñáÊ°£Âª∫ËÆÆÁöÑ AWS Secrets Manager ÊùÉÈôêÂàõÂª∫ IAM Policy 1{ 2 \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, 3 \u0026#34;Statement\u0026#34;: [ 4 { 5 \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, 6 \u0026#34;Action\u0026#34;: [ 7 \u0026#34;secretsmanager:GetResourcePolicy\u0026#34;, 8 \u0026#34;secretsmanager:GetSecretValue\u0026#34;, 9 \u0026#34;secretsmanager:DescribeSecret\u0026#34;, 10 \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34; 11 ], 12 \u0026#34;Resource\u0026#34;: [ 13 \u0026#34;arn:aws:secretsmanager:us-west-2:111122223333:secret:dev/*\u0026#34; # ÊõøÊç¢ region, accountid, ÂØÜÈí•ÁöÑÂêçÁß∞ÂâçÁºÄ 14 ] 15 } 16 ] 17} ‰ΩøÁî®eksctlÂ∑•ÂÖ∑‰∏∫ EKS ÈõÜÁæ§ÂàõÂª∫ESOÈúÄË¶ÅÁöÑ Role ÂèäÁªëÂÆö ESO ÈúÄË¶ÅÁöÑÊùÉÈôêÔºå‰æãÂ¶ÇÔºå 1eksctl create iamserviceaccount --cluster=gitops-cluster --name=external-secrets \\ 2--role-only --role-name=gitops-cluster-dev-external-secrets-role --region ap-southeast-1 \\ 3--namespace=kube-system --attach-policy-arn=arn:aws:iam::123456789012:policy/gitops-dev-external-secrets-sm \\ 4--approve namespaceÈúÄË¶ÅË∑üESOÈÉ®ÁΩ≤ÁöÑÂëΩ‰ª§Á©∫Èó¥‰øùÊåÅ‰∏ÄËá¥\nname ÈúÄË¶ÅË∑üÈÉ®ÁΩ≤ ESO Chart ÊåáÂÆöÁöÑ ServiceAccount ÂêçÁß∞‰∏ÄËá¥ÔºåÈªòËÆ§‰∏∫ external-secrets\n‰ΩøÁî® Kustomization patch ‰∏∫ ESO Chart ÂàõÂª∫ÁöÑ ServiceAccount ÊåáÂÆö IAM role 1apiVersion: kustomize.config.k8s.io/v1beta1 2kind: Kustomization 3resources: 4 - ../../base 5 - ./secrets.yaml 6patches: 7 - patch: | 8 - op: add 9 path: /spec/patches/- 10 value: 11 patch: | 12 - op: add 13 path: /spec/values/serviceAccount/annotations/eks.amazonaws.com~1role-arn 14 value: arn:aws:iam::845861764576:role/gitops-cluster-dev-external-secrets-role 15 target: 16 kind: HelmRelease 17 name: external-secrets 18 target: 19 group: kustomize.toolkit.fluxcd.io 20 version: v1beta2 21 kind: Kustomization 22 name: external-secrets ÂàõÂª∫ SecretStore Êàñ ClusterSecretStore ÈÖçÁΩÆËÆøÈóÆ AWS Secrets Manager 1apiVersion: external-secrets.io/v1beta1 2kind: ClusterSecretStore 3metadata: 4 name: secretstore 5 namespace: kube-system 6spec: 7 provider: 8 aws: 9 service: SecretsManager 10 region: ap-southeast-1 11 auth: 12 jwt: 13 serviceAccountRef: 14 name: external-secrets 15 namespace: kube-system ‰∏äÈù¢ÁöÑÈÖçÁΩÆ‰ΩøÁî®‰∫Ü ServiceAccount ÁöÑÁü≠Êó∂Èó¥ÊúâÊïàÊúü JWT token ËÆøÈóÆ AWS APIÔºåÈÅøÂÖç‰∫ÜÂú®ÈõÜÁæ§ÂÜÖÁÆ°ÁêÜ‰øùÂ≠ò AWS ËÆøÈóÆÂá≠ËØÅ„ÄÇ\nÂàõÂª∫ ExternalSecret ÂØπË±°‰ªé Secrets Manager Ëé∑ÂèñÂØÜÈí•Âπ∂ÈÖçÁΩÆÂà∞ K8S ÁöÑ Secret ÂØπË±° 1apiVersion: external-secrets.io/v1beta1 2kind: ExternalSecret 3metadata: 4 name: slack-url 5 namespace: kube-system 6spec: 7 refreshInterval: 1h 8 secretStoreRef: 9 name: secretstore 10 kind: ClusterSecretStore 11 target: 12 name: slack-url 13 creationPolicy: Owner 14 deletionPolicy: Delete 15 data: 16 - secretKey: address 17 remoteRef: 18 key: dev/slackurl Â¶Ç‰∏äÁöÑ ExternalSecret ÂØπË±°Â£∞Êòé‰∫ÜÂú® kube-system ÂëΩ‰ª§Á©∫Èó¥ÂàõÂª∫Âêç‰∏∫ slack-url ÁöÑÂØÜÈí•„ÄÇESO‰ºöÈÄöËøáÂêç‰∏∫ secretstore ÁöÑ ClusterSecretStore ÂØπË±°Ëé∑Âèñ AWS Secrets Manager ËÆøÈóÆÂá≠ËØÅÔºåÂ∞ÜÂêç‰∏∫ dev/slackurl ÁöÑ AWS Secrets Manager ÂØÜÈí•ÂÜÖÂÆπËÆæÁΩÆÂà∞ K8S Secret slack-url ÁöÑ address ÈîÆÂÄº„ÄÇ\nÁ°Æ‰øù FluxCD ÂàõÂª∫ ESO ËµÑÊ∫êÁöÑÈ°∫Â∫è Â¶Ç‰∏äÈÉ®ÁΩ≤ÈÄöËøá Helm ÈÉ®ÁΩ≤‰∫Ü ESOÔºåÈÄöËøá ESO Ëá™ÂÆö‰πâËµÑÊ∫êÂàõÂª∫‰∫Ü ClusterSecretStore Âíå ExternalSecret ÂàõÂª∫ÂØÜÈí•„ÄÇ Ëøô‰∫õËµÑÊ∫êÈÄöËøá‰∏çÂêåÁöÑ Flux ÊéßÂà∂Âô®(KustomizationÊàñHelm)ÊâÄÂàõÂª∫ÔºåËøô‰∫õËµÑÊ∫êÂèØÁî®ÁöÑÈ°∫Â∫èÊ≤°ÊúâÂäûÊ≥ï‰øùËØÅÂÖàÂêéÈ°∫Â∫è„ÄÇ‰ΩÜÊòØ ESO ÁöÑËá™ÂÆö‰πâËµÑÊ∫êÂØπË±°Â£∞ÊòéÔºàÂ¶ÇClusterSecretStoreÔºâ‰æùËµñ ESO ÂÆåÊï¥ÁöÑÈÉ®ÁΩ≤ÂàõÂª∫Ëá™ÂÆö‰πâËµÑÊ∫êÂ£∞Êòé„ÄÇËøôÈáåÈÄöËøáÂµåÂ•óÁöÑ Flux Kustomization ÂØπË±°Êù•ÁÆ°ÁêÜ‰∏çÂêåÂØπË±°Èó¥ÁöÑ‰æùËµñ„ÄÇÁ§∫‰æãÂÆûÁé∞Â¶Ç‰∏ãÔºå\n1apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 2kind: Kustomization 3metadata: 4 name: secrets 5 namespace: flux-system 6spec: 7 interval: 10m0s 8 path: ./infrastructure/overlays/development/secrets 9 prune: true 10 dependsOn: 11 - name: sealed-secrets 12 - name: external-secrets 13 sourceRef: 14 kind: GitRepository 15 name: flux-system ESO Examples ÊñáÊ°£‰πüËØ¶ÁªÜËß£Èáä‰∫Ü FluxCD ‰∏≠ÁöÑËøô‰∏™ÈóÆÈ¢òÔºåÂπ∂‰∏îÁ§∫‰æã‰∫ÜËß£ÂÜ≥ÊñπÊ≥ï„ÄÇ\nÂ∞èÁªì Êú¨Êñá‰ªãÁªç‰∫Ü External Secrets Operator Â∞ÜÊàêÁÜü‰∏îÁªèËøáÈ™åËØÅÁöÑÂØÜÈí•ÁÆ°ÁêÜÊúçÂä°ÔºàÂ¶Ç AWS Secrets ManagerÔºâÂºïÂÖ•Âà∞ Kubernetes ÂéüÁîüÁîüÊÄÅ„ÄÇ Áî®Êà∑ÂèØ‰ª•‰øùÁïô‰ΩøÁî®Ëøô‰∫õÂØÜÈí•ÊúçÂä°ÁöÑÊúÄ‰Ω≥ÂÆûË∑µÂíåÁªèÈ™åÔºåÂêåÊó∂ËÆ© K8S ÁºñÊéíÁöÑ‰ªªÂä°‰πüÊó†ÈúÄÊîπÂä®‰ªçÁÑ∂‰ΩøÁî®‰∫ëÂéüÁîüÁöÑÊñπÂºèËÆøÈóÆÂØÜÈí•„ÄÇ Êï¥‰∏™ÊñπÊ°àÂÖºÂÆπ‰∫ÜÂÆâÂÖ®ÊàêÁÜüÁöÑÂØÜÈí•ÁÆ°ÁêÜÂêå K8S ÂÜÖÁ®ãÂ∫èËÆøÈóÆÂØÜÈí•ÁöÑÈúÄÊ±Ç„ÄÇ\nÈöèÂêéÁÆÄÁü≠ÁöÑÁ§∫‰æã‰∫ÜÂ¶Ç‰ΩïÂú® EKS ÁéØÂ¢ÉÊúÄ‰Ω≥ÂÆûË∑µÁöÑÁÆ°ÁêÜ ESO ÈÉ®ÁΩ≤ÔºåÂêåÊó∂Á§∫‰æã‰∫ÜÂ¶Ç‰Ωï‰ΩøÁî® FluxCD GitOps ÊñπÂºèÂêåÊó∂ÁÆ°ÁêÜ ESO ÈÉ®ÁΩ≤ÂíåÂ§ñÈÉ®ÂØÜÈí•„ÄÇ ÂÆåÊï¥ÁöÑ‰ª£Á†ÅÁ§∫‰æãÂèØ‰ª•Ëøô‰∏™‰ªìÂ∫ìËé∑Âèñ„ÄÇ\nÂ¶ÇÊûúÁî®Êà∑ÊúâÈúÄÊ±ÇÈÄöËøáÊñá‰ª∂ËÆøÈóÆ AWS Secrets Manager ÁöÑÂØÜÈí•ÔºåÂèØ‰ª•‰ΩøÁî® AWS ÂºÄÊ∫êÁöÑ AWS Secrets Manager and Config Provider for Secret Store CSI Driver, Ëøô‰∏™È°πÁõÆÂ∞Ü Secrets Manager/Parameter Store ÈÄöËøá CSI Driver ÊåÇËΩΩÂà∞ÂÆπÂô®ÔºåÊèê‰æõÊñá‰ª∂Á≥ªÁªüÁöÑËÆøÈóÆ„ÄÇ\n","link":"https://kane.mx/posts/gitops/manage-k8s-secrets-in-external-secrets-manager/","section":"posts","tags":["External Secrets Operator","AWS Secrets Manager","Flux","GitOps","Kubernetes","Git","EKS","CD","Continuous Delivery"],"title":"‰ΩøÁî®Â§ñÈÉ®Secrets ManagerÁÆ°ÁêÜKubernetesÂØÜÈí•"},{"body":"","link":"https://kane.mx/tags/crossplane/","section":"tags","tags":null,"title":"Crossplane"},{"body":"ËÉåÊôØ Âú®Flux ÈÉ®ÁΩ≤ÂÆûÊàòÁöÑÊÄªÁªìÂ±ïÊúõ‰∏≠Êúâ‰∏Ä‰∏™ÊñπÂêëÊòØÂ¶Ç‰ΩïÂ∞Ü‰∫ë‰∏äÂü∫Á°ÄËÆæÊñΩËµÑÊ∫êÂêåKubernetesÂÜÖËµÑÊ∫êÁªü‰∏ÄÁÆ°ÁêÜÔºå ËÄåCrossplaneÊèê‰æõ‰∫Ü‰∏Ä‰∏™È´òÂ∫¶ÂèØÊâ©Â±ïÁöÑÂêéÁ´ØÔºå‰ΩøÁî®Â£∞ÊòéÂºèÁ®ãÂ∫èÂêåÊó∂ÁºñÊéíÂ∫îÁî®Á®ãÂ∫èÂíåÂü∫Á°ÄËÆæÊñΩÔºå‰∏çÁî®ÂÖ≥ÂøÉÂÆÉ‰ª¨Âú®Âì™ÈáåËøêË°å„ÄÇ\nËøëÊúü AWS ÂÆòÊñπÂçöÂÆ¢ÂÆ£Â∏É‰∫Ü AWS Blueprints for CrossplaneÔºå‰∏∫ÂÆ¢Êà∑Êèê‰æõ‰∫ÜÂú® Amazon EKS ‰∏äÂ∫îÁî® Crossplane ÁöÑÂèÇËÄÉÂÆûÁé∞„ÄÇ\nAWS Blueprints for Crossplane AWS Blueprints for Crossplane ÊòØ‰∏Ä‰∏™ Github ‰∏äÂºÄÊ∫êÈ°πÁõÆÔºåÂÆÉÊèê‰æõ‰∫ÜÂ¶Ç‰∏ãÂèÇËÄÉÊû∂ÊûÑÂèäÂäüËÉΩÔºå\n‚úÖ ‰ΩøÁî®Terraform ÂàõÂª∫ Amazon EKS ÈõÜÁæ§Âπ∂ÈÉ®ÁΩ≤Crossplane ‚úÖ ‰ΩøÁî®eksctl ÂàõÂª∫ Amazon EKS ÈõÜÁæ§Âπ∂ÈÉ®ÁΩ≤Crossplane ‚úÖ AWS Provider- Crossplane Compositions for AWS Services ‚úÖ Terrajet AWS Provider - Another Crossplane Compositions for AWS Services ‚úÖ AWS IRSA on EKS - AWS Provider Config with IRSA enabled ‚úÖ ‰ΩøÁî® AWS Provider Âíå Terrajet AWS Provider ÁöÑ Composite Resources (XRs)Á§∫‰æãÈÉ®ÁΩ≤Ê®°Âºè ‚úÖ ‰ΩøÁî®Crossplane Managed Resources (MRs) ÁöÑÁ§∫‰æãÈÉ®ÁΩ≤ ÈÉ®ÁΩ≤ Crossplane EKS Crossplane ÂèÇËÄÉËìùÂõæÁ§∫‰æã‰∫ÜÂ¶Ç‰Ωï‰ΩøÁî® Terraform(ÈÄöËøáAmazon EKS Blueprints for Terraform) Âíå eksctl ÈÉ®ÁΩ≤ EKS ÈõÜÁæ§ÂèäÈÉ®ÁΩ≤ CrossplaneÔºå Êú¨ÊñáÂ∞ÜÊºîÁ§∫Â¶Ç‰Ωï‰ΩøÁî® Flux ÊåâÁÖß GitOps ÊñπÂºèÈÉ®ÁΩ≤ÁÆ°ÁêÜ CrossplaneÔºåÊºîÁ§∫Â∞ÜÊ≤øÁî® Flux ÂÆûÊàò ÊâÄ‰ΩøÁî®ÁöÑÁ§∫‰æãrepo„ÄÇ\nÊâãÂä®ÈÉ®ÁΩ≤ Crossplane ÊåâÁÖß Crossplane ÈÉ®ÁΩ≤ÊñáÊ°£ÔºåCrossplane Âú® EKS ‰∏äÁöÑÈÉ®ÁΩ≤ÂàÜ‰∏∫‰∏ãÈù¢‰∏âÊ≠•Ôºå\nÈÄöËøá Helm ÈÉ®ÁΩ≤ Crossplane chart Áî±‰∫é Crossplane Â§ßÈáèÈÄöËøá CRD ‰ΩøÁî®Êâ©Â±ïÊÄßÔºåÈúÄË¶ÅÂú® Crossplane ÁªÑ‰ª∂ÈÉ®ÁΩ≤ÊàêÂäüÂêéÔºå ÈÄöËøá Crossplane pkg CRD ÈÉ®ÁΩ≤ÂèäÈÖçÁΩÆÂØπÂ∫îÁöÑ ProviderÔºåÂ¶ÇÂú® AWS ‰∏äÁÆ°ÁêÜ AWS Provider Êàñ Terrajet AWS Provider AWS Provider Êàñ Terrajet AWS Provider ÊòØÈÄöËøá pkg CRD ÂºÇÊ≠•ÈÉ®ÁΩ≤ÁöÑÔºåÈúÄË¶ÅÁ≠â Provider CRD ÂèØÁî®ÂêéÔºåÊâçÂèØÈÉ®ÁΩ≤ÂØπÂ∫îÁöÑ Provider Config ÈÄöËøá Flux ÂÆûÁé∞ GitOps ÈÉ®ÁΩ≤ Crossplane Èâ¥‰∫é Crossplane ÈÉ®ÁΩ≤‰∏â‰∏™Ê≠•È™§ÁöÑÂº∫‰æùËµñÊÄßÔºåÊâÄ‰ª•‰ΩøÁî® Flux ÈÉ®ÁΩ≤ÈÄöËøá Kustomization dependencies ÂäüËÉΩÂÆûÁé∞‰∏âÈÉ®ÂàÜËµÑÊ∫êÂàõÂª∫ÁöÑÂÖàÂêé‰æùËµñ„ÄÇ\n1. ÈÉ®ÁΩ≤ Crossplane Helm chart Â¶Ç‰∏ã manifest ÂàõÂª∫ Crossplane helm release kustomizationÔºå ÈÄöËøáhealthChecksÊ£ÄÊü•Á°Æ‰øù Crossplane ÁªÑ‰ª∂ÈÉ®ÁΩ≤ÊàêÂäüÂêéÊâçÂ∞Ü kustomization ËÆæÁΩÆ‰∏∫ reconcilation ÊàêÂäü„ÄÇ\n1apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 2kind: Kustomization 3metadata: 4 name: crossplane 5 namespace: flux-system 6spec: 7 interval: 10m0s 8 path: ./infrastructure/base/crossplane/release 9 targetNamespace: crossplane-system 10 prune: true 11 sourceRef: 12 kind: GitRepository 13 name: flux-system 14 namespace: flux-system 15 timeout: 5m 16 healthChecks: 17 - apiVersion: apps/v1 18 kind: Deployment 19 name: crossplane 20 namespace: crossplane-system ÈÄöËøá Flux Helm ÊîØÊåÅÈÉ®ÁΩ≤ Crossplane helm release\n1apiVersion: helm.toolkit.fluxcd.io/v2beta1 2kind: HelmRelease 3metadata: 4 name: crossplane 5 namespace: crossplane-system 6spec: 7 releaseName: crossplane 8 targetNamespace: crossplane-system 9 chart: 10 spec: 11 chart: crossplane 12 version: \u0026#34;1.8.0\u0026#34; 13 sourceRef: 14 kind: HelmRepository 15 name: crossplane-stable 16 namespace: crossplane-system 17 serviceAccountName: helm-controller 18 timeout: 5m 19 test: 20 enable: true 21 ignoreFailures: true 22 interval: 1h0m0s 23 install: 24 crds: CreateReplace 25 remediation: 26 retries: 3 27 upgrade: 28 crds: CreateReplace 29 remediation: 30 remediateLastFailure: false 2. ÂàõÂª∫ Crossplane AWS Provider Kustomization crossplane-provider Â∞Ü‰æùËµñ kustomization crossplaneÔºå Âπ∂Ê£ÄÊü• Crossplane AWS provider Ëá™ÂÆö‰πâËµÑÊ∫ê providerconfigs.aws.crossplane.io ÂàõÂª∫ÊàêÂäü‰∏éÂê¶„ÄÇ\n1apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 2kind: Kustomization 3metadata: 4 name: crossplane-provider 5 namespace: flux-system 6spec: 7 interval: 10m0s 8 path: ./infrastructure/base/crossplane/provider 9 prune: true 10 sourceRef: 11 kind: GitRepository 12 name: flux-system 13 namespace: flux-system 14 dependsOn: 15 - name: crossplane 16 targetNamespace: crossplane-system 17 healthChecks: 18 - apiVersion: apiextensions.k8s.io/v1 19 kind: CustomResourceDefinition 20 name: providerconfigs.aws.crossplane.io 21 timeout: 5m 22 patches: 23 - patch: | 24 - op: replace 25 path: /metadata/annotations/eks.amazonaws.com~1role-arn 26 value: arn:aws:iam::845861764576:role/crossplane-provider-aws 27 target: 28 group: pkg.crossplane.io 29 version: v1alpha1 30 kind: ControllerConfig 3. ÂàõÂª∫ Provider Config ÂêåÊ†∑ÊñπÂºèÂàõÂª∫ÈÉ®ÁΩ≤ ProviderConfig ËµÑÊ∫êÁöÑ kustomization ÂØπË±°Ôºå‰æùËµñ crossplane-provider kustomization ÈÉ®ÁΩ≤„ÄÇ\n1apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 2kind: Kustomization 3metadata: 4 name: crossplane-provider-config 5 namespace: flux-system 6spec: 7 interval: 10m0s 8 path: ./infrastructure/base/crossplane/provider-config 9 prune: true 10 sourceRef: 11 kind: GitRepository 12 name: flux-system 13 dependsOn: 14 - name: crossplane-provider 15 timeout: 5m ‰ΩøÁî® Crossplane ÂàõÂª∫ AWS Âü∫Á°ÄËÆæÊñΩ Crossplane Êèê‰æõ‰∫Ü‰∏§ÁßçÊñπÂºèË°®Á§∫Â§ñÈÉ®Á≥ªÁªüËµÑÊ∫êÔºå\nÊâòÁÆ°ËµÑÊ∫ê (MR) ÊòØ Crossplane ÂØπÂ§ñÈÉ®Á≥ªÁªü‰∏≠ËµÑÊ∫êÁöÑË°®Á§∫Ôºå‰æãÂ¶ÇÔºåÊúÄÂ∏∏ËßÅÁöÑÊòØ‰∫ëÊèê‰æõÂïÜ„ÄÇÂ¶Ç‰∏ãËµÑÊ∫êÁî≥ÊòéÁî± AWS Provider ÊîØÊåÅÔºåÂàõÂª∫ AWS ‰∏äÁöÑ RDS Êï∞ÊçÆÂ∫ìÂÆû‰æã„ÄÇ 1apiVersion: database.aws.crossplane.io/v1beta1 2kind: RDSInstance Crossplane Â§çÂêàËµÑÊ∫ê (XR)ÊòØÁî±ÊâòÁÆ°ËµÑÊ∫êÁªÑÊàêÁöÑÂ∞ÅË£ÖÁöÑ Kubernetes Ëá™ÂÆö‰πâËµÑÊ∫ê„ÄÇ Â§çÂêàËµÑÊ∫êÊó®Âú®ËÆ©Áî®Êà∑‰ΩøÁî®Ëá™Â∑±ÁöÑËßÇÁÇπÂíå API ÊûÑÂª∫Ëá™Â∑±ÁöÑÂπ≥Âè∞ÔºåËÄåÊó†ÈúÄ‰ªéÂ§¥ÂºÄÂßãÁºñÂÜô Kubernetes ÊéßÂà∂Âô®„ÄÇ Áõ∏ÂèçÔºåÁî®Êà∑ÂÆö‰πâÁöÑ XR Êû∂ÊûÑÊïô‰ºö Crossplane ÂΩìÊúâ‰ΩøÁî®Áî®Êà∑ÂÆö‰πâÁöÑ XR Êó∂ÂÆÉÂ∫îËØ•ÁªÑÊàêÔºàÂç≥ÂàõÂª∫ÔºâÂì™‰∫õÊâòÁÆ°ËµÑÊ∫ê„ÄÇ AWS Blueprints for Crossplane Êèê‰æõ‰∫Ü Compositions Á§∫‰æãÔºåÊ∂µÁõñ‰∫Ü VPCÔºåS3ÔºåIAMÔºåRDSÔºåDynamoDBÔºåEKS Á≠âÊúçÂä°„ÄÇ Â¶ÇÂâçÈù¢‰ªãÁªç Crossplane Compositions(XRs) ÊòØÂØπÂü∫Á°ÄËÆæÊñΩÁöÑÊ®°ÂºèÂ∞ÅË£ÖÂíåÁªÑÂêàÔºåÂπ∂‰∏ç‰ºöÁõ¥Êé•ÂàõÂª∫‰∫ëÂéüÁîüËµÑÊ∫ê„ÄÇ\nAWS Blueprints for Crossplane ÂêåÊó∂Êèê‰æõ‰∫Ü Examples Á§∫‰æã Áõ¥Êé•‰ΩøÁî® AWS Provider Êèê‰æõÁöÑÊâòÁÆ°ËµÑÊ∫ê (MR) ÂíåÁ§∫‰æãÁöÑÂ§çÂêàËµÑÊ∫ê (XR)ÔºåÂ¶Ç‰∏äCompositions‰∏≠Á§∫‰æãVPCÔºåS3, DynamoDBÁ≠âAWSËµÑÊ∫ê„ÄÇ\nÂ∞èÁªìÂèäÂ±ïÊúõ Crossplane ÁõÆÂâçÊòØ CNCF Âü∫Èáë‰ºö‰∏ãÂ≠µÂåñ‰∏≠È°πÁõÆÔºå‰∏ÄÂÆöÁ®ãÂ∫¶ÂèØ‰ª•ÂÆûÁé∞‰∫ë‰∏äÂü∫Á°ÄËÆæÊñΩËµÑÊ∫êÂíå Kubernetes ÂÜÖËµÑÊ∫êÁªü‰∏Ä‰ΩøÁî®Â£∞ÊòéÂºèÊñπÂºèÁÆ°ÁêÜ„ÄÇ Â§çÂêàËµÑÊ∫ê (Composite Resources) ÊîØÊåÅ‰∫ÜÂØπ‰∏öÂä°ÈúÄÊ±ÇÁöÑÈ´òÂ±ÇÊ¨°ÊäΩË±°ÔºåÁêÜÂøµÂêå Construct Hub Á±ª‰ºº„ÄÇ Âü∫Á°ÄÂÆûÊñΩÂõ¢ÈòüÂèØ‰ª•ÈÄöËøáÂ§çÂêàËµÑÊ∫êÊèê‰æõÈ´òÈò∂ÊäΩË±°ÔºåÂ§çÁî®ÁªèËøáÈ™åËØÅ‰∏îÁ¨¶ÂêàÁÆ°ÁêÜÈúÄÊ±ÇÁöÑÊäΩË±°ÁªÑÂêàÔºåÁÆÄÂåñ‰∏ãÊ∏∏Âõ¢ÈòüÁÆ°ÁêÜËµÑÊ∫êÁöÑÂ§çÊùÇÂ∫¶„ÄÇ\nCrossplane Ëá™Ë∫´Âà©Áî® K8S CRD ÂàõÂª∫ÁÆ°ÁêÜ Composite ResourcesÔºåÈ¶ñÂÖàÈúÄË¶ÅÁî®Êà∑ÁÜüÊÇâ CRD ÁöÑÂÆûÁé∞„ÄÇ XRs Êú¨Ë¥®ÊòØÈÄöËøáÂ£∞ÊòéÂºèÊñπÂºèÁÆ°ÁêÜ‰∫ëÂéüÁîüÂü∫Á°ÄËÆæÊñΩÔºåÂêåÊ†∑ AWS CloudFormation ÊòØÁî± AWS ÂéüÁîüÊèê‰æõÁöÑÈÄöËøáÂ£∞ÊòéÂºèÊñπÂºèÁÆ°ÁêÜ AWS ‰∏äËµÑÊ∫ê„ÄÇ Áî±‰∫é‰∫ëÂéüÁîüËµÑÊ∫êÁöÑÂäüËÉΩÂ§çÊùÇÊÄßÔºåCloudFormation Èù¢‰∏¥ÁöÑÁºñÂÜôÂ§çÊùÇÂ£∞ÊòéÂºè‰ª£Á†ÅÔºå‰∏çÊòì‰∫éÊµãËØïÂíåÂ§çÁî®ÁöÑÈóÆÈ¢òÂêåÊ†∑Âú® Crossplane XRs ‰∏äÂ≠òÂú®„ÄÇ ÂêåÊó∂Èù¢ÂØπÊï∞ÈáèÂ∫ûÂ§ßÁöÑ AWS ÊàñÂÖ∂‰ªñ‰∫ëÂéÇÂïÜÂéüÁîüÊúçÂä°ËµÑÊ∫êÔºåÈúÄË¶ÅÂ§ßÈáèÁöÑÁ§æÂå∫ËµÑÊ∫êÊù•ÂàõÂª∫ÁÆ°ÁêÜ AWS ÂèØÂ§çÁî®ÁöÑÂ§çÂêàËµÑÊ∫êÊ®°ÂºèÔºå ÂèØ‰ª•È¢ÑËßÅÂú®Áõ∏ÂΩì‰∏ÄÊÆµÊó∂Èó¥ÂÜÖ‰∫ëÂéÇÂïÜÊâòÁÆ°ËµÑÊ∫êË¶ÜÁõñÁéáÂèäÈ´òÈò∂ÁöÑÂ§çÂêàËµÑÊ∫êÊï∞ÈáèÈÉΩÊòØËØ•ÊäÄÊúØË¢´ÂπøÊ≥õÈááÁ∫≥ÁöÑ‰∏Ä‰∏™ÈöúÁ¢ç„ÄÇ\nÂØπÊØî AWS CDK/Pulumi ÁºñÁ®ãÊñπÂºèÁÆ°ÁêÜÂàõÂª∫ÁöÑÂ§çÁî®ËµÑÊ∫êÂíåÊõ¥È´òÈò∂ÁöÑÊäΩË±°Ôºå Crossplane Âú®ÂºÄÂèëÂíåÂ§çÁî®ÊïàÁéá‰∏äÂπ∂Ê≤°Êúâ‰ºòÂäø„ÄÇ Crossplane ÊúÄÂ§ßÁöÑ‰ºòÂäøÊòØÂèØÈÄöËøáÁªü‰∏Ä Kubernetes Â£∞ÊòéÂºèÊñπÂºèÊù•ÁÆ°ÁêÜ‰∫ë‰∏äËµÑÊ∫êÂíå Kubernetes ÈõÜÁæ§ÂÜÖËµÑÊ∫ê„ÄÇ ‰ΩÜÂØπÁî®Êà∑ËÄåË®ÄÈááÁî® Crossplane ÁöÑÂ≠¶‰π†ÊàêÊú¨ÂíåÂºÄÂèëÂ§çÊùÇÂ∫¶ËæÉÈ´òÔºåCrossplane ÂèäÁ±ª‰ººÊäÄÊúØÂèØÂàó‰∏∫ÊåÅÁª≠ËØÑ‰º∞Ë∞ÉÁî®‰∏≠ÔºåÂ∞èÈáèË∞®ÊÖéÁî®‰∫éÁîü‰∫ßÁéØÂ¢É„ÄÇ\n","link":"https://kane.mx/posts/gitops/crossplane-meets-gitops/","section":"posts","tags":["Crossplane","Flux","GitOps","Kubernetes","Git","EKS","CD","Continuous Delivery"],"title":"Âü∫‰∫é Flux ÁöÑ GitOps ÁÆ°ÁêÜ Crossplane ÈÉ®ÁΩ≤ÂèäËµÑÊ∫ê"},{"body":"AWS CDK is a great abstract to accelerate managing the cloud infrastructure as code. The journey will be enjoyful with leveraging the Construct Hub to use the high level contributions from AWS partners and commnunity.\nUse Case AWS CloudFormation is one of the underly technologies of AWS CDK to manage the cloud infrastructure. It easily to enable the IT administrators even business operators whom has no/limited developer skills to develop the end-to-end solutions with one-click user experience.\nSo it's a use case for effectively developing the Cloud Application via AWS CDK, then publishing it as CloudFormation template with better user experimental experience.\ncdk synth command CDK has a built-in capability to synthesize its application to CloudFormation templates, as known as the cdk synth command. You can upload the syntheized output templates to Amazon S3 bucket, then deploy it via AWS CloudFormation. Looks like it's quite easy to publish the CDK application as CloudFormation templates.\nWhy cdk synth does not work However above procedure is not working in most case while orchestrating a large application in cloud. Due to the CDK applications probably contains assets which need be uploaded to S3 and ECR before deploying the application. For example, a CDK application with using Node.js Function, Python Function, S3 Deployment, Docker Image Assets and so on will be synthesized to the templates that are not deployable directly. It requires to publish those assets(both S3 and ECR assets) firstly, then deploy the templates with parameters pointing to the assets. This step is difficult to be completed manually, because the assets are named with its content hash are not readable by human being in CDK V1. CDK v2 uses the modern bootstrapping template which uses deterministic name for resources to remove the parameters, but it still depends on the assets published priorly before deploying the CloudFormation template.\ncdk-assets command Hence there is another experimental tool provided with CDK project, it's cdk-assets. cdk-assets command use the outputs of cdk synth, then publish the assets of application to S3 and ECR, and update the templates to refer to the assets in S3 and ECR. Looks like the utility perfectly fits the requirement of my use case.\ncdk-assets drawbacks But it still has some drawbacks for this solution. For some AWS services, the assets are mandatorily required from same region. It means that the Lambda code packages(reside on S3) must be from same region S3 bucket, the container images(reside on ECR) must be from same region of SageMaker training job / inference endpoint. For the applications with multiple regions support, we have to replicate above procedure in multiple times and provide multiple CloudFormation links per region like below. It means the users can not switch to another region via region selector after opening one of the links.\nCloudFormation link per region the solution cdk-bootstrapless-synthesizer There is another commnuity tool cdk-bootstrapless-synthesizer to resolve above painful perfectly. It can help synthesize a single CloudFormation template entrypoint, then deploy it to any supported regions. Also it provides a pipeline example(based on AWS CodePipeline) to publish a CDK application to CloudFormation template with multiple regions supported.\n","link":"https://kane.mx/posts/2022/publish-cdk-app-via-cloudformation/","section":"posts","tags":["AWS CDK","AWS CloudFormation","AWS","Tip"],"title":"Publish your AWS CDK applications via AWS CloudFormation templates"},{"body":"Âú®‰∏äÁØá‰ªãÁªçÂü∫‰∫é CNCF ‰∏ãÁöÑ GitOps Â∑•ÂÖ∑ FluxCD v2 ÂÆûÁé∞‰∫ÜÁÆ°ÁêÜÂ§öË¥¶Êà∑ÁöÑ Kubernetes ÈõÜÁæ§ÁöÑÂÖ±‰∫´ÁªÑ‰ª∂ÔºåSecrets ‰ΩøÁî®ÁöÑÊúÄ‰Ω≥ÂÆûË∑µÔºå GitOps ÊµÅÊ∞¥Á∫ø‰∫ã‰ª∂Âêå IM(Slack) ÁöÑÈõÜÊàêÔºå‰ª•ÂèäÂØπ GitOps ‰ª£Á†ÅÁöÑ CI ÊµÅÁ®ã„ÄÇ\nÊú¨ÊñáÂ∞ÜÂõ¥ÁªïÂ¶Ç‰Ωï‰ΩøÁî® Flux ÁöÑÂ§öÁßüÊà∑ÁÆ°ÁêÜÊúÄ‰Ω≥ÂÆûË∑µÔºåÊâìÈÄ†Âü∫‰∫é GitOps Â∑•‰ΩúÊµÅÁ®ãÁöÑÂÖ±‰∫´ÊúçÂä°Âπ≥Âè∞Ôºå ÂÆûÁé∞ÁßüÊà∑(‰∏öÂä°/Â∫îÁî®Âõ¢Èòü)ÂèØËá™Âä©ÁöÑÊåÅÁª≠ÈÉ®ÁΩ≤„ÄÇ\n‰∏Ä„ÄÅÂü∫‰∫é GitOps ÁöÑÂÖ±‰∫´ÊúçÂä°Âπ≥Âè∞ËÆæÂÆö Kubernetes Êèê‰æõ‰∫ÜÂëΩÂêçÁ©∫Èó¥‰Ωú‰∏∫‰∏ÄÁßçÊú∫Âà∂Â∞ÜÂêå‰∏ÄÈõÜÁæ§‰∏≠ÁöÑËµÑÊ∫êÂàíÂàÜ‰∏∫Áõ∏‰∫íÈöîÁ¶ªÁöÑÁªÑ„ÄÇ Âêå‰∏Ä‰∏™ÈõÜÁæ§‰∏≠Â§öÁßüÊà∑Â§öÂõ¢ÈòüÁöÑÂ∫îÁî®ÁÆ°ÁêÜÂ∞ÜÊ≤øÁî® Kubernetes ÂÜÖÁΩÆÁöÑÂêÑÁßçÊú∫Âà∂Êù•‰∏∫‰∏çÂêåÁöÑÁßüÊà∑„ÄÅÂõ¢ÈòüÊàñÂ∫îÁî®ËøõË°åÈöîÁ¶ªÔºåÂåÖÊã¨‰∏î‰∏çÈôê‰∫éÔºå\nÂëΩÂêçÁ©∫Èó¥(Namespaces) ËµÑÊ∫êÈÖçÈ¢ù(Resource Quotas)ÔºåÈôêÂà∂Â∫îÁî®ÁöÑËµÑÊ∫êÊÄªÈáè RBAC Èâ¥ÊùÉÔºåÈôêÂà∂Â∫îÁî®ÁöÑÊùÉÈôêÔºåÂ¶ÇÂèØÂàõÂª∫ IngressÔºå‰∏çÂèØÂàõÂª∫ÂØÜÈí•ÂèØËØªÂèñÊåáÂÆöÂêçÁß∞ÁöÑÂØÜÈí•Ôºå‰∏çÂèØÂàõÂª∫ÊåÅ‰πÖÂç∑Á≠â ÁΩëÁªúÁ≠ñÁï•(Network Policies) Âü∫‰∫é Kubernetes ‰ª•‰∏äËÉΩÂäõÔºå‰∏∫Âü∫‰∫é GitOps ÁöÑÂÖ±‰∫´ÊúçÂä°Âπ≥Âè∞ËÆæÂÆöÂ¶Ç‰∏ãÔºå\nÂπ≥Âè∞Âõ¢ÈòüÈÄöËøá‰∏Ä‰∏™ Git ‰ªìÂ∫ìÊù•ÁÆ°ÁêÜÂ§ö‰∏™Ë∑®ÁΩëÁªúË∑®Ë¥¶Êà∑Ë∑®‰∫ëÂπ≥Âè∞ÁöÑ Kubernetes ÈõÜÁæ§ÔºåÂπ≥Âè∞Âõ¢ÈòüÈÄöËøá GitOps ÁÆ°ÁêÜÂ¶Ç‰∏ãËµÑÊ∫êÔºå GitOps Toolkit ÁªÑ‰ª∂ÔºåÂ¶Ç Flux ÈõÜÁæ§ÂÖ±‰∫´ÁªÑ‰ª∂ÔºåÂ¶Ç CNI, CSI Driver, Ingress ClassÔºåService Accounts, CRD, DNS Á≠â ÂèØËßÇÊµãÊÄßÁöÑÂÖ±‰∫´ÁªÑ‰ª∂ÔºåÂ¶Ç Log, Metrics, Trace ÊØè‰∏™ÁßüÊà∑/Âõ¢Èòü/Â∫îÁî®ÁöÑÂü∫Á°ÄËµÑÊ∫êÔºåÂ¶Ç Namespaces, Resource Quotas, Open PolicyÔºåService AccountsÔºåÂØÜÈí•Á≠â ‰∏∫ÈõÜÁæ§‰∏≠ÁöÑÊØè‰∏™ÁßüÊà∑/Âõ¢Èòü/Â∫îÁî®‰ΩøÁî®Áã¨Á´ãÁöÑ Git ‰ªìÂ∫ìÊù•ÈöîÁ¶ªÂÖ∂ÊåÅÁª≠ÈÉ®ÁΩ≤ÔºåÂÅáËÆæÊúâÂ∫îÁî®Âêç‰∏∫ app-aÔºå Â∫îÁî® app-a Áõ∏ÂÖ≥ÁöÑËµÑÊ∫êÈÉΩÂ∞ÜÈÉ®ÁΩ≤Âú®ÂëΩÂêçÁ©∫Èó¥ app-a ÈôêÂà∂Â∫îÁî®‰ΩøÁî®ÁöÑÊÄªËµÑÊ∫êÔºåÂ¶Ç‰∏çË∂ÖËøá 2 vCPU, 4 GiB ÂÜÖÂ≠ò Â∫îÁî®Âõ¢Èòü‰ΩøÁî®Áã¨Á´ãÁöÑ Git ‰ªìÂ∫ìÊù•ÁÆ°ÁêÜÂ∫îÁî®ÁºñÊéíÔºåÂ∫îÁî®Âõ¢ÈòüÂ∞ÜË¥üË¥£Â∫îÁî®ÂèëÂ∏ÉÂà∞‰∏çÂêå stage ÁéØÂ¢ÉÁöÑËäÇÂ•è Â∫îÁî®Âõ¢ÈòüÂèØ‰ª•‰ΩøÁî® Kustomization„ÄÅHelm ÈÉ®ÁΩ≤Â∫îÁî® Â∫îÁî®Âõ¢ÈòüÊó†Ê≥ïÂàõÂª∫ÈõÜÁæ§Áõ∏ÂÖ≥ÁöÑÁªÑ‰ª∂ÔºåÂ¶ÇÊåÅ‰πÖÂç∑„ÄÅCRD Á≠âËµÑÊ∫ê Â∫îÁî®Âõ¢ÈòüÊó†Ê≥ïÂàõÂª∫ÂØÜÈí•„ÄÅService AccountÁ≠âËµÑÊ∫êÔºå‰ΩÜ‰ªÖÂèØ‰ΩøÁî® infra Âõ¢ÈòüÊèêÂâç‰∏∫Â∫îÁî®ÂàõÂª∫ÁöÑËøôÁ±ªËµÑÊ∫ê ‰∫å„ÄÅFlux Â§öÁßüÊà∑ÁöÑÂÆâÂÖ®ËÆæÁΩÆ ÂØπ‰∫é‰∏Ä‰∏™‰ΩøÁî®ÂëΩÂêçÁ©∫Èó¥Âú®ÈöîÁ¶ªÂ§öÁßüÊà∑ÁöÑÈõÜÁæ§ÔºåFlux Êèê‰æõ‰∫ÜÈÄâÈ°πÊù•Á¶ÅÊ≠¢Ë∑®ÂëΩ‰ª§Á©∫Èó¥ÁöÑÂºïÁî®Ôºå ‰æãÂ¶ÇÔºåFlux ÁöÑ Kustomization Êàñ Helm Releases Á¶ÅÊ≠¢ÂºïÁî®ÂÖ∂‰ªñÂëΩÂêçÁ©∫Èó¥ÂÆö‰πâÁöÑ Source„ÄÇ ÂêåÊó∂ÔºåÂêØÁî®Âº∫Âà∂Ê®°ÊãüÂäüËÉΩÔºåÂ∞Ü Kustomization Êàñ Helm Releases ËµÑÊ∫êÁöÑÈÉ®ÁΩ≤ÈªòËÆ§ÈôêÂà∂Âà∞ÊúÄÂ∞èÊù•ÊòæÁ§∫ÁöÑÊèêÂçáÈÉ®ÁΩ≤ÁöÑÂÆâÂÖ®ÊÄß„ÄÇ\nÈÅµÂæ™‰ª•‰∏ä Flux ÁöÑÂ§öÁßüÊà∑ÂÆâÂÖ®ÊúÄ‰Ω≥ÂÆûË∑µÔºåËøõË°åÂ¶Ç‰∏ã Flux Toolkits ÈÖçÁΩÆÔºà./cluster/cluster-dev/kustomization.yamlÔºâ Êù•Á¶ÅÁî®Ë∑®ÂëΩÂêçÁ©∫Èó¥ÂºïÁî®ÂíåÂº∫Âà∂Ê®°ÊãüÈôêÂà∂ Kustomization Âíå Helm ÈÉ®ÁΩ≤ÁöÑÈªòËÆ§ÊùÉÈôêÔºå\n1apiVersion: kustomize.config.k8s.io/v1beta1 2kind: Kustomization 3resources: 4 - gotk-components.yaml 5 - gotk-sync.yaml 6patches: 7 - patch: | 8 - op: add 9 path: /spec/template/spec/containers/0/args/0 10 value: --no-cross-namespace-refs=true 11 target: 12 kind: Deployment 13 name: \u0026gt;- 14 (kustomize-controller|helm-controller|notification-controller|image-reflector-controller|image-automation-controller) 15 - patch: | 16 - op: add 17 path: /spec/template/spec/containers/0/args/0 18 value: --default-service-account=default 19 target: 20 kind: Deployment 21 name: (kustomize-controller|helm-controller) 22 - patch: | 23 - op: add 24 path: /spec/serviceAccountName 25 value: kustomize-controller 26 target: 27 kind: Kustomization 28 name: flux-system 29 - patch: | 30 - op: add 31 path: /spec/serviceAccountName 32 value: helm-controller 33 target: 34 kind: HelmRelease 35 name: flux-system ÂêåÊó∂‰∏∫ infra Âõ¢ÈòüÁÆ°ÁêÜÁöÑÂÖ±‰∫´ Kustomization/Helm ÁªÑ‰ª∂ÈÉ®ÁΩ≤ÊòæÁ§∫ÁöÑÊåáÂÆöÈÉ®ÁΩ≤ÊùÉÈôêÔºå ‰æãÂ¶ÇÔºåDEVÁéØÂ¢É infrastructure ÈÖçÁΩÆÁöÑÂÖ•Âè£./clusters/cluster-dev/infrastructure.yaml\n1apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 2kind: Kustomization 3metadata: 4 name: infrastructure 5 namespace: flux-system 6spec: 7 interval: 10m0s 8 serviceAccountName: kustomize-controller 9 path: ./infrastructure/overlays/product 10 prune: true 11 sourceRef: 12 kind: GitRepository 13 name: flux-system ÊàñÈÄöËøá Kustomize ÁöÑË°•‰∏ÅÊú∫Âà∂‰∏∫ÊâÄÊúâÁöÑ Kustomization/Helm Flux Ëá™ÂÆö‰πâËµÑÊ∫êÊåáÂÆöÈÉ®ÁΩ≤ÊùÉÈôêÔºå ‰æãÂ¶ÇDEVÁéØÂ¢ÉÁöÑoverlayÁöÑÂÖ•Âè£./infrastructure/overlays/development/kustomization.yamlÈÖçÁΩÆÔºå\n1apiVersion: kustomize.config.k8s.io/v1beta1 2kind: Kustomization 3resources: 4 - ../../base 5 - ./secrets.yaml 6patches: 7 - path: ./aws-load-balancer-controller-patch.yaml 8 - path: ./aws-load-balancer-serviceaccount-patch.yaml 9 - path: ./dns-patch.yaml 10 - path: ./dns-sa-patch.yaml 11 - path: ./slack-patch.yaml 12 - patch: | 13 - op: add 14 path: /spec/serviceAccountName 15 value: kustomize-controller 16 target: 17 kind: Kustomization 18 namespace: (flux-system|kube-system|mariadb) 19 - patch: | 20 - op: add 21 path: /spec/serviceAccountName 22 value: helm-controller 23 target: 24 kind: HelmRelease 25 namespace: (flux-system|kube-system|mariadb) ÊúÄ‰Ω≥ÂÆûË∑µ ÈÄöËøáÂú®Âπ≥Âè∞Âõ¢ÈòüÁÆ°ÁêÜÁöÑ Kustomization ÈÖçÁΩÆ‰∏≠ÔºåÂº∫Âà∂‰∏∫Â∫îÁî®Âõ¢Èòü Git ‰ªìÂ∫ìÁöÑ Kustomization, HelmRelease Á≠âÈÉ®ÁΩ≤ÂØπË±°ÊåáÂÆöÈÉ®ÁΩ≤Êó∂‰ΩøÁî®ÁöÑ Service Account„ÄÇ\n‰∏â„ÄÅÁßüÊà∑ÁöÑÈõÜÁæ§ËµÑÊ∫êÁÆ°ÁêÜ Âü∫‰∫éÂâçÈù¢ÁöÑÁÆ°ÁêÜÈúÄÊ±ÇÂÅáËÆæÔºåÂú® infrastructure Git ‰ªìÂ∫ì‰∏≠Ôºå‰∏ìÈó®‰∏∫Â§öÁßüÊà∑/Â§öÂõ¢Èòü/Â§öÂ∫îÁî®ÂàõÂª∫Â¶Ç‰∏ãÁõÆÂΩïÁªìÊûÑÔºå ÂÖ±‰∫´ apps ÈÄöÂ∏∏ÁöÑÁßüÊà∑ÈÖçÁΩÆÔºå‰æãÂ¶ÇÔºåÂëΩÂêçÁ©∫Èó¥ÔºåRBAC(ÈÄöËøá Service Account)Âä†‰∏ä Policy ÂÆûÁé∞Á≠â„ÄÇ\n1apps 2|-- base 3| |-- app-a 4| | |-- bitnami.yaml 5| | |-- kustomization.yaml 6| | |-- namespace.yaml 7| | |-- policies.yaml 8| | `-- rbac.yaml 9| `-- kustomization.yaml 10`-- overlays 11 `-- development 12 |-- app-a 13 | |-- kustomization.yaml 14 | `-- prestashop-sealed-secrets.yaml 15 `-- kustomization.yaml ÂêåÊó∂ÂàõÂª∫‰∏Ä‰∏™ apps ÁöÑ Kustomization ÂÖ•Âè£ÈÖçÁΩÆÂêåÈõÜÁæ§ÈõÜÊàêÔºå‰æãÂ¶Ç ./clusters/cluster-dev/apps.yaml Êñá‰ª∂ÂÜÖÂÆπÂ¶Ç‰∏ãÔºå\n1apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 2kind: Kustomization 3metadata: 4 name: apps 5 namespace: flux-system 6spec: 7 dependsOn: 8 - name: infrastructure 9 interval: 3m0s 10 serviceAccountName: kustomize-controller 11 path: ./apps/overlays/development 12 prune: true 13 sourceRef: 14 kind: GitRepository 15 name: flux-system ÊúÄ‰Ω≥ÂÆûË∑µ Kubernetes ÂéüÁîüÁöÑ RBAC ÊùÉÈôêÊéßÂà∂Êó†Ê≥ïÁªÜÁ≤íÂ∫¶ÁöÑÊéßÂà∂ËµÑÊ∫êÊùÉÈôêÔºåÂ¶ÇËµÑÊ∫êÂàõÂª∫ÂøÖÈ°ªÊåáÂÆöÊüê‰∫õ Label Á≠â„ÄÇ ‰ΩÜÁªìÂêà Policy as CodeÔºåÂ¶Ç Gatekeeper, Kyverno ÂèØ‰ª•Êª°Ë∂≥ÁªÜÁ≤íÂ∫¶ÁöÑÁÆ°ÁêÜÈúÄÊ±Ç„ÄÇ\n‰∏∫Â∫îÁî® app-a ÂàõÂª∫‰∫ÜÂ¶Ç‰∏ã PolicyÔºå‰ªÖÂÖÅËÆ∏Â∫îÁî®ÈÄöËøáËá™Âä©ÁöÑ Git ‰ªìÂ∫ìÂú®ÈÉ®ÁΩ≤Êó∂‰ªÖÂèØÂàõÂª∫ Helm Chart ÈÉ®ÁΩ≤ÂøÖÈ°ªÁöÑ Secrets„ÄÇ\n1apiVersion: kyverno.io/v1 2kind: Policy 3metadata: 4 name: restrict-secrets-by-type 5 namespace: app-a 6 annotations: 7 policies.kyverno.io/title: Restrict Secrets by Name 8 policies.kyverno.io/category: security 9 policies.kyverno.io/subject: Secret 10 policies.kyverno.io/description: \u0026gt;- 11 Disallow creating/deleting secrets in namespace \u0026#39;app-a\u0026#39; beside the helm 12 storage. 13spec: 14 background: false 15 validationFailureAction: enforce 16 rules: 17 - name: safe-secrets-for-helm-storage 18 match: 19 resources: 20 kinds: 21 - Secret 22 preconditions: 23 all: 24 - key: \u0026#39;{{request.operation}}\u0026#39; 25 operator: In 26 value: 27 - CREATE 28 - UPDATE 29 - DELETE 30 - key: \u0026#39;{{serviceAccountName}}\u0026#39; 31 operator: Equals 32 value: app-a-reconciler 33 validate: 34 message: Only Secrets are created by Helm v3+ 35 pattern: 36 type: helm.sh/release.v1 Âõõ„ÄÅÁßüÊà∑ÈöîÁ¶ª‰∏îËá™ÊúçÂä°ÁöÑÂ∫îÁî®ÊåÅÁª≠ÈÉ®ÁΩ≤ ‰∏ä‰∏ÄÊ≠•‰∏∫ÁßüÊà∑/Â∫îÁî® app-a ÈÖçÁΩÆ‰∫ÜÁã¨Á´ãÁöÑÂëΩ‰ª§Á©∫Èó¥ÔºåÈÉ®ÁΩ≤ÊùÉÈôêÔºåÁ≠ñÁï•Á≠â„ÄÇÂêåÊó∂‰∏∫Â∫îÁî® app-a ÂàõÂª∫‰∫ÜÁã¨Á´ãÁöÑ GitOps ‰ªìÂ∫ìÔºå Â∫îÁî®Âõ¢ÈòüÂèØ‰ª•ÈÄöËøáÁã¨Á´ãÁöÑ Git ‰ªìÂ∫ìËá™‰∏ªÁöÑÂèëÂ∏ÉÂÖ∂Â∫îÁî®Á®ãÂ∫èÂà∞‰∏çÂêåÁöÑ STAGING ÈõÜÁæ§„ÄÇ Â¶ÇÁ§∫‰æã‰∏≠ÁöÑ‰ªìÂ∫ìÔºåÂ∫îÁî®Âõ¢Èòü‰ΩøÁî® Kustomize ÁÆ°ÁêÜ‰∏çÂêå STAGING ÁéØÂ¢ÉÁöÑÈÉ®ÁΩ≤Ôºå‰∏îÈÄöËøá Helm ÊñπÂºèÈÉ®ÁΩ≤‰∫ÜÁîµÂïÜÂ∫îÁî® Prestashop„ÄÇ Â∫îÁî®Âõ¢ÈòüÁöÑÈÉ®ÁΩ≤ÂèØ‰ª•‰ΩøÁî®Áî± infrastructure Âõ¢ÈòüÁªü‰∏ÄÁÆ°ÁêÜÁöÑ External DNS, Ingress Class, Â∫îÁî®ÊâÄÂú®ÂëΩÂêçÁ©∫Èó¥ÁöÑ Secrets„ÄÇ\nÊúÄÁªàÂπ≥Âè∞Âõ¢ÈòüÂ∞ÜÂ∫îÁî® app-a Áã¨Á´ãÁöÑ‰ªìÂ∫ì‰Ωú‰∏∫‰∏Ä‰∏™Êñ∞ÁöÑ GitOps Êù•Ê∫êÔºåÈÄöËøáÂ¶Ç‰∏ãÈÖçÁΩÆÂ∞ÜÂ∫îÁî®‰ªìÂ∫ìÁöÑÈÉ®ÁΩ≤ÂêåÈõÜÁæ§ÂÖ≥ËÅî‰∏äÔºå\n1apiVersion: source.toolkit.fluxcd.io/v1beta1 2kind: GitRepository 3metadata: 4 name: app-a-tenant 5spec: 6 interval: 1m 7 url: https://github.com/zxkane/eks-gitops-app-a.git 8 ref: 9 branch: main 10--- 11apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 12kind: Kustomization 13metadata: 14 name: app-a-tenant 15spec: 16 serviceAccountName: app-a-reconciler 17 interval: 5m0s 18 retryInterval: 5m0s 19 prune: true 20 sourceRef: 21 kind: GitRepository 22 name: app-a-tenant 23 namespace: app-a 24 patches: 25 - patch: |- 26 - op: replace 27 path: /spec/serviceAccountName 28 value: app-a-reconciler 29 - op: replace 30 path: /metadata/namespace 31 value: app-a 32 target: 33 group: helm.toolkit.fluxcd.io 34 version: v2beta1 35 kind: HelmRelease 36 - patch: |- 37 - op: replace 38 path: /spec/serviceAccountName 39 value: app-a-reconciler 40 - op: replace 41 path: /metadata/namespace 42 value: app-a 43 target: 44 group: kustomize.toolkit.fluxcd.io 45 version: v1beta2 46 kind: Kustomization 47 - patch: |- 48 - op: replace 49 path: /namespace 50 value: app-a 51 target: 52 group: kustomize.config.k8s.io 53 version: v1beta1 54 kind: Kustomization Â∫îÁî® app-a Âõ¢ÈòüÂ∞ÜËá™Âä©ÁöÑÈÄöËøáÁã¨Á´ãÁöÑÂ∫îÁî® GitOps ‰ªìÂ∫ìÊåÅÁª≠ÂèëÂ∏ÉÂõ¢ÈòüÁöÑÂ∫îÁî®„ÄÇ Â¶Ç‰∏ãÁ§∫‰æã app-a Âú®ÂÖ∂Ëá™Âä©ÁöÑ Git ‰ªìÂ∫ìÈÄöËøá HelmRelease ÈÉ®ÁΩ≤‰∫Ü Web Â∫îÁî®„ÄÇ\n1apiVersion: helm.toolkit.fluxcd.io/v2beta1 2kind: HelmRelease 3metadata: 4 name: prestashop 5spec: 6 releaseName: prestashop 7 chart: 8 spec: 9 chart: prestashop 10 sourceRef: 11 kind: HelmRepository 12 name: bitnami 13 namespace: app-a 14 version: 14.0.10 15 values: 16 existingSecret: prestashop 17 service: 18 type: ClusterIP 19 ingress: 20 enabled: true 21 path: \u0026#39;/*\u0026#39; 22 annotations: 23 alb.ingress.kubernetes.io/scheme: internet-facing 24 alb.ingress.kubernetes.io/inbound-cidrs: \u0026#39;0.0.0.0/0\u0026#39; 25 alb.ingress.kubernetes.io/auth-type: none 26 alb.ingress.kubernetes.io/target-type: ip 27 kubernetes.io/ingress.class: alb 28 alb.ingress.kubernetes.io/ssl-policy: ELBSecurityPolicy-TLS-1-2-Ext-2018-06 29 alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTP\u0026#34;: 80}]\u0026#39; 30 alb.ingress.kubernetes.io/backend-protocol: HTTP 31 alb.ingress.kubernetes.io/healthcheck-path: \u0026#39;/\u0026#39; 32 persistence: 33 enabled: false 34 storageClass: gp2 35 # for mariadb 36 mariadb: 37 enabled: false 38 externalDatabase: 39 host: mariadb.kube-system.svc.cluster.local 40 user: prestashop 41 database: prestashop 42 existingSecret: prestashop-db-secrets 43 allowEmptyPassword: false 44 interval: 1h0m0s 45 install: 46 remediation: 47 retries: 3 ‰∫î„ÄÅËá™Âä®ÂèëÂ∏ÉÈïúÂÉèÊõ¥Êñ∞ Âú®Êú¨ËäÇÂÆûË∑µ‰∏≠Êàë‰ª¨Â∞Ü‰ΩøÁî® Sock ShopÔºà‰∏Ä‰∏™‰ΩøÁî® Spring Boot, Go kit, Node.js ÂÆπÂô®ÂåñÁöÑÂæÆÊúçÂä°Á§∫‰æãÂ∫îÁî®Ôºâ„ÄÇ ÂêåÂú®Á¨¨‰∏âÔºåÁ¨¨ÂõõÁ´†ËäÇÈÖçÁΩÆÂ∫îÁî® app-a ‰∏ÄÊ†∑Ôºå‰∏∫ sock-shop Â∫îÁî®Âú® infrastructure GitOps ‰ªìÂ∫ì‰∏≠ÂàõÂª∫‰∫ÜÂçïÁã¨ÁöÑÂëΩÂêçÁ©∫Èó¥„ÄÅRBAC„ÄÅÁã¨Á´ãÁöÑ Git ‰ªìÂ∫ìÊù•ÁÆ°ÁêÜÂ∫îÁî®ÁöÑÂèëÂ∏ÉÔºå ÂÖ∑‰ΩìÂÆûÁé∞ÂèØÂèÇËÄÉ commit1, commit2„ÄÇ\n1. ÈÉ®ÁΩ≤ÂæÆÊúçÂä°Â∫îÁî®Á®ãÂ∫è Sock Shop Âú®Êàë‰ª¨ÂàÜÂèâÁöÑ Sock Shop ÈÄöËøá Kustomization ÂÆûÁé∞‰∫ÜÂ§öÈõÜÁæ§ÈÉ®ÁΩ≤ÁöÑÊîØÊåÅÔºå ÂêåÊó∂Â∞Ü front-end ÊúçÂä°ÈÄöËøá LoadBalancer Á±ªÂûãÂØπÂ§ñÊö¥Èú≤Âá∫Êù•ÔºåÂà©Áî® Amazon EKS Âêå Amazon Elastic Load Balancing ÁöÑÈõÜÊàêÊù•Ë¥üËΩΩÂùáË°° Sock Shop Â∫îÁî®ÁöÑÂÖ•Âè£ front-end ÊúçÂä°„ÄÇ\n1apiVersion: kustomize.config.k8s.io/v1beta1 2kind: Kustomization 3resources: 4 - ./complete-demo.yaml 5patchesStrategicMerge: 6 - delete-ns.yaml 7patches: 8 - patch: |- 9 - op: replace 10 path: /spec/type 11 value: LoadBalancer 12 - op: replace 13 path: /metadata/annotations/service.beta.kubernetes.io~1aws-load-balancer-type 14 value: external 15 - op: replace 16 path: /metadata/annotations/service.beta.kubernetes.io~1aws-load-balancer-nlb-target-type 17 value: ip 18 - op: replace 19 path: /metadata/annotations/service.beta.kubernetes.io~1aws-load-balancer-scheme 20 value: internet-facing 21 target: 22 version: v1 23 kind: Service 24 name: front-end ÈÄöËøáÂÆöÂà∂Âåñ front-end ÂæÆÊúçÂä°‰∏∫Êàë‰ª¨ÁöÑ Sock Shop Â∫îÁî®ÊåÅÁª≠ÊîπËøõÔºåÊúÄÊñ∞ÁöÑ front-end ÈÄöËøáËá™Âä®ÂåñÊµãËØïÂêéÊâìÂåÖÁöÑÈïúÂÉèÁâàÊú¨ÈÄöËøá Github packages ÂÆπÂô®ÈïúÂÉè‰ªìÂ∫ìÂØπÂ§ñÂèëÂ∏É„ÄÇ Êàë‰ª¨Âú® DEV ÁéØÂ¢ÉÂ∞Ü‰ΩøÁî® Kustomization overlays Â∞Ü front-end ÂæÆÊúçÂä°ÊõøÊç¢‰∏∫ÂÆöÂà∂ÂåñÊõ¥Êñ∞ÁöÑÁâàÊú¨„ÄÇ\n1apiVersion: kustomize.config.k8s.io/v1beta1 2kind: Kustomization 3resources: 4 - ../../base 5patches: 6 - patch: |- 7 - op: replace 8 path: /metadata/annotations/external-dns.alpha.kubernetes.io~1hostname 9 value: socks-dev.test.kane.mx 10 target: 11 version: v1 12 kind: Service 13 name: front-end 14images: 15- name: weaveworksdemos/front-end 16 newName: ghcr.io/zxkane/weaveworksdemos/front-end 17 newTag: 0.3.13-rc0 Âú® DEV Á≠âÂèØÊåÅÁª≠ÈõÜÊàêÁöÑÊïèÊç∑ÁéØÂ¢ÉÔºåÂú®ÊûÑÂª∫Êñ∞ÊúçÂä°ÈïúÂÉè‰∏îÂèëÂ∏ÉÂêéÔºåÈÄöËøá‰∫∫Â∑•ÊàñËÑöÊú¨Êõ¥Êñ∞ GitOps ‰ª£Á†Å‰ªìÂ∫ìÊòæÂæóËøá‰∫éÁπÅÁêê„ÄÇ Flux Ëá™Ë∫´Êèê‰æõ‰∫ÜÂÆåÂñÑ‰∏îÂº∫Â§ßÁöÑ Git ‰ªìÂ∫ìÈïúÂÉèËá™Âä®ÂçáÁ∫ßÂäüËÉΩ„ÄÇ‰∏ãÈù¢Âú®Êàë‰ª¨ÁöÑ GitOps ÈÉ®ÁΩ≤‰ªìÂ∫ìÊù•ÂÆûÁé∞ËØ•ËÉΩÂäõ„ÄÇ\nÊ≥®ÊÑè ÈïúÂÉèËá™Âä®Êõ¥Êñ∞ÂäüËÉΩÈúÄË¶ÅÁ°Æ‰øù Flux Âú®ÂÆâË£ÖÈÖçÁΩÆÊó∂Â∑≤ÂêØÁî®ÈïúÂÉèËá™Âä®Êõ¥Êñ∞ÁªÑ‰ª∂„ÄÇÂ¶ÇÊú™ÂêØÁî®ÔºåÂèØÈáçÂ§ç bootstrap Flux Êó∂Âä†‰∏ä --components-extra=image-reflector-controller,image-automation-controller ÂèÇÊï∞Êù•ÂêØÁî®„ÄÇ\n2. Ê≥®ÂÜå front-end ÂæÆÊúçÂä°ÁöÑÈïúÂÉè‰ªìÂ∫ì 1apiVersion: image.toolkit.fluxcd.io/v1beta1 2kind: ImageRepository 3metadata: 4 name: sock-shop-front-end 5spec: 6 image: ghcr.io/zxkane/weaveworksdemos/front-end 7 interval: 1m0s 3. ËÆæÁΩÆÈïúÂÉèÊõ¥Êñ∞Á≠ñÁï• Â¶Ç‰∏ãËßÑÂàô ^0.3.x-0 Â∞ÜÂåπÈÖç 0.3.13-rc0, 0.3.13-rc1, 0.3.13 Á≠âÈïúÂÉèÁâàÊú¨„ÄÇ\n1apiVersion: image.toolkit.fluxcd.io/v1beta1 2kind: ImagePolicy 3metadata: 4 name: sock-shop-front-end 5spec: 6 imageRepositoryRef: 7 name: sock-shop-front-end 8 policy: 9 semver: 10 range: \u0026#39;^0.3.x-0\u0026#39; 4. ÂàõÂª∫Ëá™Âä®ÈïúÂÉèÊõ¥Êñ∞ÈÖçÁΩÆ Flux Ëá™Âä®ÈïúÂÉèÈÖçÁΩÆ‰ºöÊåáÂÆöÂ∫îÁî®ÈÖçÁΩÆÁöÑ Git ‰ªìÂ∫ìÔºåÂåÖÊã¨ÂàÜÊîØ„ÄÅË∑ØÂæÑÁ≠â‰ø°ÊÅØ„ÄÇ\n1apiVersion: image.toolkit.fluxcd.io/v1beta1 2kind: ImageUpdateAutomation 3metadata: 4 name: sock-shop-front-end 5spec: 6 git: 7 checkout: 8 ref: 9 branch: gitops 10 commit: 11 author: 12 email: fluxcdbot@users.noreply.github.com 13 name: fluxcdbot 14 messageTemplate: \u0026#39;{{range .Updated.Images}}{{println .}}{{end}}\u0026#39; 15 push: 16 branch: gitops 17 interval: 1m0s 18 sourceRef: 19 kind: GitRepository 20 name: sock-shop-tenant 21 namespace: sock-shop 22 update: 23 path: ./deploy/kubernetes/overlays/development 24 strategy: Setters 5. ‰∏∫Â∫îÁî® GitOps ‰ªìÂ∫ìÈÖçÁΩÆËØªÂÜôÂá≠ËØÅ Áî±‰∫é Flux ÈúÄË¶ÅÂ∞ÜÊõ¥Êñ∞ÂêéÁöÑÈïúÂÉèÁâàÊú¨‰ø°ÊÅØÊèê‰∫§ÂõûÂ∫îÁî®‰ªìÂ∫ìÔºåÈúÄË¶Å‰∏∫ Flux ‰∏≠ÈÖçÁΩÆÁöÑÂ∫îÁî® GitRepository ÊåáÂÆöÂèØËØªÂÜôÁöÑËÆøÈóÆÂá≠ËØÅ„ÄÇ ‰∏ãÈù¢Êèê‰æõÂèÇËÄÉÊ≠•È™§ÂàõÂª∫ Git ‰ªìÂ∫ìËÆøÈóÆÂá≠ËØÅÂπ∂ÈÖçÁΩÆ„ÄÇ\n1. ÂàõÂª∫ Sealed Secret ‰øùÂ≠ò Git ‰ªìÂ∫ìËØªÂÜôÊùÉÈôêÁöÑÁßÅÈí• 1kubectl -n sock-shop create secret generic flux-image-automation \\ 2--from-file=identity=/path/gitops-image-update-id-ecdsa \\ 3--from-file=identity.pub=/path/gitops-image-update-id-ecdsa.pub \\ # Á°Æ‰øùÊ≠§ÂÖ¨Èí•Â∑≤ÈÖçÁΩÆÂú® Git ‰ªìÂ∫ì‰∏îÂÖ∑ÊúâËØªÂÜôÊùÉÈôêÔºåÂ¶Ç Github ‰ªìÂ∫ìÁöÑ `Deploy Keys` 4--from-literal=known_hosts=\u0026#34;github.com ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBEmKSENjQEezOmxkZMy7opKgwFB9nkt5YRrYMjNuG5N87uRgg6CLrbo5wAdT/y6v0mKV0U2w0WZ2YB/++Tpockg=\u0026#34; \\ 5--dry-run=client \\ 6-o yaml \u0026gt; flux-image-automation-secrets.yaml 7 8kubeseal --format=yaml --cert=pub-sealed-secrets-dev.pem \\ 9\u0026lt; flux-image-automation-secrets.yaml \u0026gt; ./apps/overlays/development/sock-shop/sealed-git-token.yaml 2. ÈÄöËøá Kustomize ‰∏∫ DEV ÁéØÂ¢ÉÁöÑ GitRepository ÈÖçÁΩÆÊåáÂÆöËÆøÈóÆÂá≠ËØÅ 1apiVersion: kustomize.config.k8s.io/v1beta1 2kind: Kustomization 3namespace: sock-shop 4resources: 5 - ../../../base/sock-shop 6 - ./sealed-slack-secrets.yaml 7 - ./sealed-git-token.yaml 8 - ./registry.yaml 9 - ./policy.yaml 10 - ./image-automation.yaml 11patches: 12 - patch: |- 13 - op: replace 14 path: /spec/path 15 value: ./deploy/kubernetes/overlays/development 16 target: 17 group: kustomize.toolkit.fluxcd.io 18 version: v1beta2 19 kind: Kustomization 20 name: sock-shop-tenant 21 - patch: | 22 - op: replace 23 path: /spec/channel 24 value: gitops-flux 25 target: 26 group: notification.toolkit.fluxcd.io 27 version: v1beta1 28 kind: Provider 29 name: slack 30 - patch: | 31 - op: replace 32 path: /spec/url 33 value: git@github.com:zxkane/microservices-demo.git 34 - op: replace 35 path: /spec/secretRef 36 value: {} 37 - op: replace 38 path: /spec/secretRef/name 39 value: flux-image-automation 40 target: 41 group: source.toolkit.fluxcd.io 42 version: v1beta1 43 kind: GitRepository 44 name: sock-shop-tenant 6. È™åËØÅÈïúÂÉèËá™Âä®Êõ¥Êñ∞ Êõ¥Êñ∞ÂæÆÊúçÂä° front-end ‰ª£Á†Å‰∏îtagÁâàÊú¨ÂêéÔºåÊñ∞ÁöÑÈïúÂÉèÁâàÊú¨Ë¢´ÂèëÂ∏ÉÂà∞ÈïúÂÉè‰ªìÂ∫ì„ÄÇ ÈÄöËøáÂâçÈù¢ÈÖçÁΩÆÁöÑ ImageRepository Âíå ImagePolicy Êâ´ÊèèÂà∞ front-end ÈïúÂÉèÁ¨¶ÂêàÁ≠ñÁï•ÁöÑÊñ∞ÁâàÊú¨ÂèëÂ∏ÉÔºå Ê†πÊçÆ ImageUpdateAutomation ÈÖçÁΩÆÁöÑ Sock Shop Â∫îÁî®‰ªìÂ∫ìÔºåÊü•ÊâæÊåáÂÆöÁöÑÈïúÂÉèÂèòÈáèÔºå Flux ÁöÑ image-automation-controller Ëá™Âä®Â∞ÜÊõ¥Êñ∞ÁöÑÈïúÂÉè‰ø°ÊÅØÊèê‰∫§Âà∞Â∫îÁî®‰ªìÂ∫ìÂÆûÁé∞ÊåÅÁª≠ÈÉ®ÁΩ≤„ÄÇ\nÂõæ1ÔºöÈïúÂÉèËá™Âä®Êõ¥Êñ∞Ê∂àÊÅØÈÄöÁü• ÂÖ≠„ÄÅÂ∞èÁªìÂèäÂ±ïÊúõ Êú¨Êñá‰ªãÁªç‰∫ÜÂ¶Ç‰Ωï‰ΩøÁî® GitOps Â∑•ÂÖ∑ FluxCD v2 ÊûÑÂª∫‰ºÅ‰∏öÂÜÖÈÉ®Âú® Kubernetes ‰∏äÊåÅÁª≠‰∫§‰ªòÂÖ±‰∫´ÊúçÂä°Âπ≥Âè∞Ôºå Â∞ÜÂπ≥Âè∞Âõ¢ÈòüÂíåÂ∫îÁî®/‰∏öÂä°Âõ¢ÈòüÁªü‰∏ÄÂú®ÂêåÊ†∑ÁöÑ Git Â∑•‰ΩúÊµÅÁ®ã‰∏ãÔºåÂêåÊó∂ÊéàÊùÉÂ∫îÁî®/‰∏öÂä°Âõ¢ÈòüÁî®Ëá™ÊúçÂä°ÁöÑÊñπÂºèÊåÅÁª≠‰∫§‰ªòÂ∫îÁî®ÁöÑÊïèÊç∑ÈÉ®ÁΩ≤„ÄÇ Ê≠§ÊñπÊ°àÂ∞ÜÂÆâÂÖ®ÂíåÊïàÁéáÊúâÊïàÁöÑÁªìÂêàÂú®‰∏ÄËµ∑„ÄÇÂâçËø∞ÁöÑÁ§∫‰æãÂèØÂú®Ê≠§‰ªìÂ∫ìËé∑ÂèñÂÆåÊï¥ÁöÑ GitOps ‰ª£Á†Å„ÄÇ\nÂêåÊó∂Èù¢ÂØπÂ§çÊùÇÁöÑ‰ºÅ‰∏öÂú∫ÊôØÔºåËøòÊúâ‰∏Ä‰∫õÊñπÈù¢ËøòÂèØ‰ª•ÊåÅÁª≠ÁöÑ‰ºòÂåñÔºå‰æãÂ¶ÇÔºå\nÈù¢ÂØπÂÖ≥ÈîÆÁöÑÁ∫ø‰∏äÁîü‰∫ßÁ≥ªÁªüÔºåÂ¶Ç‰ΩïÂÆâÂÖ®Â¢ûÈáèÁöÑÁÅ∞Â∫¶ÂèëÂ∏ÉÔºü Sealed Secrets ÂºïÂÖ•‰∫ÜÈ¢ùÂ§ñÁöÑÁßÅÈí•ÁÆ°ÁêÜÈúÄÊ±ÇÔºåÂú®‰∫ëËÆ°ÁÆóÁéØÂ¢ÉÂ¶Ç‰ΩïÊîπÂñÑ GitOps ÂØÜÈí•ÁöÑÁÆ°ÁêÜÔºü Â¶Ç‰ΩïÂ∞Ü‰∫ëÂπ≥Âè∞ÁöÑËµÑÊ∫ê IaC Âêå Kubernetes ÂÜÖËµÑÊ∫ê GitOps ÂçèÂêåÁÆ°ÁêÜÔºü Â¶Ç‰ΩïÊõ¥Âä†È´òÊïàÁöÑÂºÄÂèë Kubernetes manifests(YAML)Ôºü Â∞ÜÂú®ÂêéÁª≠ÁöÑÊñáÁ´†‰∏≠ÈÄê‰∏™Êé¢ËÆ®Ëøô‰∫õÈóÆÈ¢ò„ÄÇ\n","link":"https://kane.mx/posts/gitops/flux-in-action-2/","section":"posts","tags":["GitOps","Kubernetes","Flux","Git","EKS","CD","Continuous Delivery"],"title":"Âü∫‰∫é Flux ÁöÑ GitOps ÂÆûÊàòÔºà‰∏ãÔºâ"},{"body":"Âú®ÂâçÊñá‰ªãÁªç‰∫Ü GitOps ÁöÑÊ¶ÇÂøµÔºåKubernetes ‰∏ä GitOps ÊúÄ‰Ω≥ÂÆûË∑µ‰ª•ÂèäÂØπÊØî‰∫Ü CNCF Âü∫Èáë‰ºö‰∏ã ‰∫ëÂéüÁîüÁöÑ GitOps Â∑•ÂÖ∑ÔºàArgoCD Âíå FluxÔºâ„ÄÇÊú¨ÁØáÂ∞ÜÂ∏¶‰Ω†ÊåâÁÖß Flux ÁöÑÊúÄ‰Ω≥ÂÆûË∑µÂú®Ë∑®VPCË∑®Ë¥¶Êà∑ÁöÑ Kubernetes ‰∏äÂÆûË∑µ GitOps ÁöÑÊåÅÁª≠ÈõÜÊàêÔºåËΩªÊùæÁÆ°ÁêÜÊï∞ÂçÅÊï∞Áôæ‰πÉËá≥Êõ¥Â§öÁöÑÈõÜÁæ§ÂèäÈÉ®ÁΩ≤Âú®‰∏äÈù¢ÁöÑÂ∫îÁî®„ÄÇ\n0. ÂøÖÂ§áÊù°‰ª∂ ÂÅáËÆæ‰∏öÂä°ÂØπÁ®≥ÂÆöÊÄßÁöÑÈúÄÊ±ÇÔºå‰ΩøÁî®3‰∏™ Kubernetes ÈõÜÁæ§ÂàÜÂà´ÂØπÂ∫î DEV, STAGING Âíå PRODUCT ÁéØÂ¢É„ÄÇËøô‰∫õÈõÜÁæ§ÁéØÂ¢ÉÊ†πÊçÆ‰ºÅ‰∏öÁöÑÈúÄÊ±Ç ÂèØËÉΩ‰ºöÂàÜÂ∏ÉÂú®‰∏çÂêåÁöÑ‰∫ëË¥¶Êà∑ÂíåVPCÁΩëÁªú‰∏≠„ÄÇËØªËÄÖÂèØÊ†πÊçÆÂÆûÈôÖ‰ºÅ‰∏öÊÉÖÂÜµÂàõÂª∫‰∏Ä‰∏™ÊàñÂ§ö‰∏™ÈõÜÁæ§„ÄÇÊú¨Êñá‰ª• Amazon EKS ‰∏∫‰æãÔºåEKSÈõÜÁæ§ÁöÑÂàõÂª∫ËØ∑ÂèÇÈòÖÂÖ∂ÊñáÊ°£„ÄÇ Git ‰ªìÂ∫ìÁî®‰∫é‰øùÂ≠òÈõÜÁæ§ÁöÑÂ£∞ÊòéÂºèÈÖçÁΩÆ„ÄÇFlux ÊîØÊåÅ Git Âú®Á∫øÊúçÂä°ÔºàÂåÖÊã¨ Github, Gitlab, BitbucketÔºâÂíåÂÖ∂‰ªñ‰ªªÊÑè Git ÊúçÂä°„ÄÇÊú¨ÊñáÂ∞Ü‰ΩøÁî® Github ÊâòÁÆ° Git ‰ªìÂ∫ì‰∏∫‰æã„ÄÇ ÂÆâË£Ö Flux CLI 1. Kubernetes ÈõÜÁæ§ÂÆâË£ÖÈÖçÁΩÆ Flux Github repo ‰∏∫‰æãÔºåÊâßË°å‰ª•‰∏ãÂëΩ‰ª§Ôºå\n1export GITHUB_TOKEN=\u0026lt;your-token\u0026gt; 2 3flux bootstrap github \\ 4 --components-extra=image-reflector-controller,image-automation-controller \\ 5 --owner=zxkane \\ 6 --repository=eks-gitops \\ 7 --path=clusters/cluster-dev \\ 8 --personal ÈáçË¶Å ËØ∑Á°Æ‰øù Flux CLI ÊâßË°åÁéØÂ¢ÉÂèØ‰ª•ÈÄöËøá kubectl ËøûÊé•Âà∞ Kubernetes ÈõÜÁæ§Ôºå‰∏îÁî®Êà∑ÂÖ∑Â§á admin ÊùÉÈôê„ÄÇ\nÈáçË¶Å ÂàõÂª∫ÁöÑ Github Personal Accesss Token ÈúÄË¶ÅËá≥Â∞ëÂêåÊó∂ÈÄâ‰∏≠ÂÖ®ÈÉ® repo Âíå user ÁöÑÊùÉÈôê„ÄÇ\nÊ≥®ÊÑè Â¶ÇÈúÄÂú® DEV ÁéØÂ¢É ÂêØÁî®ÈïúÂÉèËá™Âä®Êõ¥Êñ∞ÂäüËÉΩÔºåbootstrap Flux Êó∂ÈúÄË¶ÅÂä†‰∏ä --components-extra=image-reflector-controller,image-automation-controller ÂèÇÊï∞„ÄÇ\nÈÄöËøáÁ±ª‰ººÁöÑÊ≠•È™§Âú® STAGING Âíå PRODUCT ÈõÜÁæ§ÂÆâË£ÖÈÖçÁΩÆ Flux „ÄÇ\n1export KUBECONFIG=$HOME/.kube/config-cluster-staging 2flux bootstrap github \\ 3 --owner=zxkane \\ 4 --repository=eks-gitops \\ 5 --path=clusters/cluster-staging \\ 6 --personal 7 8export KUBECONFIG=$HOME/.kube/config-cluster-product 9flux bootstrap github \\ 10 --owner=zxkane \\ 11 --repository=eks-gitops \\ 12 --path=clusters/cluster-product \\ 13 --personal ‰ª•‰∏äÊ≠•È™§ÊòØÊâãÂä®ÂÆâË£ÖÂèäÈÖçÁΩÆ Flux ÔºåFlux ‰πüÊîØÊåÅÂêåÁé∞ÊúâÁöÑ IaC ‰ª£Á†ÅÈõÜÊàêÔºåÂ¶Ç eksctl, Terraform„ÄÇ\nÊúÄ‰Ω≥ÂÆûË∑µ ‰∏äÈù¢Á§∫‰æãÂØπÂ§öÁéØÂ¢ÉÈõÜÁæ§ÁöÑÊîØÊåÅÂπ∂Ê≤°ÊúâÈááÁî®Â§ö‰ªìÂ∫ì/Â§öÂàÜÊîØÁöÑÁ≠ñÁï•ÔºåËÄåÊòØÁî®ÁöÑ‰ΩøÁî®‰∏çÂêåË∑ØÂæÑÊù•ÁÆ°ÁêÜ‰∏çÂêåÁöÑÈõÜÁæ§„ÄÇ Ëøô‰πüÊòØ Flux Êé®ËçêÁöÑÁ≠ñÁï•ÔºåÂèØ‰ª•ÂáèÂ∞ë‰ª£Á†ÅÁª¥Êä§ÂíåÂêàÂπ∂ÁöÑÈöæÂ∫¶„ÄÇ\n1./clusters/ 2‚îú‚îÄ‚îÄ cluster-dev [ÈõÜÁæ§ÂêçÁß∞] 3‚îÇ¬†‚îú‚îÄ‚îÄ flux-system [ÂëΩÂêçÁ©∫Èó¥] 4‚îÇ¬†‚îú‚îÄ‚îÄ gotk-components.yaml [ÈªòËÆ§ Flux ÈÖçÁΩÆÔºåËØ∑ÂãøÊâãÂä®‰øÆÊîπ] 5‚îÇ¬†‚îú‚îÄ‚îÄ gotk-sync.yaml [ÈªòËÆ§ Flux ÈÖçÁΩÆÔºåËØ∑ÂãøÊâãÂä®‰øÆÊîπ] 6‚îÇ¬†‚îî‚îÄ‚îÄ kustomize.yaml [Kustomize ÈÖçÁΩÆÂÖ•Âè£Êñá‰ª∂ÔºåÂ∞ÜÈÄöËøáÊ≠§ÂÖ•Âè£ËÅöÂêà‰∫ÜÈõÜÁæ§ÁöÑÂÖ®ÈÉ®ÈÖçÁΩÆ] 7‚îú‚îÄ‚îÄ cluster-product 8‚îÇ¬†‚îú‚îÄ‚îÄ flux-system 9‚îÇ¬†‚îú‚îÄ‚îÄ gotk-components.yaml 10‚îÇ¬†‚îú‚îÄ‚îÄ gotk-sync.yaml 11‚îÇ¬†‚îî‚îÄ‚îÄ kustomize.yaml 12‚îú‚îÄ‚îÄ cluster-staging 13‚îÇ¬†‚îú‚îÄ‚îÄ flux-system 14‚îÇ¬†‚îú‚îÄ‚îÄ gotk-components.yaml 15‚îÇ¬†‚îú‚îÄ‚îÄ gotk-sync.yaml 16‚îÇ¬†‚îî‚îÄ‚îÄ kustomize.yaml Âú®ÂÆåÊàêÂàùÂßãÂåñ‰∏çÂêåÁöÑÁéØÂ¢ÉÈõÜÁæ§ÂêéÔºåÂ∞ÜÂú®Êàë‰ª¨ÁöÑGit‰ªìÂ∫ì‰∏≠Êü•ÁúãÂà∞Â¶Ç‰∏äÁõÆÂΩïÁªìÊûÑ„ÄÇ Êàë‰ª¨ÂèØ‰ª•ÁúãÂà∞ Flux Ëá™Ë∫´ÁöÑÈÖçÁΩÆ‰πüÊòØÈÄöËøá GitOps ÁöÑÊñπÂºèÊù•ÁÆ°ÁêÜÁöÑ„ÄÇ\n2. ÁÆ°ÁêÜÈõÜÁæ§ÂÖ±‰∫´ÁöÑÁªÑ‰ª∂ Âú®‰ºÅ‰∏ö‰∏≠ÈÄöÂ∏∏‰ºöÁî± Infrastructure Âõ¢ÈòüÁªü‰∏ÄÁÆ°ÁêÜÈõÜÁæ§ÁöÑÂÖ±‰∫´ÁªÑ‰ª∂Ôºå‰æãÂ¶ÇÔºåNamespace, CSI Driver, Ingress Class, Persist Volume, Service Account, Secret, DaemonSet, NetworkPolicyÔºåCustomResource Á≠âÁ≠â Kubernetes ÂØπË±°„ÄÇ Êé•‰∏ãÊù•Â∞ÜÊºîÁ§∫Â¶Ç‰ΩïÂú®Â§öÈõÜÁæ§‰∏≠ÂàõÂª∫ÈõÜÁæ§ÂÜÖÂÖ±‰∫´ÁªÑ‰ª∂Ôºå‰æãÂ¶ÇÔºåAWS Load Balancer Controller Âíå External DNSÔºå Âπ∂‰∏îÈÄêÊ≠•Â∞ÜËøô‰∫õÁªÑ‰ª∂ÈÉ®ÁΩ≤Âú®‰∏çÂêåÁöÑÁéØÂ¢É‰∏≠„ÄÇ\nFlux Ëá™Ë∫´Â§ßÈáè‰æùËµñ‰∫Ü KustomizeÔºåÈÄöËøá Flux ÁöÑ Kustomize Controller Êù•Ê∏≤ÊüìÊúÄÁªàÁöÑ Kubernetes Â£∞ÊòéÂºèÈÖçÁΩÆÔºåÂπ∂ÈõÜÊàê‰∫Ü HookÔºåServiceAccountÔºåË∂ÖÊó∂Á≠âÈ¢ùÂ§ñÈÖçÁΩÆ„ÄÇ\nÈÄöËøáÂ¶Ç‰∏ãFlux KustomizeÂØπË±°Â£∞Êòé‰∏∫DEVÁéØÂ¢ÉÂ£∞Êòé‰∫ÜÂÖ±‰∫´ Infrastructure ÈÖçÁΩÆÊâÄÂú®ÁöÑË∑ØÂæÑÔºàËØ•ÈÖçÁΩÆÊñá‰ª∂ÊîæÁΩÆÂú®cluster/cluster-devÁõÆÂΩï‰∏ãÔºâÔºå\n1apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 2kind: Kustomization 3metadata: 4 name: infrastructure 5 namespace: flux-system 6spec: 7 interval: 10m0s 8 path: ./infrastructure/overlays/development 9 prune: true 10 sourceRef: 11 kind: GitRepository 12 name: flux-system ‰ª• External DNS(‰∏Ä‰∏™ CNCF Âü∫Èáë‰ºöÈ°πÁõÆÔºå‰∏∫ K8S Service LoadBalancer / Ingress ÂØπË±°Êèê‰æõ DNS ÂüüÂêçËß£ÊûêÊ≥®ÂÜå) ‰∏∫ÂÆåÊï¥Á§∫‰æã„ÄÇ\n‰ΩøÁî® Flux ÁöÑ Helm Repositories Ëá™ÂÆö‰πâÂØπË±°ÔºåÊ≥®ÂÜå bitnami ÁöÑ Helm Charts ‰ªìÂ∫ì„ÄÇ\n1apiVersion: source.toolkit.fluxcd.io/v1beta1 2kind: HelmRepository 3metadata: 4 name: bitnami 5spec: 6 interval: 30m 7 url: https://charts.bitnami.com/bitnami ÊåâÁÖß External DNS for Amazon Route 53 ÁöÑÊñáÊ°£‰∏∫ external-dns POD ÂàõÂª∫ÊâßË°å IAM ËßíËâ≤Ôºå ÂèØ‰ª•ÈÄöËøá Route 53 API Êù•ÂàõÂª∫‰øÆÊîπÁõ∏Â∫îÁöÑÂüüÂêçËß£Êûê„ÄÇÈíàÂØπ External DNS ÈÉ®ÁΩ≤ÁöÑÂú® K8S ÈõÜÁæ§ÈÖçÁΩÆÂ¶Ç‰∏ãÔºå\n‰∏∫ External DNS ÂàõÂª∫Áã¨Á´ãÁöÑ service accountÔºåÂêåÂØπÂ∫îÁöÑ AWS IAM Role ÁªëÂÆöÔºåÈôêÂà∂ËØ• Pod ‰ªÖÊã•ÊúâÂøÖÈúÄÁöÑÊúÄÂ∞èÊùÉÈôê„ÄÇ ÂÖ≥‰∫é EKS ‰∏äÂ¶Ç‰ΩïÁªëÂÆöÊúÄÂ∞è AWS ÊùÉÈôêÂà∞ pod ‰∏äËØ∑ÂèÇËÄÉIAM roles for service accounts„ÄÇ 1apiVersion: v1 2kind: ServiceAccount 3metadata: 4 name: external-dns 5 annotations: 6 # create IAM role via following docs, 7 # https://docs.aws.amazon.com/eks/latest/userguide/specify-service-account-role.html 8 # https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/aws.md#iam-permissions 9 # the role specified by kustomize 10 # eks.amazonaws.com/role-arn: arn:aws:iam::123456789012:role/external-dns-role 11 eks.amazonaws.com/sts-regional-endpoints: true ÂÆö‰πâ HelmRelease Flux ÂØπË±° ‰ªé Bitnami ÁöÑ Helm Charts ‰ªìÂ∫ìÂÆâË£Ö external-dns „ÄÇ 1apiVersion: helm.toolkit.fluxcd.io/v2beta1 2kind: HelmRelease 3metadata: 4 name: external-dns 5spec: 6 releaseName: external-dns 7 targetNamespace: kube-system 8 chart: 9 spec: 10 chart: external-dns 11 version: \u0026#39;\u0026gt;=6.2.1 \u0026lt;7\u0026#39; 12 sourceRef: 13 kind: HelmRepository 14 name: bitnami 15 namespace: kube-system 16 interval: 1h0m0s 17 install: 18 remediation: 19 retries: 3 20 values: 21 provider: aws 22 aws: 23 zoneType: public 24 serviceAccount: 25 create: false 26 name: external-dns 27 podSecurityContext: 28 fsGroup: 65534 29 runAsUser: 0 ‰ΩøÁî® Kustomization Â∞ÜÁõ∏ÂÖ≥ÁöÑÈÖçÁΩÆÊï¥Âêà„ÄÇ 1apiVersion: kustomize.config.k8s.io/v1beta1 2kind: Kustomization 3namespace: kube-system 4resources: 5 - serviceaccount.yaml 6 - release.yaml Áî® Kustomization Êï¥ÂêàÂ§ö‰∏™ÁªÑ‰ª∂ÁöÑÈÖçÁΩÆ„ÄÇ 1apiVersion: kustomize.config.k8s.io/v1beta1 2kind: Kustomization 3resources: 4 - sources 5 - aws-load-balancer-controller 6 - dns ‰ª•‰∏äÊâÄÊúâÈÖçÁΩÆÈÉΩ‰øùÂ≠òÂú®Git‰ªìÂ∫ìinfrastructure/base‰∏ãÔºàËØ¶ËßÅ‰∏ãÂõæÔºâ‰Ωú‰∏∫Â§öÂ•óÁéØÂ¢ÉÈÄöÁî®ÁöÑÈÖçÁΩÆÔºåÊåâÁÖß Kustomize ÁöÑ Overlays Â∏ÉÂ±Ä„ÄÇ\n1./infrastructure/ 2|-- base 3| |-- aws-load-balancer-controller 4| | |-- kustomization.yaml 5| | |-- release.yaml 6| | `-- serviceaccount.yaml 7| |-- dns 8| | |-- kustomization.yaml 9| | |-- release.yaml 10| | `-- serviceaccount.yaml 11| |-- kustomization.yaml 12| `-- sources 13| |-- bitnami.yaml 14| |-- eks-charts.yaml 15| `-- kustomization.yaml 16`-- overlays 17 |-- development 18 | |-- aws-load-balancer-controller-patch.yaml 19 | |-- aws-load-balancer-serviceaccount-patch.yaml 20 | |-- dns-patch.yaml 21 | |-- dns-sa-patch.yaml 22 | `-- kustomization.yaml Âú®DEVÁéØÂ¢ÉÂØπÂ∫îÁöÑ overlay ‰∏ãÈù¢ÂàõÂª∫Â¶Ç‰∏ãÁöÑË°•‰∏ÅÊù•Ë¶ÜÁõñË∑üDEVÁéØÂ¢ÉÁõ∏ÂÖ≥ÁöÑ‰ø°ÊÅØÂ£∞ÊòéÔºåÂ¶ÇÈõÜÁæ§ÂêçÁß∞„ÄÅÂüüÂêç„ÄÅ ‰∏∫External DNS podÊâÄÂàõÂª∫AWS IAM RoleÁöÑARN„ÄÇ\n1apiVersion: helm.toolkit.fluxcd.io/v2beta1 2kind: HelmRelease 3metadata: 4 name: external-dns 5spec: 6 values: 7 txtOwnerId: gitops-cluster 8 domainFilters[0]: test.kane.mx 9 policy: sync 10--- 11apiVersion: v1 12kind: ServiceAccount 13metadata: 14 name: external-dns 15 annotations: 16 eks.amazonaws.com/role-arn: \u0026gt;- 17 arn:aws:iam::845861764576:role/gitops-cluster-external-dns-role ÊúÄ‰Ω≥ÂÆûË∑µ ÂÖÖÂàÜÂà©Áî® Kustomize ÁöÑ Overlays Êú∫Âà∂Êù•ÊäΩË±°ÈÄöÁî®ÁöÑÈÖçÁΩÆÂíåË¶ÜÁõñÊØè‰∏™ÁéØÂ¢ÉÊâÄÂØπÂ∫îÁöÑÁâπÊÆäÈÉ®ÂàÜ„ÄÇ\nÊúÄ‰Ω≥ÂÆûË∑µ Â∞ÜÂÖ±‰∫´ÁªÑ‰ª∂ÈÉ®ÁΩ≤Âú®Èùû Flux ÂëΩÂêçÁ©∫Èó¥(ÈªòËÆ§flux-system)ÔºåÈÅøÂÖçÊ∏ÖÁêÜ Flux Êó∂ÂΩ±ÂìçËøêË°å‰∏≠ÁöÑÈÉ®ÁΩ≤„ÄÇ\nÂêåÊ†∑Âú®DEVÁéØÂ¢ÉÈ™åËØÅExternal DNSÁªÑ‰ª∂ÈÉ®ÁΩ≤ÊàêÂäüÂêéÔºåÂ∞ÜÁõ∏‰ººÁöÑÈÖçÁΩÆÂ∫îÁî®Âà∞STAGINGÂíåPRODUCTÁéØÂ¢É„ÄÇ ÈÄöËøáKustomizeÁöÑOverlaysÂàÜÂà´ËÆæÁΩÆSTAGINGÂíåPRODUCTÁéØÂ¢ÉÁõ∏ÂÖ≥ÁöÑÈÖçÁΩÆ„ÄÇÂÜçÂ∞ÜÂèòÊõ¥Êé®ÈÄÅÂà∞Git‰ªìÂ∫ìÔºå FluxÂ∞Ü‰ºö‰∏∫Êàë‰ª¨ÈÉ®ÁΩ≤Ëøô‰∫õÂ£∞ÊòéÂú®Git‰ªìÂ∫ì‰∏≠ÁöÑÁªÑ‰ª∂ÔºÅÂèØÊü•ÈòÖDEV, STAGING, PRODUCTËøô‰∏â‰∏™Êèê‰∫§Êü•ÁúãÂÆåÊï¥ÂÆûÁé∞„ÄÇ\n3. ÂØÜÈí•ÁöÑÁÆ°ÁêÜ ÊúÄ‰Ω≥ÂÆûË∑µ 2022/06Êõ¥Êñ∞Ôºö‰ΩøÁî®ÊàêÁÜüÁöÑK8SÈõÜÁæ§Â§ñÁΩÆÁöÑÂØÜÈí•ÁÆ°ÁêÜÊúçÂä°ÂèØ‰ª•ÂæàÂ•ΩÁöÑÂ∞ÜÊàêÁÜüÂØÜÈí•ÁÆ°ÁêÜÊúÄ‰Ω≥ÂÆûË∑µÂíåK8SÂéüÁîüÁîüÊÄÅÈõÜÊàêÂú®‰∏ÄËµ∑„ÄÇ ËØ¶ËßÅÂçöÊñá‰ΩøÁî®Â§ñÈÉ®Secrets ManagerÁÆ°ÁêÜKubernetesÂØÜÈí•„ÄÇ\nGitOps ÁöÑÁêÜÂøµÊòØÂ∞Ü‰∏ÄÂàáÈÖçÁΩÆ‰ª•Â£∞ÊòéÂºèÊñáÊú¨ÊñπÂºè‰øùÂ≠òÂú®‰ªìÂ∫ì‰∏≠„ÄÇËÄåÂØπ‰øùÂ≠ò Kubernetes Secrets ÊòØ‰∏™ÊåëÊàòÔºå Âõ†‰∏∫ Git ‰ªìÂ∫ìÂØπÊâÄÊúâËØªÊùÉÈôêÁöÑÁî®Êà∑ÂÖ¨ÂºÄÔºåÁîöËá≥È°πÁõÆÁöÑ‰ªìÂ∫ìÊòØÂºÄÊ∫ê„ÄÇFlux ÈÄöËøáÊîØÊåÅ Bitnami Sealed Secrets Âíå Mozilla SOPS ÂÆâÂÖ®ÁöÑÂú® Git ‰ªìÂ∫ì‰∏≠ÁÆ°ÁêÜÂØÜÈí•„ÄÇÊé•‰∏ãÊù•Â∞ÜÁ§∫‰æãÂ¶Ç‰Ωï‰ΩøÁî® Sealed Secrets ‰∏∫ MariaDB ÂàõÂª∫ÂØÜÁ†Å„ÄÇ\nÈ¶ñÂÖà‰ΩøÁî® HelmRelease ÈÉ®ÁΩ≤ Bitnami Sealed Secrets„ÄÇÁ±ª‰ºº‰∏äÈù¢ÈÉ®ÁΩ≤ External DNSÔºåÂ∞Ü sealed secrets Ê∑ªÂä†Âà∞ infrastructure/base Èáå‰Ωú‰∏∫ÂÖ±‰∫´ÁªÑ‰ª∂„ÄÇ 1apiVersion: helm.toolkit.fluxcd.io/v2beta1 2kind: HelmRelease 3metadata: 4 name: sealed-secrets 5 namespace: kube-system 6spec: 7 chart: 8 spec: 9 chart: sealed-secrets 10 sourceRef: 11 kind: HelmRepository 12 name: sealed-secrets 13 version: \u0026#34;\u0026gt;=1.15.0-0\u0026#34; 14 interval: 1h0m0s 15 releaseName: sealed-secrets-controller 16 targetNamespace: kube-system 17 install: 18 crds: Create 19 upgrade: 20 crds: CreateReplace ÊåâÁÖß Flux ÁöÑ Sealed Secrets ÊñáÊ°£ÔºåÂÆâË£Ö kubeseal„ÄÇ ‰ΩøÁî® kubeseal ‰ªéÈõÜÁæ§‰∏≠‰∏ãËΩΩÂÖ¨Èí•„ÄÇ 1kubeseal --fetch-cert \\ 2--controller-name=sealed-secrets-controller \\ 3--controller-namespace=kube-system \\ 4\u0026gt; pub-sealed-secrets-dev.pem ‰∏∫ Bitnami MariaDB ÁîüÊàêÂØÜÈí•„ÄÇ 1kubectl -n kube-system create secret generic prestashop-mariadb \\ 2--from-literal=mariadb-root-password=\u0026lt;put the ariadb root password here\u0026gt; \\ 3--from-literal=mariadb-replication-password=\u0026lt;put the replication password here\u0026gt; \\ 4--from-literal=mariadb-password=\u0026lt;put the mariadb password here\u0026gt; \\ 5--dry-run=client \\ 6-o yaml \u0026gt; /tmp/mariadb-secrets.yaml ‰ªé K8S ÂÜÖÁΩÆÁöÑ Opaque Secrets Ê†ºÂºèÊñá‰ª∂ÁîüÊàê sealed secret„ÄÇ 1kubeseal --format=yaml --cert=pub-sealed-secrets-dev.pem \\ 2\u0026lt; /tmp/mariadb-secrets.yaml \u0026gt; infrastructure/overlays/development/prestashop-mariadb-secrets.yaml ÈÉ®ÁΩ≤ Bitnami Helm Chart MariaDBÔºå‰ΩøÁî®ÊèêÂâçÂàõÂª∫ÁöÑÂØÜÈí•‰Ωú‰∏∫ DB ÁöÑÂØÜÈí•„ÄÇ 1apiVersion: helm.toolkit.fluxcd.io/v2beta1 2kind: HelmRelease 3metadata: 4 name: prestashop-mariadb 5spec: 6 releaseName: mariadb 7 chart: 8 spec: 9 chart: mariadb 10 sourceRef: 11 kind: HelmRepository 12 name: bitnami 13 namespace: kube-system 14 version: 10.4.0 15 interval: 1h0m0s 16 install: 17 remediation: 18 retries: 3 19 valuesFrom: 20 - kind: ConfigMap 21 name: prestashop-values 22--- 23auth: 24 existingSecret: prestashop-mariadb 25primary: 26 persistence: 27 enabled: false 28 storageClass: standard ÊúÄ‰Ω≥ÂÆûË∑µ ÂàáËÆ∞‰ΩøÁî® Sealed Secrets, SOPS Á≠âÂ∑•ÂÖ∑‰ªÖÂ∞ÜÂä†ÂØÜÂêéÁöÑÂØÜÈí•Êèê‰∫§Âà∞ Git ‰ªìÂ∫ìÔºåÈÅøÂÖçÂØÜÈí•ÁöÑÊ≥ÑÈú≤ÔºÅ\nÊü•ÈòÖDEV, STAGING, PRODUCTËøô‰∏â‰∏™Êèê‰∫§Êü•ÁúãÂÆåÊï¥ sealed secrets ‰ΩøÁî®„ÄÇ\n4. ÈÄöÁü•ÈõÜÊàê Âú®ËøêÁª¥ÈõÜÁæ§ÁöÑÊó∂ÂÄôÔºå‰∏çÂêåÁöÑÂõ¢ÈòüÊúâËÆ¢ÈòÖ‰∏çÂêåÁöÑ GitOps ÊµÅÊ∞¥Á∫øÈÄöÁü•ÁöÑÈúÄÊ±Ç„ÄÇ ‰æãÂ¶ÇÔºåoncall Âõ¢ÈòüÂ∞ÜÊî∂Âà∞ÊúâÂÖ≥ÈõÜÁæ§‰∏≠ÂçèË∞ÉÂ§±Ë¥•ÁöÑË≠¶Êä•Ôºå ËÄåÂºÄÂèëÂõ¢ÈòüÂèØËÉΩÂ∏åÊúõÂú®ÈÉ®ÁΩ≤Êñ∞ÁâàÊú¨ÁöÑÂ∫îÁî®Á®ãÂ∫è‰ª•ÂèäÈÉ®ÁΩ≤ÊòØÂê¶ÂÅ•Â∫∑Êó∂Êî∂Âà∞Ë≠¶Êä•„ÄÇ\nFlux ÂÜÖÁΩÆ‰∫ÜÂêå Slack, MS Teams, Discord Á≠âÁü•Âêç IM Â∑•ÂÖ∑ÁöÑÈõÜÊàêÔºå‰πüÊîØÊåÅÂ∞ÜÊ∂àÊÅØÂèëÈÄÅÂà∞ webhook Êé•Âè£Ôºå Áî±Áî®Êà∑Ëá™Ë°åÂÆûÁé∞Ê∂àÊÅØÈÄöÁü•„ÄÇ\n‰∏ãÈù¢‰ª• Slack ‰∏∫‰æãÔºåÁ§∫‰æãÂ¶Ç‰ΩïÈõÜÊàê GitOps ÊµÅÊ∞¥Á∫øÊ∂àÊÅØ„ÄÇ\nÂÆö‰πâ‰∏Ä‰∏™Âêç‰∏∫ slack ÁöÑ Flux Ëá™ÂÆö‰πâËµÑÊ∫ê Provider 1apiVersion: notification.toolkit.fluxcd.io/v1beta1 2kind: Provider 3metadata: 4 name: slack 5 namespace: kube-system 6spec: 7 type: slack 8 secretRef: 9 name: slack-url Âõ†‰∏∫ Slack WebHook Âπ∂Ê≤°ÊúâÈ¢ùÂ§ñÁöÑÈâ¥ÊùÉ‰øùÊä§ÔºåËøôÈáåÊàë‰ª¨‰ΩøÁî®‰∏ä‰∏ÄËäÇÁöÑÂØÜÈí•ÁÆ°ÁêÜÊú∫Âà∂Âä†ÂØÜ‰øùÂ≠òÂú® Git ‰ªìÂ∫ìÁöÑ slack webhook urlÔºå ÂêåÊó∂ Provider ÂºïÁî® Secrets ÂØπË±°‰∏≠‰øùÂ≠òÁöÑ url„ÄÇ 2. ÂàõÂª∫ Flux Alert ÂØπË±°ËÆ¢ÈòÖÂëΩÂêçÁ©∫Èó¥ÁöÑÂêÑÁ±ª Flux ÂØπË±°‰∫ã‰ª∂ÔºåÂπ∂‰∏îÂêåÁ¨¨‰∏ÄÊ≠•ÂÆö‰πâÁöÑ Provider ÂÖ≥ËÅî„ÄÇ 3. ÂΩìÂàõÂª∫Â§ö‰∏™ Alert Âíå‰∏çÂêåÁöÑ Provider ÂÖ≥ËÅîÔºåÂèØ‰ª•Â∞ÜÊ∂àÊÅØÂèëÈÄÅÂà∞‰∏çÂêåÁöÑ Slack channel ÁîöËá≥ÊòØ‰∏çÂêåÁöÑ IM„ÄÇ\n1apiVersion: notification.toolkit.fluxcd.io/v1beta1 2kind: Alert 3metadata: 4 name: flux-alert 5 namespace: kube-system 6spec: 7 providerRef: 8 name: slack 9 eventSeverity: info 10 eventSources: 11 - kind: GitRepository 12 name: \u0026#39;*\u0026#39; 13 - kind: Kustomization 14 name: \u0026#39;*\u0026#39; 15 - kind: HelmRelease 16 name: \u0026#39;*\u0026#39; 17--- 18apiVersion: notification.toolkit.fluxcd.io/v1beta1 19kind: Alert 20metadata: 21 name: kube-system-alert 22 namespace: kube-system 23spec: 24 providerRef: 25 name: slack 26 eventSeverity: info 27 eventSources: 28 - kind: Kustomization 29 name: \u0026#39;*\u0026#39; 30 namespace: \u0026#39;kube-system\u0026#39; 31 - kind: HelmRelease 32 name: \u0026#39;*\u0026#39; 33 namespace: \u0026#39;kube-system\u0026#39; Êü•ÈòÖDEV, STAGING, PRODUCTËøô‰∏â‰∏™Êèê‰∫§Êü•ÁúãÂÆåÊï¥ commits Â¶Ç‰ΩïÂú®‰∏çÂêåÁéØÂ¢ÉÈõÜÁæ§‰∏≠ÈÉ®ÁΩ≤‰∫Ü Slack ÈÄöÁü•ÈõÜÊàê„ÄÇ\nÂõæ1ÔºöSlack channel ËÆ¢ÈòÖ GitOps ÊµÅÊ∞¥Á∫øÊ∂àÊÅØÈÄöÁü• ÊúÄ‰Ω≥ÂÆûË∑µ ÈíàÂØπËÆ¢ÈòÖ‰∏çÂêåÂëΩÂêçÁ©∫Èó¥(ÈùûAlertÂØπË±°ÂÆö‰πâÁöÑÂëΩ‰ª§Á©∫Èó¥)ÁöÑ‰∫ã‰ª∂ÈÄöÁü•ÔºåÈúÄË¶ÅÊòæÁ§∫ÊåáÂÆöÂëΩÂêçÁ©∫Èó¥Â±ûÊÄß„ÄÇ\n5. GitOps ‰ª£Á†ÅÁöÑ CI GitOps Ê®°ÂºèÂ∏¶Êù•ÁöÑÂèà‰∏Ä‰∏™Â•ΩÂ§ÑÊòØÂèØ‰ª•‰ΩøÁî®‰ºÅ‰∏öÊàêÁÜü‰∏îÊÉØÁî®ÁöÑ‰ª£Á†ÅÁÆ°ÁêÜÂ∑•‰ΩúÊµÅÊù•Ëá™Âä®ÂåñÈ™åËØÅÂèòÊõ¥Âèä‰ª£Á†ÅÂÆ°Ê†∏ÂÆ°Êâπ„ÄÇ ÈíàÂØπ GitOps ‰ª£Á†ÅÂèØ‰ª•ÂºïÂÖ•Â¶Ç‰∏ã CI Ê≠•È™§Ôºå\nÁî±‰∫é Flux Â§ßÈáè‰ΩøÁî® Kustomize Êù•ÁîüÊàêÊúÄÁªàÁöÑÂ£∞ÊòéÂºèÈÖçÁΩÆÔºåÂèØ‰ª•ÂÆûÁé∞Âú®ÊØèÊ¨°Êèê‰∫§ Pull Request/Merge Request ÂêéÁöÑÈ™åËØÅÈò∂ÊÆµÂºïÂÖ• kustomize CLI È™åËØÅ GitOps ÈÖçÁΩÆÊòØÂê¶ÂèØ‰ª•Ê≠£Á°ÆÁöÑË¢´ÁîüÊàê„ÄÇÂêåÊó∂Ôºå‰ΩøÁî® Flux OpenAPI ÁªìÂêà kubeconform È™åËØÅ Kubernetes ÂÜÖÁΩÆËµÑÊ∫êÂíå Flux CRD Á±ªÂûãÊòØÂê¶ÈÖçÁΩÆÊ≠£Á°Æ„ÄÇ ÂÄüÂä© KIND(Kubernetes in Docker) ÂÆûÁé∞ÂÆåÊï¥ÁöÑÁ´ØÂà∞Á´ØÊµãËØï„ÄÇKIND ÂÆûÁé∞‰∫Ü Docker ÂÆπÂô®ÊâìÂåÖÁöÑ Kubernetes ÁéØÂ¢ÉÔºå ÂèØ‰ª•ÊØèÊ¨° PR È™åËØÅÈò∂ÊÆµÂêØÂä®Êñ∞ÁöÑ KIND ÁéØÂ¢É‰∏îÂÆâË£Ö Flux ÂêéÔºåÊâßË°å GitOps ‰ª£Á†ÅÁöÑ reconciliationÔºå È™åËØÅ GitOps ‰ª£Á†ÅÈÖçÁΩÆÁöÑËµÑÊ∫êÊòØÂê¶ÂèØ‰ª•Ë¢´ÂàõÂª∫‰∏îÁä∂ÊÄÅ‰∏∫READY„ÄÇ ÂÄüÂä© Git ÊúçÂä°ÁöÑ CI ËÉΩÂäõÔºåÂ¶Ç Github Actions, Gitlab CI/CD Á≠âÔºå ÂÆûÁé∞ GitOps ‰ª£Á†ÅÁöÑ‰∏äËø∞‰∏§ÁßçËá™Âä®ÂåñÊ£ÄÊü•Ôºå‰ª•ÂèäÂêå‰ª£Á†ÅÂÆ°Ê†∏ÂÆ°ÊâπÈõÜÊàê„ÄÇ Êü•ÈòÖÊ≠§ Github Actions workflow ÈÖçÁΩÆÂÆûÁé∞Âú® KIND ÁéØÂ¢É End-To-End È™åËØÅ GitOps ÈÖçÁΩÆÔºåÂíå Â£∞ÊòéÂºèÈÖçÁΩÆ Manifests È™åËØÅ„ÄÇ\nÊúÄ‰Ω≥ÂÆûË∑µ Âà©Áî® Github Actions Êàñ Gitlab CI/CD ÈùûÂ∏∏ÂÆπÊòìÁöÑÂ∞Ü GitOps ‰ª£Á†ÅÈõÜÊàêÂà∞ CI ÁéØÂ¢ÉÔºå ÈÄöËøá KIND/Kubeconform È™åËØÅ‰ª£Á†ÅÁöÑÊ≠£Á°ÆÊÄß„ÄÇ\n6. Â∞èÁªì Êú¨Êñá‰ªãÁªç‰ΩøÁî® GitOps Â∑•ÂÖ∑ FluxCD v2 ÂÆûÁé∞‰∫ÜÁÆ°ÁêÜÂ§öË¥¶Êà∑Â§ö VPC ÁéØÂ¢É‰∏ãÁöÑ Kubernetes ÈõÜÁæ§ÁöÑÂÖ±‰∫´ÁªÑ‰ª∂ÔºåÂÆûË∑µ‰∫Ü Secrets ‰ΩøÁî®ÁöÑÊúÄ‰Ω≥ÂÆûË∑µÔºå CD ÈÉ®ÁΩ≤‰∫ã‰ª∂Âêå IM(Slack) ÁöÑÈõÜÊàêÔºåÊúÄÁªàÁ§∫‰æã‰∫ÜÈÄöËøá GitOps ‰ª£Á†ÅÁöÑ CI ÊµÅÁ®ãÊù•ÊèêÈ´ò GitOps ‰ª£Á†ÅÁöÑË¥®ÈáèÔºåÂáèÂ∞ëÈÉ®ÁΩ≤‰∏≠Êñ≠‰∫ãÊïÖ„ÄÇ ÂèØÂú®Ê≠§‰ªìÂ∫ìËé∑ÂèñÂÆåÊï¥ÁöÑ GitOps ‰ª£Á†Å„ÄÇ\n‰∏ãÁØáÂ∞Ü‰ªãÁªçÂü∫‰∫é Flux ÂÆûÁé∞ GitOps Â∑•‰ΩúÊ®°Âûã‰∏ãÁöÑÂÖ±‰∫´ÊúçÂä°Âπ≥Âè∞„ÄÇ\n","link":"https://kane.mx/posts/gitops/flux-in-action-1/","section":"posts","tags":["GitOps","Kubernetes","Flux","Git","EKS","CD","Continuous Delivery"],"title":"Âü∫‰∫é Flux ÁöÑ GitOps ÂÆûÊàòÔºà‰∏äÔºâ"},{"body":"","link":"https://kane.mx/tags/argocd/","section":"tags","tags":null,"title":"ArgoCD"},{"body":"‰ªäÂ§© Kuberentes Â∑≤ÁªèÊàê‰∏∫ITÂü∫Á°ÄËÆæÊñΩÁöÑÈáçË¶ÅÁé©ÂÆ∂ÔºåÂÆπÂô®ÁºñÊéíÈ¢ÜÂüüÁöÑ‰∫ãÂÆûÊ†áÂáÜ„ÄÇÂÜô‰∫é3Âπ¥ÂâçÁöÑÊñáÁ´†‰∏çË¶ÅËá™Âª∫ Kuberentes ÁöÑËßÇÁÇπÂ∑≤ÁªèË¢´ÁªùÂ§ßÂ§öÊï∞ÁöÑ‰ºÅ‰∏öÊâÄËÆ§ÂèØÂíåÊé•Âèó„ÄÇ\nÁÑ∂ËÄåÂêå‰ºóÂ§ö‰ºÅ‰∏öÊé•Ëß¶‰∏≠ÂèëÁé∞Ôºå‰ºÅ‰∏öÊúâÂæàÈ´òÁöÑÊÑèÊÑøÈááÁî® Kuberentes ÁÆ°ÁêÜÂ∑•‰ΩúË¥üËΩΩÔºåÂπ∂‰∏îÂ∑≤ÊúâÂ§ßÈáèÁöÑ‰ºÅ‰∏öÂ∑≤ÁªèÂ∞Ü Kuberentes Áî®‰∫éÁîü‰∫ßÁéØÂ¢É„ÄÇ ‰ΩÜÂ¶Ç‰ΩïÂØπÂ§öÂ•ó‰∏çÂêåÈò∂ÊÆµÁöÑ Kuberentes ÈõÜÁæ§Êù•ÂÅöÊåÅÁª≠ÈÉ®ÁΩ≤ÔºåÂÅöÂà∞È´òÂÆâÂÖ®ÊÄß„ÄÅÊùÉÈôêÂàÜÁ¶ª„ÄÅÂèØÂÆ°ËÆ°„ÄÅ‰øùËØÅ‰∏öÂä°Âõ¢ÈòüÁöÑÊïèÊç∑Á≠âÈúÄÊ±ÇÊÑüÂà∞Âõ∞ÊÉë„ÄÇ ÁõÆÂâçÂÆ¢Êà∑ÂÆûÁé∞ÊñπÂºèÈùûÂ∏∏Â§öÊ†∑ÔºåÂπ∂Ê≤°ÊúâÂæàÂ•ΩÁöÑÈÅµÂæ™‰∏öÁïåÁöÑÊúÄ‰Ω≥ÂÆûË∑µ„ÄÇ\nGitOps ÊòØÁõÆÂâçÊúÄ‰Ω≥ÁöÑ‰∏ÄÁßçÊñπÊ≥ïÊù•ÂÆûÁé∞Âü∫‰∫é Kuberentes ÈõÜÁæ§ÁöÑÊåÅÁª≠ÈÉ®ÁΩ≤Ôºå‰∏îÂêåÊó∂Êª°Ë∂≥ÂÆâÂÖ®ÊÄß„ÄÅÊùÉÈôêÂàÜÁ¶ªÁ≠â‰ºÅ‰∏öÁ∫ßÈúÄÊ±Ç„ÄÇ\n‰ªÄ‰πàÊòØ GitOps GitOps ÊòØ‰∏ÄÁßç‰∏∫‰∫ëÂéüÁîüÂ∫îÁî®Á®ãÂ∫èÂÆûÊñΩÊåÅÁª≠ÈÉ®ÁΩ≤ÁöÑÊñπÊ≥ï„ÄÇ ÂÆÉÈÄöËøá‰ΩøÁî®ÂºÄÂèë‰∫∫ÂëòÂ∑≤ÁªèÁÜüÊÇâÁöÑÂ∑•ÂÖ∑ÔºåÂåÖÊã¨ Git ÂíåÊåÅÁª≠ÈÉ®ÁΩ≤Â∑•ÂÖ∑Ôºå ‰∏ìÊ≥®‰∫éÂú®Êìç‰ΩúÂü∫Á°ÄÊû∂ÊûÑÊó∂‰ª•ÂºÄÂèë‰∫∫Âëò‰∏∫‰∏≠ÂøÉÁöÑ‰ΩìÈ™å„ÄÇ\nGitOps ÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÂåÖÊã¨Ôºå\nÊã•Êúâ‰∏Ä‰∏™ Git Â≠òÂÇ®Â∫ìÔºåËØ•Â≠òÂÇ®Â∫ìÂßãÁªàÂåÖÂê´ÂØπÁîü‰∫ßÁéØÂ¢É‰∏≠ÂΩìÂâçÊâÄÈúÄÂü∫Á°ÄËÆæÊñΩÁöÑÂ£∞ÊòéÊÄßÊèèËø∞Ôºå‰ª•Âèä‰∏Ä‰∏™‰ΩøÁîü‰∫ßÁéØÂ¢É‰∏éÂ≠òÂÇ®Â∫ì‰∏≠ÊèèËø∞ÁöÑÁä∂ÊÄÅÁõ∏ÂåπÈÖçÁöÑËá™Âä®ÂåñËøáÁ®ã ÊúüÊúõÁöÑÁä∂ÊÄÅ‰ª•Âº∫Âà∂‰∏çÂèòÊÄßÂíåÁâàÊú¨ÊéßÂà∂ÁöÑÊñπÂºèÂ≠òÂÇ®ÔºåÂπ∂‰øùÁïôÂÆåÊï¥ÁöÑÁâàÊú¨ÂéÜÂè≤ Â¶ÇÊûúÊÇ®ÊÉ≥ÈÉ®ÁΩ≤‰∏Ä‰∏™Êñ∞ÁöÑÂ∫îÁî®Á®ãÂ∫èÊàñÊõ¥Êñ∞‰∏Ä‰∏™Áé∞ÊúâÁöÑÂ∫îÁî®Á®ãÂ∫èÔºåÊÇ®Âè™ÈúÄË¶ÅÊõ¥Êñ∞Â≠òÂÇ®Â∫ìÔºåËΩØ‰ª∂‰ª£ÁêÜËá™Âä®‰ªéÊ∫ê‰∏≠ÊèêÂèñÊâÄÈúÄÁöÑÁä∂ÊÄÅÂ£∞ÊòéÔºåËá™Âä®ÂåñËøáÁ®ã‰ºöÂ§ÑÁêÜÂÖ∂‰ªñÊâÄÊúâ‰∫ãÊÉÖ ËΩØ‰ª∂‰ª£ÁêÜÊåÅÁª≠ËßÇÂØüÂÆûÈôÖÁ≥ªÁªüÁä∂ÊÄÅÂπ∂Â∞ùËØïÂ∫îÁî®ÊâÄÈúÄÁä∂ÊÄÅ Âêå‰º†ÁªüÁöÑÊåÅÁª≠ÈÉ®ÁΩ≤Á≥ªÁªüÂØπÊØîÂ¶Ç‰∏ãÔºå\n‰º†Áªü CD Á≥ªÁªü GitOps Á≥ªÁªü Áî±Êé®ÈÄÅ‰∫ã‰ª∂Ëß¶ÂèëÔºåÂ¶Ç‰ª£Á†ÅÊèê‰∫§„ÄÅÂÆöÊó∂‰ªªÂä°„ÄÅÊâãÂä®Á≠â Á≥ªÁªü‰∏çÊñ≠ËΩÆËØ¢ÂèòÊõ¥ ‰ªÖÈÉ®ÁΩ≤ÂèòÊõ¥ÁöÑÈÉ®ÂàÜ ‰∏∫‰ªª‰ΩïÈÉ®ÁΩ≤Â£∞Êòé‰∫ÜÊï¥‰∏™Á≥ªÁªü Á≥ªÁªüÂèØËÉΩ‰ºöÂú®ÈÉ®ÁΩ≤‰πãÈó¥ÊºÇÁßª Á≥ªÁªüÂ∞ÜÁ∫†Ê≠£‰ªª‰ΩïÊºÇÁßª CD Á®ãÂ∫èÂøÖÈ°ªÊúâÊùÉËÆøÈóÆÈÉ®ÁΩ≤ÁéØÂ¢É ÈÉ®ÁΩ≤ÁÆ°ÈÅìÂú®Á≥ªÁªüËåÉÂõ¥ÂÜÖË¢´ÊéàÊùÉËøêË°å GitOps ‰∏ç‰ºöÂ§ÑÁêÜÂ¶Ç‰∏ãÂ£∞ÊòéÔºå\nÊåÅ‰πÖÂåñÁöÑÂ∫îÁî®Êï∞ÊçÆÔºå‰æãÂ¶ÇÊù•Ëá™Áî®Êà∑‰∏ä‰º† Âü∫‰∫é Schema ÁöÑÈÉ®ÁΩ≤Ôºå‰æãÂ¶ÇÊï∞ÊçÆÂ∫ì schema Âç≥‰ΩøÈááÁî®‰∫Ü GitOps ÈÉ®ÁΩ≤ÔºåÂØπ‰∫é‰ª•‰∏äÊï∞ÊçÆÁöÑÂ§á‰ªΩÂíåÊÅ¢Â§çÂêåÊ†∑ÂæàÈáçË¶Å„ÄÇ\nÂ£∞ÊòéÂºèÈÖçÁΩÆÊòØ Kuberentes ‰ªé Day 1 ÂºÄÂßãÊèê‰æõÊîØÊåÅÁöÑÔºå ÂèØ‰ª•ËØ¥ Kuberentes Â£∞ÊòéÂºèÈÖçÁΩÆÂæàÂ•ΩÁöÑÂåπÈÖç‰∫Ü GitOps ÂéüÁêÜ‰∏≠ÁöÑÂ£∞ÊòéÊÄßÊèèËø∞ÈúÄÊ±Ç„ÄÇ ÁªìÂêàËá™ÂÆö‰πâËµÑÊ∫êÂÆö‰πâÂ∞ÜÂ£∞ÊòéÂºèÈÖçÁΩÆÊâ©Â±ïÂà∞‰∫ÜËá™ÂÆö‰πâËµÑÊ∫êÔºåK8S ‰∏≠ÁöÑËá™ÂÆö‰πâËµÑÊ∫ê‰πüÂèØ‰ª• Êó†ÁºùÁöÑÈÄÇÈÖç GitOps ÈÉ®ÁΩ≤ÊñπÊ≥ï„ÄÇ\nKuberentes ‰∏ä GitOps ÊúÄ‰Ω≥ÂÆûË∑µ GitOps ÊñπÊ≥ï‰∏ãÔºåGit Êàê‰∏∫Á≥ªÁªüÊâÄÈúÄÁä∂ÊÄÅÁöÑÂîØ‰∏Ä‰∫ãÂÆûÊù•Ê∫êÔºåÊîØÊåÅÂèØÈáçÂ§çÁöÑËá™Âä®ÂåñÈÉ®ÁΩ≤„ÄÅÈõÜÁæ§ÁÆ°ÁêÜÂíåÁõëÊéß„ÄÇ Â§çÁî®‰ºÅ‰∏ö‰∏≠Â∑≤ÁªèÈùûÂ∏∏ÊàêÁÜüÁöÑ Git Â∑•‰ΩúÊµÅÁ®ãÊù•ÂÆåÊàêÁºñËØë„ÄÅÊµãËØï„ÄÅÊâ´ÊèèÁ≠âÊåÅÁª≠ÈõÜÊàêÊ≠•È™§Ôºå ÂΩìÁ≥ªÁªüÊúÄÁªàÁä∂ÊÄÅÁöÑÂ£∞Êòé‰ª£Á†ÅËøõÂÖ• Git ‰ªìÂ∫ì‰∏ªÁ∫øÂàÜÊîØÂêéÔºå‰æùÊâò GitOps Â∑•ÂÖ∑ÈìæÊù•ÂÆåÊàêÈ™åËØÅÈÉ®ÁΩ≤ÔºåÂà∞ËßÇÊµãÂëäË≠¶Ôºå Âà∞Êìç‰Ωú‰øÆÂ§çËææÂà∞Á≥ªÁªüÊúÄÁªàÁä∂ÊÄÅÁöÑÈó≠ÁéØ(ËßÅÂõæ1)„ÄÇ\nÂõæ1ÔºöGitOps -- ‰∏Ä‰∏™ÊåÅÁª≠ËøêÁª¥ÁöÑÊ®°Âûã ‰∏ãÈù¢ËÆ©Êàë‰ª¨Êù•ÁúãÁúãÁêÜÊÉ≥‰∏≠ÁöÑ Kuberentes ‰∏äÁöÑ GitOps ÊúÄ‰Ω≥ÂÆûË∑µ„ÄÇ\nÂü∫‰∫é GitOps ÁöÑÊåÅÁª≠ÈÉ®ÁΩ≤ ‰ª•‰∏ãÊòØÂü∫‰∫é GitOps ÁöÑ Kuberentes ÈÉ®ÁΩ≤ÊµÅÁ®ãÔºå\nÂºÄÂèëËÄÖÊèê‰∫§Â∫îÁî®‰ª£Á†ÅÂíåÈÖçÁΩÆÂà∞Ê∫ê‰ª£Á†Å‰ªìÂ∫ì„ÄÇ ÊåÅÁª≠ÈõÜÊàêÊúçÂä°Âô®ÁºñËØë„ÄÅÊµãËØï„ÄÅÊâ´ÊèèÂπ∂‰∏îÊé®ÈÄÅÊñ∞ÁâàÊú¨ÁöÑÈïúÂÉèÂà∞ÈïúÂÉè‰ªìÂ∫ìÔºåÂêåÊó∂Êõ¥Êñ∞ K8S manifests Âà∞ Git ‰ªìÂ∫ì„ÄÇ GitOps ‰ª£ÁêÜ(Â¶Ç ArgoCD/Flux) Ê£ÄÊµãÂà∞ Git ‰ªìÂ∫ìÂèòÊõ¥Âπ∂Ëá™Âä®ÂèòÂä®Âà∞ K8S ÈõÜÁæ§„ÄÇ Âõæ2ÔºöÂü∫‰∫é GitOps ÁöÑÊåÅÁª≠ÈÉ®ÁΩ≤Âûã Âü∫‰∫é GitOps ÁöÑÂ§öÈõÜÁæ§ÁÆ°ÁêÜ ÂΩì Pull Request Ë¢´ÂêàÂπ∂Âà∞Âπ≥Âè∞‰ªìÂ∫ìÔºåÈÖçÁΩÆÂú®Â§öÈõÜÁæ§‰∏äÁöÑ GitOps ‰ª£ÁêÜÂ∞ÜÁõëÊéßËøô‰∏™‰ªìÂ∫ìÔºå Âπ∂‰∏îÈÉ®ÁΩ≤Ëøô‰∫õÊõ¥Êñ∞Âà∞ kube-system ÊàñËÄÖÂπ≥Âè∞ÂëΩÂêçÁ©∫Èó¥„ÄÇ ÊØè‰∏™ÈõÜÁæ§Êúâ‰∏Ä‰∏™ÈõÜÁæ§‰ªìÂ∫ìÁî®‰∫éÂ≠òÂÇ®ÈÖçÁΩÆÔºå‰æãÂ¶ÇËÆøÈóÆÊéßÂà∂„ÄÅDNSÁ≠â„ÄÇÂêåÊó∂ÊØè‰∏™ÈõÜÁæ§ÈÄöËøáÂπ≥Âè∞‰ªìÂ∫ì Âêå‰∏äÊ∏∏Âπ≥Âè∞ÂêåÊ≠•„ÄÇ Âõæ3ÔºöÂü∫‰∫é GitOps ÁöÑÂ§öÈõÜÁæ§ÁÆ°ÁêÜ GitOps ‰∏ãÁöÑÂ∫îÁî®ÂºÄÂèë‰ΩìÈ™å ‰∏öÂä°Âõ¢ÈòüË¥üË¥£ÈïúÂÉèÁöÑÁºñËØë„ÄÅÊµãËØïÂíåÊâ´ÊèèÁ≠â„ÄÇÂ£∞ÊòéÈÖçÁΩÆË¢´Êèê‰∫§Âà∞‰∏öÂä°/Â∫îÁî®ÁâπÂÆöÁöÑÈÖçÁΩÆ‰ªìÂ∫ì„ÄÇ GitOps ‰ª£ÁêÜËøêË°åÂú®ÁâπÂÆöÁöÑÁßüÊà∑ÂëΩÂêçÁ©∫Èó¥ÔºåÂ∫îÁî®ÁöÑÁä∂ÊÄÅ‰ªéÂ∫îÁî®Âõ¢ÈòüÁöÑ‰ªìÂ∫ìÂêåÊ≠•Âà∞ÁâπÂÆöÁöÑÁßüÊà∑ÂëΩÂêçÁ©∫Èó¥„ÄÇ Âõæ4ÔºöGitOps ‰∏ãÁöÑÂ∫îÁî®ÂºÄÂèë ÊîØÊåÅÂ§öÁßüÊà∑ÁöÑÈõÜÁæ§ Âêå‰∏äÔºå‰∏çÂêåÁöÑ‰∏öÂä°/Â∫îÁî®Âõ¢ÈòüÊúâÂêÑËá™ÁöÑÈÖçÁΩÆ‰ªìÂ∫ì„ÄÇÂú®ÈõÜÁæ§‰∏≠Ôºå‰∏çÂêåÁöÑ‰∏öÂä°/Â∫îÁî®Áî±‰∏çÂêåÁöÑÂëΩÂêçÁ©∫Èó¥ÈöîÁ¶ªÔºå Ë¢´ GitOps ‰ª£ÁêÜÊåÅÁª≠ÈÉ®ÁΩ≤Âú®ÂêÑËá™‰∏çÂêåÁöÑÂëΩÂêçÁ©∫Èó¥‰∏≠„ÄÇ\nÂõæ5ÔºöGitOps Â§öÁßüÊà∑ÊîØÊåÅ ‰ΩøÁî® Helm ÁÆ°ÁêÜÂ∫îÁî®Á®ãÂ∫è Helm ÊòØ Kuberentes ÁîüÊÄÅ‰∏ãÁöÑÂåÖÁÆ°ÁêÜÂô®ÔºåÁ±ª‰ºº Debian ‰∏ãÁöÑ apt„ÄÇ ÂØπ Helm Êèê‰æõÊó†ÁºùÁöÑÊîØÊåÅÂ∞ÜÂèØ‰ª•Âà©Áî®Âà∞Áé∞ÊúâÊàêÁÜüÁöÑK8SÂ∫îÁî®ÊâìÂåÖÁîüÊÄÅÔºåÊó†ËÆ∫ÊòØÈÉ®ÁΩ≤‰∏âÊñπÁªÑ‰ª∂ËøòÊòØ‰ºÅ‰∏öÂ∑≤ÁªèÂ≠òÂú®ÁöÑÂ∫îÁî®„ÄÇ\nÂØπÂ§öÁéØÂ¢ÉÁöÑÈÉ®ÁΩ≤ÁÆ°ÁêÜ Âú®‰ºÅ‰∏öÂ∫îÁî®‰∏≠Â§öÂ•óÁéØÂ¢É„ÄÅÂ§ö‰∫ë„ÄÅÊ∑∑Âêà‰∫ëÁöÑÂú∫ÊôØÊòØÈùûÂ∏∏Â∏∏ËßÅÁöÑÔºåÂØπÂ∫îÁî®ÁöÑÈÉ®ÁΩ≤ÈúÄË¶ÅÈÄöËøáÂü∫Á°ÄÊ∏ÖÂçïÂä†‰∏äÂêÑ‰∏™ÁéØÂ¢É‰∏™ÊÄßÂåñÁöÑÊ∏ÖÂçïÊù• ÁÆÄÂåñÂØπÂ§öÂ•óÁéØÂ¢ÉÁöÑÈÉ®ÁΩ≤ÁÆ°ÁêÜ„ÄÇ\nKustomize Ê≠£ÊòØ‰∏∫Ëß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òËÄåÂàõÂª∫ÔºåÂπ∂‰∏îÂ∑≤ÁªèÊàê‰∏∫ K8S ÂéüÁîüÂ∑•ÂÖ∑Èìæ‰∏≠ÁöÑ‰∏ÄÈÉ®ÂàÜ„ÄÇÂØπ Kustomize Êèê‰æõÊîØÊåÅÔºå Â∞ÜÂæàÂ•ΩÁöÑÂú® GitOps ‰∏≠Êª°Ë∂≥Ê≠§Á±ªÈúÄÊ±Ç„ÄÇ\nÂõæ6ÔºöGitOps Â§öÁéØÂ¢ÉÊîØÊåÅ Áªº‰∏äÔºå‰ªéÊåÅÁª≠ÈÉ®ÁΩ≤ÔºåÂ§öÈõÜÁæ§ÁÆ°ÁêÜÔºåÂ§öÁßüÊà∑ÊîØÊåÅÂíåÁé∞ÊúâÂ∑•ÂÖ∑Èìæ„ÄÅÁîüÊÄÅÈõÜÊàêÊñπÈù¢ÔºåÊèèËø∞‰∫ÜÂØπ Kubernetes ‰∏ä GitOps ÁöÑÁêÜÊÉ≥Áä∂ÊÄÅ„ÄÇÊé•‰∏ãÊù•ËÆ©Êàë‰ª¨Êù•ËÆ®ËÆ∫‰∏ãÁé∞ÊúâÁöÑ GitOps Â∑•ÂÖ∑Ôºå ÊòØÂê¶ÂèØ‰ª•ÂæàÂ•ΩÁöÑÊîØÊåÅÂâçÈù¢ÊèèÁªòÁöÑ GitOps ÁöÑÁêÜÊÉ≥Áä∂ÊÄÅ„ÄÇ\n‰∫ëÂéüÁîüÁöÑ GitOps Â∑•ÂÖ∑ Áî±‰∫é Kubernetes ÊòØ CNCF Âü∫Èáë‰ºöÁöÑÊ†∏ÂøÉÈ°πÁõÆÔºåÊï¥‰∏™ÁîüÊÄÅ‰ºöÈ¶ñÂÖàÂÖ≥Ê≥® CNCF Âü∫Èáë‰ºö‰∏ãÁöÑÈ°πÁõÆÔºåÂØπ GitOps Êù•ËØ¥ÂêåÊ†∑Â¶ÇÊ≠§„ÄÇ\nCNCF Âú®2020Âπ¥ÂèëÂ∏É‰∫Ü Continuous Delivery ÊäÄÊúØÈõ∑ËææÔºåFlux Âíå Helm ‰∏§‰∏™È°πÁõÆË¢´ÂΩíÁ±ª‰∏∫ ÈááÁî®ÔºåKustomize ÊòØË¢´ÂΩíÁ±ª‰∏∫ËØïÁî®ÔºåArgo CD Âú®Èõ∑Ëææ‰∏≠‰∏∫ÊäÄÊúØËØÑ‰º∞„ÄÇ\nCNCF Âú®2021Âπ¥ÂèëÂ∏ÉÁöÑ Multicluster Management ÊäÄÊúØÈõ∑Ëææ ‰∏≠ÁöÑ Core Services and Add-ons ÁÆ°ÁêÜÈõ∑ËææÔºåFlux, Kustomize, Argo Âíå Helm Á≠âÈ°πÁõÆÈÉΩË¢´ËØÑ‰∏∫ÂèØÈááÁî®„ÄÇ\nÂü∫‰∫éÂâçÈù¢ÂØπ GitOps ÁöÑÊ†∏ÂøÉÂÆö‰πâÔºåCNCF ÁöÑÊäÄÊúØÈõ∑ËææË±°Èôê‰ª•ÂèäÁ§æÂå∫ÁöÑÂØπÊØîÔºåÁõÆÂâçÊï¥‰∏™Á§æÂå∫‰∏≠ÊúÄ‰∏∫ ÊôÆÂèäÁöÑ GitOps Â∑•ÂÖ∑ÊòØ Argo CD Âíå Flux„ÄÇ\nPush ËøòÊòØ Poll? GitOps Âú®ÂÆûË∑µ‰∏≠Èù¢‰∏¥ÊòØÈááÁî®Êé®(push)ËøòÊòØÊãâ(pull)ÁöÑÈÉ®ÁΩ≤È£éÊ†ºÈÄâÊã©„ÄÇ\nÈááÁî®Êé®ÈÉ®ÁΩ≤È£éÊ†º‰ºöÊúâÂ¶Ç‰∏ãÂ•ΩÂ§ÑÔºå\nÁÆÄÂçïÊòìÁêÜËß£„ÄÇËøôÁßçÈÉ®ÁΩ≤ÊñπÂºèÂ∑≤ÁªèË¢´‰ºóÂ§öÁü•ÂêçÁöÑ CI/CD Â∑•ÂÖ∑ÊâÄÈááÁî®Ôºå‰æãÂ¶Ç Jenkins CIÔºåAWS CodeÁ≥ªÂàó„ÄÇ ÁÅµÊ¥ª„ÄÇÊòì‰∫éÂêåÂÖ∂‰ªñÁöÑËÑöÊú¨ÊàñÂ∑•ÂÖ∑ÈõÜÊàê„ÄÇÊãâ(pull)È£éÊ†ºÁöÑ GitOps ‰ª£ÁêÜÂè™ËÉΩËøêË°åÂú® Kuberentes ‰∏≠„ÄÇ ÈááÁî®ÊãâÈÉ®ÁΩ≤È£éÊ†º‰ºöÊúâÂ¶Ç‰∏ãÂ•ΩÂ§ÑÔºå\nÊõ¥Âä†ÂÆâÂÖ®„ÄÇÂõ†‰∏∫ GitOps ‰ª£ÁêÜËøêË°åÂú® Kubernetes ÈõÜÁæ§‰∏≠ÔºåÂõ†Ê≠§‰ªÖÈúÄË¶ÅÊúÄÂ∞èÁöÑÊùÉÈôêÁî®‰∫éÈÉ®ÁΩ≤„ÄÇÁÆÄÂåñÁΩëÁªúÈÖçÁΩÆ‰∏çÈúÄË¶ÅËØ•ÈõÜÁæ§Âêå CD Á®ãÂ∫èÂª∫Á´ãÁΩëÁªúËøûÊé•ÔºåÂ∞§ÂÖ∂Âú®ÁÆ°ÁêÜÂ§öÈõÜÁæ§Êó∂Â∞§‰∏∫ÁÆÄÊ¥Å„ÄÇ ‰∏ÄËá¥ÊÄß„ÄÇÁÆ°ÁêÜÂ§öÈõÜÁæ§Êó∂ÔºåÁ°Æ‰øù‰∫ÜÊØè‰∏™ÈõÜÁæ§ÁöÑÁÆ°ÁêÜÊñπÂºèÈÉΩÊòØ‰∏ÄÊ†∑ÁöÑ„ÄÇ ÈöîÁ¶ªÊÄß„ÄÇÊØè‰∏™ÈõÜÁæ§ÁöÑÈÉ®ÁΩ≤‰∏ç‰æùËµñ‰∫éÈõÜ‰∏≠ÁöÑÊµÅÊ∞¥Á∫ø CD Á®ãÂ∫è„ÄÇ ÂèØ‰º∏Áº©ÊÄß„ÄÇËØ•ÊñπÂºèÂèØ‰ª•ÂÆπÊòìÁöÑÊâ©Â±ïÂà∞ÂêåÊó∂ÁÆ°ÁêÜÊàêÁôæ‰∏äÂçÉÁöÑÈõÜÁæ§„ÄÇ ‰ªé‰ª•‰∏äÂØπÊØîÂèØËßÅÔºåÈááÁî®Êãâ(pull)ÁöÑÈÉ®ÁΩ≤È£éÊ†º‰ªéÂÆâÂÖ®ÊÄß„ÄÅÂèØ‰º∏Áº©ÊÄß„ÄÅÈöîÁ¶ªÊÄß„ÄÅ‰∏ÄËá¥ÊÄßÈÉΩÊõ¥‰ºòÔºåGitOps ÈÉ®ÁΩ≤ÊñπÂºèÂ∫îËØ•È¶ñÈÄâÊãâÈÉ®ÁΩ≤È£éÊ†º„ÄÇ\n‰∏ªÊµÅ‰∫ëÂéüÁîü GitOps ÂØπÊØî ‰∏ãË°®ËØ¶ÁªÜÂØπÊØî‰∫Ü Argo CD / Flux v2Ôºå‰æõÂèÇËÄÉ„ÄÇ\nArgoCD Flux v2 ÂÆâË£Ö/ÈÖçÁΩÆ ‰∏Ä‰∏™ÂëΩ‰ª§ÂÆâË£ÖÔºå‰ΩÜÊ≤°ÊúâÂéüÁîüÁöÑÊú∫Âà∂ÂÆûÁé∞ÈÖçÁΩÆ„ÄÇÈúÄË¶ÅÈÄöËøá UI ÊàñÂàõÂª∫Â§ßÈáèÊ∏ÖÂçï ‰∏Ä‰∏™ÂëΩ‰ª§ÂÆåÊàêÂÆâË£ÖÂíåÈÖçÁΩÆ ÈÉ®ÁΩ≤È£éÊ†º Êé®(push) / Êãâ(pull) Êãâ(pull) ÁßòÈí•ÁÆ°ÁêÜ[1] Sealed secrets Sealed secrets / Mozilla SOPS Webhook Êé•Êî∂[2] ÊîØÊåÅ ÊîØÊåÅ ÂëäË≠¶ÂíåÈÄöÁü• ÂÜÖÁΩÆÈõÜÊàê slackÔºåemailÔºåGoogle Chat Á≠â„ÄÇ ÂÜÖÁΩÆÈõÜÊàê slackÔºådiscordÔºåMS Teams Á≠â„ÄÇ ÈïúÂÉèÊõ¥Êñ∞Ëá™Âä®Âåñ ÊîØÊåÅ ÊîØÊåÅ Reconciliation ÂèØÈÖçÁΩÆÊÄß ÊúâÈôêÊîØÊåÅÔºàÂè™ËÉΩÂÖ®Â±ÄËÆæÁΩÆ reconciliation Êó∂Èó¥Ôºå‰∏çËÉΩ‰∏∫ÊØè‰∏™Â∫îÁî®ËÆæÁΩÆ‰∏çÂêåÁöÑ reconciliation Êó∂Èó¥„ÄÇÔºâ ÊîØÊåÅËÆæÁΩÆ sync(ÂêåÊ≠•) Âíå reconciliation Èó¥Èöî Â∫îÁî®‰∫§‰ªò -- ÂéüÁîü Kuberentes Ê∏ÖÂçï(YAML) ÊòØÁöÑÔºåArgo CD Â∫îÁî®Á®ãÂ∫èÁöÑÂâØ‰ΩúÁî®ÊòØÈúÄË¶ÅÂú®ÈáçËØï‰∏≠ÈáçÊñ∞Â∫îÁî®ËµÑÊ∫ê„ÄÇ ÊîØÊåÅÔºåË¢´ËßÜÂêå‰∏∫ kustomization yaml Â∫îÁî®‰∫§‰ªò -- Kustomization ÊîØÊåÅ ÊîØÊåÅÔºåËøòÊèê‰æõ GitOps ÁªÑ‰ª∂Èó¥ÁöÑ‰æùËµñ Â∫îÁî®‰∫§‰ªò -- Helm charts ÊîØÊåÅÔºå‰ΩÜÊ≤°Êúâ‰ΩøÁî® Helm Go ËØ≠Ë®ÄÁ®ãÂ∫èÂ∫ì„ÄÇ Helm chart Èí©Â≠êË¢´ËΩ¨‰∏∫ Argo CD syncwaves/hooks„ÄÇÂõ†Ê≠§Ôºå‰∏çÊîØÊåÅ Helm cli„ÄÇ ÊîØÊåÅÔºåÂéüÁîüÊîØÊåÅ chart Èí©Â≠êÔºåÂèØ‰Ωú‰∏∫ÁªÑ‰ª∂Áõ∏‰∫í‰æùËµñ„ÄÇÊîØÊåÅ Helm cli„ÄÇ Web UI ÊîØÊåÅÔºåÊèê‰æõ‰∫ÜÂÆåÊï¥ÁöÑ UI Êìç‰Ωú„ÄÇ Ê≤°ÊúâÂÆòÊñπ UI ÂÆûÁé∞„ÄÇÂºÄÊ∫êÂπ≥Âè∞ Weave GitOps Âü∫‰∫é Flux Êèê‰æõ UI„ÄÇ Â§öÁßüÊà∑ÊùÉÈôêÁÆ°ÁêÜ ÊîØÊåÅÔºåÂÆûÁé∞‰∫ÜÁã¨Á´ã‰∫é Kubernetes Âü∫‰∫éËÆøÈóÆÊéßÂà∂ÂàóË°®ÁöÑ RBACÔºåÂÖ∑ÊúâÁªÜÁ≤íÂ∫¶ÊéßÂà∂„ÄÇ ÊòØÁöÑÔºåÂÆÉ‰∏•Ê†ºÂü∫‰∫é Kubernetes ÁöÑ RBAC ËÉΩÂäõÔºåÈúÄË¶ÅÁªìÂêàÂÖ∂‰ªñ CNCF È°πÁõÆÂÅöÁ≤íÂ∫¶ÊéßÂà∂ÔºåÊØîÂ¶Ç Kyverno„ÄÇ Â§öÈõÜÁæ§ÁÆ°ÁêÜ -- Â§öÈõÜÁæ§ÁÆ°ÁêÜÂíåÈÉ®ÁΩ≤ ÊîØÊåÅÔºåÂØπÈõÜÁæ§ÂÅö‰∫ÜÂéüÁîüÊäΩË±°„ÄÇ ÊîØÊåÅ„ÄÇÁêÜËÆ∫‰∏äÊîØÊåÅ‰ΩøÁî® KubeConfig ËÆæÁΩÆÈÄöËøá Kustomziation Âíå Helm Âú®‰∏Ä‰∏™ Flux ‰∏≠ÁÆ°ÁêÜÂ§ö‰∏™ÈõÜÁæ§‰∏äÁöÑÂ∑•‰ΩúË¥üËΩΩ„ÄÇ Â§öÈõÜÁæ§ÁÆ°ÁêÜ -- ÂàõÂª∫ÈõÜÁæ§ ‰∏çÊîØÊåÅÔºåÈúÄË¶ÅÈÄöËøáÁ¨¨‰∏âÊñπÂ∑•ÂÖ∑„ÄÇ‰æãÂ¶ÇÔºåCAPI, Crossplane, Open Cluster Management Á≠â„ÄÇ ‰∏çÊîØÊåÅÔºåÈúÄË¶ÅÈÄöËøáÁ¨¨‰∏âÊñπÂ∑•ÂÖ∑„ÄÇ‰æãÂ¶ÇÔºåCAPI, Crossplane, Open Cluster Management Á≠â„ÄÇ GitOps Â∑•ÂÖ∑Ëá™Ë∫´ÁöÑÂèØËßÇÊµãÊÄß ÈÄöËøá Prometheus + Grafana Êèê‰æõ„ÄÇ ÈÄöËøá Prometheus + Grafana Êèê‰æõ„ÄÇ ÈÄöËøá‰∏äË°®‰ªé GitOps Ê†∏ÂøÉÁêÜÂøµÊù•ÁúãÔºåÊó†ËÆ∫ ArgoCD ËøòÊòØ Flux ÈÉΩÊª°Ë∂≥ GitOps ÁöÑÁêÜÂøµ„ÄÇ‰∏îÊª°Ë∂≥‰∫Ü‰ºÅ‰∏öÁ∫ßÈúÄÊ±ÇÔºåÂ¶ÇÂ§öÁßüÊà∑ÊùÉÈôê„ÄÅÂ§öÈõÜÁæ§ÁÆ°ÁêÜ„ÄÅ ÁßòÈí•ÁÆ°ÁêÜ„ÄÅÂëäË≠¶ÈÄöÁü•„ÄÅÊîØÊåÅ Helm Âíå Kustomization„ÄÇ‰ªéËá™Ë∫´ÁöÑÂÆûÁé∞‰∏äËØ¥ÔºåArgoCD Êèê‰æõ‰∫ÜÂÆåÊï¥ÁöÑÊäΩË±°ÔºåÂåÖÊã¨‰∏î‰∏çÈôê‰∫é Cluster„ÄÅRBAC„ÄÅApplication„ÄÅHook Á≠â„ÄÇ ËøôÊ†∑ÁöÑÂÅöÊ≥ïÂÖ∑Â§á‰∫ÜÊõ¥Âä†ÂπøÊ≥õÁöÑÂäüËÉΩÈõÜÁöÑÂèØËÉΩÔºå‰ΩÜÂêåÊó∂Â¢ûÂä†‰∫ÜËá™Ë∫´Á®ãÂ∫èÁöÑÂ§çÊùÇÂ∫¶Ôºå‰πüÊèêÈ´ò‰∫ÜÁî®Êà∑ÁöÑÂ≠¶‰π†Èó®Êßõ„ÄÇFlux Ëá™Ë∫´Êû∂ÊûÑÊõ¥Âä†ÁÆÄÊ¥ÅÔºåÈªòËÆ§ÁªÑ‰ª∂‰ªÖÊúâ Source, Kustomize, Helm, Notification, Image automation Ëøô5‰∏™ÁªÑ‰ª∂ÔºåÂ∞ΩÈáèÂ§çÁî® Kuberentes ÂéüÁîüËÉΩÂäõÔºå‰æãÂ¶Ç‰ΩøÁî® ServiceAccount ÂÆûÁé∞Â§öÁßüÊà∑ÁöÑ RBAC ÊéßÂà∂Ôºå Èôç‰Ωé‰∫ÜÁî®Êà∑ÁöÑÂ≠¶‰π†Èó®ÊßõÔºåÂêå‰∫ëÂéüÁîüÁ§æÂå∫ÂÖ∂‰ªñÈ°πÁõÆÁöÑÂÖºÂÆπÊÄßÊõ¥Âº∫„ÄÇ\n[1]: Git‰ªìÂ∫ìÂèØËÉΩÂÖ¨ÂºÄËØª‰∏îÊòéÊñá‰øùÂ≠ò Secrets ÂØπË±°ÔºåÈúÄË¶ÅÂ∞ÜÂÖ∂Âä†ÂØÜÂêéÂÜçÊèê‰∫§Âà∞ Git\n[2]: ÈªòËÆ§ÈááÁî® Poll ËΩÆËØ¢ÊãâÂèñ Git ‰ªìÂ∫ìÂèòÊõ¥ÔºåÊèê‰æõ Webhook Êé•Âè£ÂèØË¢´ Git ‰ªìÂ∫ìÊèê‰∫§‰∫ã‰ª∂Ëß¶Âèë\nÂ∞èÁªì Êú¨Êñá‰ªãÁªç‰∫Ü‰ªÄ‰πàÊòØ GitOpsÔºåKuberentes ‰∏äÂü∫‰∫é GitOps ÂÆûÁé∞ÊåÅÁª≠ÈÉ®ÁΩ≤ÁöÑÊúÄ‰Ω≥ÂÆûË∑µÔºå‰ª•Âèä CNCF ‰∏ã GitOps ÊñπÂêëÊúÄ‰∏∫ÊµÅË°åÈ°πÁõÆ Argo CD Âíå Flux ÁöÑÂØπÊØî„ÄÇÂêéÁª≠Â∞Ü‰ª• Flux v2 ‰∏∫ÂÆûÊàòÔºåÊ∑±ÂÖ•‰ªãÁªçÂ¶Ç‰ΩïÂÆûÁé∞ GitOps ÊåÅÁª≠ÈÉ®ÁΩ≤‰∏îÂêåÊó∂Êª°Ë∂≥ÂêÑÁ±ª‰ºÅ‰∏öÁ∫ßÈúÄÊ±Ç„ÄÇ\nÂèÇËÄÉËµÑÊñô GitOps: Cloud-native Continuous Deployment The CNCF End User Technology Radar Continuous Delivery, June 2020 The CNCF End User Technology Radar Multicluster Management, June 2021 Push vs. Pull in GitOps: Is There Really a Difference? Why is a PULL vs a PUSH pipeline important? GitOps on Kubernetes: Deciding Between Argo CD and Flux ","link":"https://kane.mx/posts/gitops/the-best-practise-of-gitops-in-kubernetes/","section":"posts","tags":["GitOps","Kubernetes","Flux","ArgoCD","Git","CD","Continuous Delivery"],"title":"Kuberentes ‰∏ä GitOps ÊúÄ‰Ω≥ÂÆûË∑µ"},{"body":"","link":"https://kane.mx/tags/athena/","section":"tags","tags":null,"title":"Athena"},{"body":"","link":"https://kane.mx/tags/cost/","section":"tags","tags":null,"title":"Cost"},{"body":"As a builder in cloud, you might feel confused about which resources cost mostly in your account.\nIn AWS, you can quickly find out which services even functionality cost a lot via AWS Billing or AWS Cost Explorer. However sometimes it sucks on finding out which functions cost mostly if you have hundreds of Lambda functions, or which metrics/log groups cost mostly in Amazon CloudWatch.\nAWS Cost and Usage Reports should be helpful in above scenairos. The AWS Cost and Usage Reports (AWS CUR) contains the most comprehensive set of cost and usage data available, including product and product resource, and tags that you define yourself. You can use Cost and Usage Reports to publish your AWS billing reports to an Amazon S3 bucket that you own. The CUR reports are plain CSV text file, you still need analysis the report to find out the insight what you want. So Amazon Athena is one of simplest and effcientst ways to analyze your cost on demand. See the doc to how set up the Athea to analyze your AWS cost.\nAthena is out-of-the-box integrated with AWS Glue Data Catalog, allowing you to create a unified metadata repository across various services. With Amazon Athena, you pay only for the queries that you run. See my post how using Glue and Athena to analyze images in Docker repository.\nBelow are few samples to find out the mostly cost resources in your AWS account,\n1SELECT line_item_resource_id as resource, sum(line_item_unblended_cost) as total_cost FROM \u0026#34;athenacurcfn_main_account\u0026#34;.\u0026#34;main_account\u0026#34; 2WHERE year=\u0026#39;2022\u0026#39; and month=\u0026#39;1\u0026#39; and product_product_name = \u0026#39;AmazonCloudWatch\u0026#39; 3GROUP BY line_item_resource_id 4ORDER BY total_cost DESC 5LIMIT 10 Sample 1: find out the top 10 costly resources in CloudWatch, including Log Groups, Metrics, Synthetics and so on\n1SELECT line_item_resource_id, sum(line_item_usage_amount) as usage_amount, sum(line_item_blended_cost) as paid_amount FROM \u0026#34;athenacurcfn_main_account\u0026#34;.\u0026#34;main_account\u0026#34; 2 WHERE line_item_product_code=\u0026#39;AWSLambda\u0026#39; and product_group=\u0026#39;AWS-Lambda-Duration\u0026#39; 3 and year=\u0026#39;2022\u0026#39; and month=\u0026#39;1\u0026#39; 4 GROUP BY line_item_resource_id 5 ORDER BY usage_amount desc 6 LIMIT 10; Sample 2: find out the top 10 costly Lambda functions\nYou can refer to Data dictionary of CUR to understand the field definitions of report.\n","link":"https://kane.mx/posts/2022/find-out-most-costly-resource-in-your-aws-account/","section":"posts","tags":["AWS","Cost","Athena","Glue","Tip"],"title":"Find out the most costly resources in your AWS account"},{"body":"","link":"https://kane.mx/tags/glue/","section":"tags","tags":null,"title":"Glue"},{"body":"","link":"https://kane.mx/tags/aws-eks/","section":"tags","tags":null,"title":"AWS EKS"},{"body":"Though you're administrator of your AWS account, you probably see below warnings when viewing your cluster in EKS console.\nYour current user or role does not have access to Kubernetes objects on this EKS cluster.\nIt's caused by the Kuberentes has itself RBAC authorization. And AWS uses IAM to grant permissions to users. You have to map your IAM user or role to K8S RBAC authorization to grant the permissions to access K8S resources in EKS cluster.\nAbove documentation demonstrate how adding IAM roles/users to EKS cluster to grant the roles/users to access K8S resources. However the documentation is not clear to how adding federated users to EKS cluser.\nI'm facing two scenarios of federated AWS users to access K8S resources in EKS console,\nUse corp SSO to access internal system, then logging into AWS account via assuming existing role of the AWS account Use tool like AWS Vault/Alfred workflow to login AWS console via ak/sk of an IAM user Finally turn out below configuration to grant both federated users to access K8S resources in EKS console,\n1apiVersion: v1 2data: 3 mapRoles: | 4 - groups: 5 - system:bootstrappers 6 - system:nodes 7 rolearn: arn:aws:iam::123456789012:role/cluster-nodegroup-n-NodeInstanceRole-1OQT1WT84WVS8 # created by eksctl when bootrapping cluster 8 username: system:node:{{EC2PrivateDNSName}} 9 - groups: 10 - eks-console-dashboard-full-access-group 11 rolearn: arn:aws:iam::123456789012:role/Admin # granting the federated user via assuming role 12 username: Admin/kane 13 mapUsers: | 14 - userarn: arn:aws:sts::123456789012:federated-user/kane # granting the federated user via aws-vault 15 username: ops-user 16 groups: 17 - eks-console-dashboard-full-access-group","link":"https://kane.mx/posts/2022/grant-federated-users-accessing-k8s-resources-in-eks-console/","section":"posts","tags":["Kubernetes","AWS EKS","Tip","AWS"],"title":"Grant federated users accessing kubernetes resources in EKS console"},{"body":"","link":"https://kane.mx/tags/construct-hub/","section":"tags","tags":null,"title":"Construct Hub"},{"body":"","link":"https://kane.mx/tags/npm/","section":"tags","tags":null,"title":"Npm"},{"body":"","link":"https://kane.mx/tags/projen/","section":"tags","tags":null,"title":"Projen"},{"body":"Construct Hub is a web portal to collect the constructs for AWS CDK, CDK8s and CDKtf. The construct could support multiple programming languages, such as Javascript/TypeScript, Python, Java and C#. Actually the construct is developed by TypeScript, then it's compiled as across languages library by jsii! Any npm/pypi package with certain tags will be discovered by Construct Hub, the package will be automatically recognized as construct package and listed in Construct Hub.\nProjen is a project generator to create project with simplifying the project configuration to support dependencies management, building, unit testing, code style linting, CI/CD via Github actions PR and actions. So projen supports the construct project out of box, which configures construct project with jsii configuration that build the construct to across languages library, though publish the packages to kinds of package registries, such as npmjs, pypi and maven central.\nProjen provides a Publishing capability to publish construct library to supported package managers. For example, npm for JavaScript/TypeScript, it could publish the package to several npm registries, for example, npm public registry, Github packages, AWS CodeArtifact and any public accessible private npm registry.\nHowever the projen only supports publishing the package to single npm registry by default, how about you would like to publish your package to both npm public registry and Github packages?\nThere is no mature way to archive it, but projen is a flexible tool, we can hack it like below to add multiple npm registries support to publish the package to both npm public registry and Github packages,\n1const target = \u0026#39;js\u0026#39;; 2const REPO_TEMP_DIRECTORY = \u0026#39;.repo\u0026#39;; 3const options = { 4 registry: \u0026#39;npm.pkg.github.com\u0026#39;, 5 prePublishSteps: [ 6 { 7 name: \u0026#39;Prepare Repository\u0026#39;, 8 run: `mv ${project.artifactsDirectory} ${REPO_TEMP_DIRECTORY}`, 9 }, 10 { 11 name: \u0026#39;Install Dependencies\u0026#39;, 12 run: `cd ${REPO_TEMP_DIRECTORY} \u0026amp;\u0026amp; ${project.package.installCommand}`, 13 }, 14 { 15 // remove this if your package name already has scope 16 name: \u0026#39;Update package name\u0026#39;, 17 run: `cd ${REPO_TEMP_DIRECTORY} \u0026amp;\u0026amp; sed -i \u0026#34;1,5s/\\\\\u0026#34;packagename\\\\\u0026#34;/\\\\\u0026#34;@scope\\\\/packagename\\\\\u0026#34;/g\u0026#34; package.json`, 18 }, 19 { 20 name: `Create ${target} artifact`, 21 run: `cd ${REPO_TEMP_DIRECTORY} \u0026amp;\u0026amp; npx projen package:js`, 22 }, 23 { 24 name: `Collect ${target} Artifact`, 25 run: `mv ${REPO_TEMP_DIRECTORY}/${project.artifactsDirectory} ${project.artifactsDirectory}`, 26 }, 27 ], 28}; 29project.release.publisher.addPublishJob((_branch, branchOptions) =\u0026gt; { 30 return { 31 name: \u0026#39;npm_github\u0026#39;, 32 publishTools: {}, 33 prePublishSteps: options.prePublishSteps ?? [], 34 run: project.release.publisher.publibCommand(\u0026#39;publib-npm\u0026#39;), 35 registryName: \u0026#39;npm-github\u0026#39;, 36 env: { 37 NPM_DIST_TAG: branchOptions.npmDistTag ?? options.distTag ?? \u0026#39;latest\u0026#39;, 38 NPM_REGISTRY: options.registry, 39 }, 40 permissions: { 41 contents: github.workflows.JobPermission.READ, 42 packages: github.workflows.JobPermission.WRITE, 43 }, 44 workflowEnv: { 45 NPM_TOKEN: \u0026#39;${{ secrets.YOUR_GITHUB_REGISTRY_TOKEN }}\u0026#39;, 46 // if we are publishing to AWS CodeArtifact, pass AWS access keys that will be used to generate NPM_TOKEN using AWS CLI. 47 AWS_ACCESS_KEY_ID: undefined, 48 AWS_SECRET_ACCESS_KEY: undefined, 49 AWS_ROLE_TO_ASSUME: undefined, 50 }, 51 }; 52}); Above code snippet adds an additonal step in release workflow of Github action that is managed by projen, which publishes the package to Github packages.\nHAPPY Projen!\n","link":"https://kane.mx/posts/2022/publishing-npm-packages-to-multiple-registries-with-projen/","section":"posts","tags":["CDK Construct","AWS CDK","npm","projen","continuous delivery","construct hub"],"title":"Publishing npm packages to multiple registries with Projen"},{"body":"ËøëÊúüÂú®‰∏Ä‰∏™ Webinar ÂàÜ‰∫´‰∫ÜÂ¶Ç‰ΩïÂú® AWS ‰∏äÊúçÂä°Âéª‰∏≠ÂøÉÂåñÁ†îÂèëÂõ¢ÈòüÊûÑÂª∫ÂÖ±‰∫´ÊúçÂä°Âπ≥Âè∞ÔºåÊ†∏ÂøÉËßÇÁÇπÊÄªÁªìÂ¶Ç‰∏ãÔºå\nËøôÈáåÁöÑÂéª‰∏≠ÂøÉÂåñÂõ¢ÈòüÊòØÂêåÁêÜÊÉ≥ÁöÑÂÆåÂÖ®ÂåñÁöÑ DevOps Âõ¢Èòü(Ë¥üË¥£ËÆæËÆ°„ÄÅÂºÄÂèë„ÄÅÊµãËØï„ÄÅËøêÁª¥‰ª•ÂèäËøêËê•Á≠âÊâÄÊúâÁéØËäÇ)Áõ∏ÂØπÁ´ãÁöÑ„ÄÇ Âú®ËæÉÂ§ßÂûãÁöÑÁªÑÁªá‰∏≠ÔºåË¥¶Êà∑ÁÆ°ÁêÜ„ÄÅÁΩëÁªúËßÑÂàí„ÄÅÊúçÂä°ÂÆ°ËÆ°Á≠âÊ®°Âùó‰ºöÁî±Âπ≥Âè∞ÔºåÂü∫Á°ÄËÆæÊñΩÊàñÂÆâÂÖ®Âõ¢ÈòüÊâÄË¥üË¥£Ôºå Â§ö‰∏™Á†îÂèëÂõ¢Èòü‰ºöË¥üË¥£ÂêÑ‰∏™‰∏öÂä°Á≥ªÁªüÁöÑÂºÄÂèë„ÄÅÊµãËØï„ÄÅËøêÁª¥Á≠â„ÄÇ\nÂ¶Ç‰ªäÁªÑÁªáÁöÑÂÅ•Â∫∑ËøêËê•ÂØπÂÆâÂÖ®ÊÄßÂêàËßÑÊÄßË¶ÅÊ±ÇË∂äÊù•Ë∂äÈ´òÔºåÈÄöÂ∏∏Âü∫Á°ÄËÆæÊñΩÂõ¢ÈòüÂ§ñÂä†ÂÆâÂÖ®Âõ¢ÈòüË¥üË¥£ÊâøÊãÖÂÆâÂÖ®„ÄÅÂêàËßÑÈúÄÊ±ÇÁöÑÊï¥‰ΩìÁ≠ñÁï•ËßÑÂàíÂèäÂÆûÊñΩ„ÄÇ ‰ΩÜÊòØÊª°Ë∂≥ÂÆâÂÖ®„ÄÅÂêàËßÑÈúÄÊ±ÇÈÄöÂ∏∏ÊòØÂêå‰∏öÂä°‰∫§‰ªòÈÄüÂ∫¶ÊúüÊúõÊòØÁõ∏ÊÇñÁöÑ„ÄÇ‰∏ÄÊñπÈù¢ÔºåÂπ≥Âè∞„ÄÅÂÆâÂÖ®Âõ¢ÈòüË¶Å‰∏∫Â∫îÁî®‰∏äÁ∫øÊàñÂèòÊõ¥ËøõË°åÂÆâÂÖ®ÊÄß‰∏éÂêàËßÑÊÄßÂÆ°Êü•Ôºå ËÄåÁ†îÂèëÂõ¢ÈòüÈúÄË¶ÅÊäïÂÖ•Êõ¥Â§öÁöÑËµÑÊ∫êÂéªÊª°Ë∂≥ÂÆâÂÖ®„ÄÅÂêàËßÑÁöÑÈúÄÊ±ÇÔºåËøôÂøÖÁÑ∂‰ºöÊé®Ëøü‰∫§‰ªò„ÄÇÂè¶‰∏ÄÊñπÈù¢ÔºåÁ†îÂèëÂõ¢ÈòüÁöÑ‰∫§‰ªòÂèòÊõ¥ËøòÈúÄË¶ÅÂÜÖÈÉ®ÊµÅÁ®ã‰ª•Âèä ‰∫∫Â∑•Êìç‰ΩúÁöÑËØùÔºåË∑®Âõ¢ÈòüÁöÑÊ≤üÈÄö„ÄÅÂçè‰ΩúÂøÖÁÑ∂‰πü‰ºöÂª∂Áºì‰∫§‰ªòÈÄüÂ∫¶„ÄÇÊâÄ‰ª•Âü∫‰∫éÂÆâÂÖ®„ÄÅÂêàËßÑÈúÄÊ±ÇÁöÑÊäΩË±°ÔºåÂ§ñÂä†Ëá™Âä©ÊúçÂä°ÁöÑÂÖ±‰∫´ÊúçÂä°Âπ≥Âè∞Ôºå ÈíàÂØπËøôÁßçÂú∫ÊôØËÄåÁîüÔºåÂèØ‰ª•Â§ßÂπÖÊîπÂñÑ‰∫§‰ªòÈÄüÂ∫¶ÂêåÊó∂Êª°Ë∂≥ÂÆâÂÖ®„ÄÅÂêàËßÑË¶ÅÊ±Ç„ÄÇ\nAWS ‰∏äÊäΩË±°ÁöÑÂä©Âäõ Âú®‰∫öÈ©¨ÈÄäËΩØ‰ª∂ÂºÄÂèë‰∏≠ÁöÑÊäΩË±°ÂàÜ‰∏∫‰ª•‰∏ãÂá†Á±ªÔºå\nÊ°ÜÊû∂ Ê°ÜÊû∂‰∏∫Â∫îÁî®Á®ãÂ∫èÁºñÂÜô‰ª£Á†ÅÁöÑÊó∂ÂÄôÔºå‰∏∫‰∫ÜÁºñÂÜôÊõ¥Â∞èÊõ¥È´òÊïàÁöÑ‰ª£Á†ÅÔºå‰ª£Á†ÅË¢´Êâ©Â±ïÊàêÊàñË¢´Âª∫ÊàêÊõ¥ÂÆûË¥®ÊÄßÁöÑËΩØ‰ª∂ÈÉ®ÂàÜ„ÄÇ ÂºÄÂèë‰∫∫ÂëòÂØπÊ°ÜÊû∂Â∫îËØ•ÈùûÂ∏∏ÁÜüÊÇâ‰∫ÜÔºåÊó†ËÆ∫ÊòØWebÂºÄÂèëÁöÑSpring„ÄÅDjango„ÄÅVueJS„ÄÅReactËøòÊòØMLÊ®°ÂûãËÆ≠ÁªÉÁöÑTesnorFlowÂíåPyTorchÈÉΩÊòØ Ê°ÜÊû∂ÔºåÂ∏ÆÂä©ÂºÄÂèëËÄÖÈôç‰ΩéÂºÄÂèëÈó®ÊßõÔºå‰∏ìÊ≥®Âú®Ê†∏ÂøÉ‰∏öÂä°‰∏ä„ÄÇ‰ªéÁªÑÁªáÁöÑÂÆâÂÖ®„ÄÅÂêàËßÑËßíÂ∫¶Âá∫ÂèëÔºåÂü∫Á°ÄËÆæÊñΩÊàñÂÆâÂÖ®Âõ¢ÈòüÂèØ‰ª•ÈÄöËøáÊ®°ÊùøÊàñËÄÖÊ®°ÂºèÊù• Âº∫Âà∂ÂÆûÊñΩ‰ºÅ‰∏öËßÑËåÉÊù•ËææÂà∞ÂÆâÂÖ®ÊÄßÂíåÂêàËßÑÊÄßÈúÄÊ±Ç„ÄÇ\nAWS ÁõÆÂâçÊèê‰æõÁöÑÊ°ÜÊû∂Á±ªÂ∑•ÂÖ∑ÊàñÊúçÂä°Êúâ AWS SAM, AWS CDK‰ª•ÂèäAWS CloudFormation„ÄÇ Âü∫‰∫éËøô‰∫õÊ°ÜÊû∂Á±ªÂ∑•ÂÖ∑ÂíåÊúçÂä°ÔºåÁî®Êà∑ÂèØ‰ª•Âø´ÈÄüÊûÑÂª∫‰∫ë‰∏äÂ∫îÁî®ÔºåÊàñËÄÖÊòØÂàõÂª∫‰∫ë‰∏äÂ∫îÁî®ÁöÑÁªÑ‰ª∂‰∏îÂêåÊó∂Êª°Ë∂≥‰ºÅ‰∏öÂÆâÂÖ®ÂêàËßÑÈúÄÊ±Ç„ÄÇ\nÂëΩ‰ª§Ë°å CLI Ê°ÜÊû∂ÁöÑÂëΩ‰ª§Ë°å CLI ËÆ©ÂºÄÂèëÂõ¢ÈòüÁî®‰ªñ‰ª¨ÁÜüÁªÉÁöÑËØ≠Ë®ÄÊ†πÊçÆÂÆÉÊûÑÂª∫Ëá™Âä®Âåñ„ÄÇCLI ËÉΩÂ§üÁÆÄÂåñÊ°ÜÊû∂ÁöÑ‰ΩøÁî®ÈöæÂ∫¶ÔºåÂºÄÂèëËÄÖÂèØ‰ª•‰ΩøÁî®ÊúÄÁÜüÊÇâÁöÑÂºÄÂèëËØ≠Ë®Ä (‰æãÂ¶Ç Shell ËÑöÊú¨/Python)Êù•Ë∞ÉÁî® CLIÔºåÂÆûÁé∞‰∏öÂä°ÈÄªËæëÁöÑÂ∞ÅË£Ö„ÄÇ\nAWS CopilotÔºåAWS SAM CLIÊòØ AWS Êèê‰æõÁöÑ CLI Â∑•ÂÖ∑„ÄÇ\nÈÉ®ÁΩ≤ÊúçÂä° ÊúÄÂêéÈÉ®ÁΩ≤ÊúçÂä°ÊãøÂ∫îÁî®ËΩØ‰ª∂Êù•ËØ¥ÔºåÁî±ÂºÄÂèëÂõ¢ÈòüÁºñÂÜôÂπ∂ÂÆö‰πâ‰∫ÜÂ¶Ç‰Ωï‰ΩøÁî®ÈÇ£‰∏™ËΩØ‰ª∂ÔºåÂπ∂ËÆ©ÂÆÉÂú®ÁúüÂÆûÁöÑÂü∫Á°ÄËÆæÊñΩÁéØÂ¢ÉËøêË°å„ÄÇÂºÄÂèëÂõ¢ÈòüÈúÄË¶ÅÂä†Âø´ Â∫îÁî®‰∫§‰ªòÈÄüÂ∫¶Êù•Êª°Ë∂≥‰∏öÂä°ÈúÄÊ±ÇÔºåËÄåÂü∫Á°ÄËÆæÊñΩÂíåÂÆâÂÖ®Âõ¢ÈòüÈúÄË¶Å‰Ωú‰∏∫‰ºÅ‰∏öÂÆâÂÖ®ÂêàËßÑÊñπÈù¢ÁöÑ GateKeeper„ÄÇ‰∏ÄÂ•óÂêàÁêÜÁöÑÂçè‰ΩúÊ®°ÂºèÂ∞Ü‰ºöÂä†ÈÄü ÂºÄÂèëÂõ¢ÈòüÁöÑ‰∫§‰ªò‰∏îÊª°Ë∂≥ÂÆâÂÖ®ÈúÄÊ±ÇÔºå‰æãÂ¶ÇÂÖ±‰∫´Ëá™ÊúçÂä°Âπ≥Âè∞„ÄÇ\nAWS Êèê‰æõ‰∫ÜÂæàÂ§öÂºÄÁÆ±Âç≥Áî®ÁöÑ‰∫ßÂìÅÂºÄÂêØÂª∫Á´ã‰∏Ä‰∏™ÂÖ±‰∫´ÊúçÂä°Âπ≥Âè∞Ôºå‰æãÂ¶ÇÔºåAWS ProtonÔºåAWS CodeDeploy„ÄÇ\nÂπ≥Âè∞ÊâÄÊúâÊùÉÊ®°ÂºèÁöÑÊÄùËÄÉ ‰Ωú‰∏∫‰∏Ä‰∏™ÂÖ±‰∫´ÊúçÂä°Âπ≥Âè∞Ôºå‰∏çÂêåÁöÑÁÆ°ÁêÜÈúÄÊ±Ç/Ê®°ÂºèÊàñÂ∫îÁî®Ë¥üËΩΩÁ±ªÂûãÔºå‰ºöÊúâ‰∏çÂêåÁöÑÊúçÂä°ÊâÆÊºî‰∏çÂêåÁöÑËßíËâ≤„ÄÇ‰∏ãÈù¢ÊòØ‰∏Ä‰∫õÂÆ¢Êà∑ÂÆûË∑µÁöÑÂπ≥Âè∞Ê®°Âºè„ÄÇ\nAWS Ë¥¶Êà∑‰Ωú‰∏∫\u0026quot;Âπ≥Âè∞\u0026quot; ÁªÑÁªáÂ∞Ü AWS Ë¥¶Êà∑‰Ωú‰∏∫Âπ≥Âè∞ÔºåËá™Âä®ÂåñÂàõÂª∫ÔºåÂÖ∂ÁâπÁÇπÂ¶Ç‰∏ãÔºå\nAWS Ë¥¶Êà∑Â±û‰∫éÂ∫îÁî®Âõ¢Èòü‰∏îÁî±‰ªñ‰ª¨ËøêËê• Âü∫Á°ÄËÆæÊñΩÂõ¢Èòü‰∏∫Â∫îÁî®Ë¥¶Êà∑Êèê‰æõÂ∑•ÂÖ∑ÔºåÂåÖÊã¨‰∏î‰∏çÈôê‰∫éÔºå ÊîØÊíëÂûãÁöÑÂü∫Á°ÄËÆæÊñΩ(‰æãÂ¶ÇÔºåÁΩëÁªúÔºåÂÆâÂÖ®ÔºåÂüüÂêçÔºåÂÖ¨Âè∏Ê†áÂáÜÁ≠â) AWS Proton ÁéØÂ¢ÉÂíåÂ∫îÁî® AWS Service Catalog ‰∫ßÂìÅ Ê≠§Â§ñ [AWS Control Tower][control-towner], AWS Organizations, CloudFormation StackSets, AWS Config ‰∏ÄËá¥ÊÄßÂäüËÉΩÔºå ÈÉΩÊòØ‰∏Ä‰∫õÊúçÂä°ÊàñÂäüËÉΩÂèØ‰ª•ÊîØÊåÅ‰ª•Ë¥¶Êà∑‰∏∫ÁÆ°ÁêÜÂçï‰ΩçÁöÑÂÖ±‰∫´Âπ≥Âè∞„ÄÇ\nÊâòÁÆ°ÂÆπÂô®ÈõÜÁæ§‰Ωú‰∏∫\u0026quot;Âπ≥Âè∞\u0026quot; AWS Ë¥¶Êà∑Áî±Âπ≥Âè∞/Âü∫Á°ÄËÆæÊñΩÂõ¢ÈòüËøêËê•Ôºå‰ªñ‰ª¨Â∞ÜÁÆ°ÁêÜÂÆπÂô®ÈõÜÁæ§ÁöÑÁîüÂëΩÂë®ÊúüÔºå‰æãÂ¶ÇÊâòÁÆ°ÁöÑ EKS(Kubernentes) ÈõÜÁæ§„ÄÇÂ∫îÁî®Âõ¢ÈòüË¥üË¥£Â∞Ü Â∫îÁî®ÈÉ®ÁΩ≤Âà∞Âü∫Á°ÄËÆæÊñΩÂõ¢ÈòüÁÆ°ÁêÜÁöÑÂ§öÁßüÊà∑ÈõÜÁæ§ÔºåÂ∫îÁî®Âõ¢ÈòüÂ∞ÜÊâøÊãÖÂ¶Ç‰∏ãËÅåË¥£(ÂåÖÊã¨‰∏î‰∏çÈôê‰∫é)Ôºå\nÂ∫îÁî®Á®ãÂ∫èÊåÅÁª≠ÈõÜÊàê ÂÖ•Âè£ÊµÅÈáèÊéßÂà∂ ËÆøÈóÆÁÆ°ÁêÜ ËøêËê•ÂèØËßÅÊÄß ÊúçÂä°‰∫éÂ∫îÁî®Á®ãÂ∫èÁöÑÂë®ËæπÂü∫Á°ÄËÆæÊñΩ(‰æãÂ¶ÇÔºåÊï∞ÊçÆÂ∫ì„ÄÅÈòüÂàóÊàñÁºìÂ≠òÁ≠â)ÁöÑÈÉ®ÁΩ≤ÂèØ‰ª•ÈÄöËøáÊåÅÁª≠ÈõÜÊàêÁöÑ AWS CloudFormation/AWS CDK Ê®°ÊùøÔºåAWS Proton ÁéØÂ¢É/Â∫îÁî®Êù•ÂÆûÁé∞„ÄÇ\nÂèØÈÉ®ÁΩ≤ÁöÑÂ∫îÁî®Á®ãÂ∫èÊ®°Âºè‰Ωú‰∏∫\u0026quot;Âπ≥Âè∞\u0026quot; ËøôÁßçÊ®°Âºè‰∏ãÈÄöÂ∏∏‰ºöÁî±Âü∫Á°ÄËÆæÊñΩÂõ¢ÈòüÊã•ÊúâÂÖ±‰∫´Ë¥¶Êà∑ÔºåË¥üË¥£ÁΩëÁªú„ÄÅÂüüÂêç„ÄÅÂÆ°ËÆ°Á≠âËµÑÊ∫ê„ÄÇÂ∫îÁî®Âõ¢ÈòüË¥üË¥£Â∫îÁî®Ë¥¶Êà∑„ÄÇÂü∫Á°ÄËÆæÊñΩÂõ¢ÈòüÂàõÂª∫Áé∞ÊàêÁöÑ Â∫îÁî®Á®ãÂ∫è„ÄÅÈÉ®ÁΩ≤Êú∫Âà∂„ÄÅÊäΩË±°Â∫ì‰æõÂ∫îÁî®Á®ãÂ∫èÂõ¢Èòü‰ΩøÁî®ÊàñËá™ÂÆö‰πâÔºåÂ¶Ç‰∏ã‰∏Ä‰∫õÊúçÂä°ÊàñÂäüËÉΩÂèØ‰ª•ÂÆûÁé∞ËØ•Ê®°ÂºèÔºå\nAWS Proton AWS CloudFormation / AWS Service Catalog AWS CDK constructs ‰∫ßÂìÅÂåñËøêËê•ÂÖ±‰∫´Ëá™ÊúçÂä°Âπ≥Âè∞ ÂÖ±‰∫´Ëá™ÊúçÂä°Âπ≥Âè∞Èô§‰∫ÜÊ†πÊçÆÁªÑÁªáÁÆ°ÁêÜÂíåÊäÄÊúØÊ†àÈÄâÊã©ÂêàÈÄÇÁöÑÂÆûÁé∞Ê®°ÂºèÂ§ñÔºåÊòØÂê¶ËÉΩÂ§üÂú®‰∏Ä‰∏™ÁªÑÁªáÂÜÖÁúüÊ≠£ÁöÑ‰ºòÂåñÊïàÁéáÔºåÊèêÂçáÁ†îÂèë‰∫§‰ªòÈÄüÂ∫¶ÂèñÂÜ≥‰∫é Â¶Ç‰ΩïËøêËê•Ëøô‰∏™Âπ≥Âè∞„ÄÇËøôÈáåÊàë‰ª¨ÁúãÂà∞ÁöÑÊàêÂäüÊ°à‰æãÈÉΩÊòØÂ∞ÜËØ•Âπ≥Âè∞‰∫ßÂìÅÂåñÔºåÊåâÁªèËê•‰∫ßÂìÅÁöÑÊñπÂºèÊù•ËøêËê•‰ªñ„ÄÇÈÄöÂ∏∏ÊàêÂäüÁöÑÂÖ±‰∫´Ëá™ÊúçÂä°Âπ≥Âè∞ ÂàÜ‰∏∫‰∏â‰∏™Ê≠•È™§Êù•ÂÆûÁé∞Ëá™Ë∫´‰ª∑ÂÄºÔºåÂΩ¢ÊàêÂ¢ûÈïøÈ£ûËΩÆ„ÄÇ\n1. Â∞ùËØïÔºåËØÅÊòé‰ª∑ÂÄº ÊâÄË∞ì‰∏á‰∫ãÂºÄÂ§¥ÈöæÔºåÊûÑÂª∫ÂÖ±‰∫´ÊúçÂä°Âπ≥Âè∞ÈúÄË¶ÅÊâæÂà∞‰∏Ä‰∏™ÂêàÈÄÇÁöÑÂ∫îÁî®Âõ¢Èòü‰Ωú‰∏∫ÁßçÂ≠êÁî®Êà∑„ÄÇËØ•Âõ¢ÈòüÁöÑÂ∫îÁî®Âú∫ÊôØÂ∫îËØ•ÊòØ‰∏Ä‰∏™ÂÖ∏ÂûãÁî®‰æãÔºåÂ∫îÁî®Âõ¢Èòü ÈúÄË¶ÅÈ¢ëÁπÅÂèëÂ∏ÉÊù•‰∫§‰ªò‰∏öÂä°‰ª∑ÂÄºÔºåËÄåËØ•Â∫îÁî®Â∞ÜÊ∂âÂèäÂà∞Âü∫Á°ÄËÆæÊñΩÂõ¢ÈòüË¥üË¥£ÁöÑÊ®°ÂùóÔºå‰∏îË¶ÅÁ¨¶ÂêàÁªÑÁªáÂØπÂÆâÂÖ®ÊÄßÂíåÂêàËßÑÊÄßÁöÑÂÆ°Êü•„ÄÇÁî±ÂèåÊñπÂõ¢ÈòüÂÖ±Âêå ÂÆö‰πâ‰∫§‰∫íÊ®°ÂûãÔºå‰æãÂ¶ÇÔºåÂ¶Ç‰ΩïÂºÄÂèëÂü∫Á°ÄËÆæÊñΩÂç≥‰ª£Á†ÅÔºåÂπ≥Âè∞Âü∫Á°ÄÊû∂ÊûÑÔºåÂ∫îÁî®ÈÉ®ÁΩ≤ÊñπÂºè„ÄÇÂπ∂‰∏îÂ∫îÁî®Âõ¢ÈòüÊñπÈúÄË¶ÅËÆ§ÂèØËØ•Âπ≥Âè∞ÁöÑ‰ª∑ÂÄºÔºåÂú®ÁßçÂ≠êÂ∫îÁî® ÊàêÂäüËêΩÂú∞ÂÖ±‰∫´ÊúçÂä°Âπ≥Âè∞ÂêéÔºå‰ºöÈÄêÊ∏êÂ∞ÜÊõ¥Â§öÂ∫îÁî®ËêΩÂú∞Âà∞ÂÖ±‰∫´Âπ≥Âè∞Ê®°Âºè‰∏ä„ÄÇ\nÂêåÊó∂ÔºåÂπ≥Âè∞ÁöÑÊäÄÊúØÈÄâÈ°π‰πüÈùûÂ∏∏ÈáçË¶ÅÔºåË¶ÅÈÄÇÂ∫î‰ºÅ‰∏öËá™Ë∫´ÁöÑÁªÑÁªáÁÆ°ÁêÜÁªìÊûÑÂíåÊäÄÊúØÊ†àËÉΩÂäõ„ÄÇ‰æãÂ¶ÇÔºå\nAWS CloudFormation / AWS CDK: ÈÄöÁî®‰∏îÊúÄÁÅµÊ¥ªÁöÑÂÆûÁé∞ÔºõÂêåÊó∂‰πüÊòØÂèåÂàÉÂâëÔºåÁª¥Êä§Â§ßÂûãÁöÑ CloudFormation Ê®°ÊùøÊòØÈùûÂ∏∏Âõ∞ÈöæÁöÑÔºåÈááÁî® AWS CDK ÈúÄË¶ÅÂ≠¶‰π†Êñ∞ÁöÑÊäÄËÉΩÔºå‰∏îÊúâÊó∂ÈúÄË¶ÅÊ∑±ÂÖ•Á†îÁ©∂ÊâçÂèØËÉΩ Hack Êüê‰∫õÂÜÖÈÉ®ÂÆûÁé∞ÔºõÊòØÂ¶ÇÊûúÈúÄË¶ÅÂÆåÂÖ®ÊéåÊè°ÂÆûÁé∞ÁªÜËäÇÊó∂ÁöÑÈÄâÊã©Ôºå AWS Copilot: ‰ªÖÈÄÇÁî®‰∫é ECS ‰∏äÈÉ®ÁΩ≤ÁöÑÂÆπÂô®Â∫îÁî®ÔºõÂºÄÂèëÂ∫îÁî®ÁöÑÂõ¢Èòü‰∏çÂÖ≥ÂøÉÊàñ‰∏çÈúÄË¶ÅÁÆ°ÁêÜÂü∫Á°ÄËÆæÊñΩÔºå AWS Proton: ÈÄÇÁî®‰∫éÂ∞ÜÂ∫îÁî®ÂíåÂü∫Á°ÄËÆæÊñΩÊùÉÈôêÂàÜÁ¶ªÁöÑÂú∫ÊôØÔºõÊª°Ë∂≥Â∫îÁî®Âõ¢ÈòüÈúÄË¶ÅËá™ÊúçÂä°ÁöÑÊ®°Âºè„ÄÇ 2. Â§çÂà∂Êé®Âπø Âú®Êª°Ë∂≥‰∫ÜÁßçÂ≠êÂ∫îÁî®ÁöÑÈúÄÊ±ÇÂêéÔºåËÆ©ÂÖ±‰∫´ÊúçÂä°Âπ≥Âè∞Ëµ∞ÂêëÊàêÂäüÁöÑÂÖ≥ÈîÆÊòØÂ¶Ç‰ΩïËøêËê•Êé®Âπø‰ªñ„ÄÇ‰∫ßÂìÅÂåñÁöÑËøêËê•ÈúÄË¶ÅÂÅöÁöÑ‰ª•‰∏ãÂá†ÁÇπÔºå\nÊñáÊ°£Âåñ‰Ω†ÊûÑÂª∫ÁöÑÂπ≥Âè∞ÔºåÂåÖÊã¨ Ëá™ÊúçÂä°/Ëá™Âä®ÂåñÁöÑÈááÁî®Ê≠•È™§ Áî®Êà∑ÊâãÂÜå„ÄÅAPI ÊñáÊ°£ ÊîØÊåÅÁöÑÊú∫Âà∂ÔºåÂ¶Ç‰Ωï‰∏∫ÈááÁî®Âπ≥Âè∞ÁöÑÂÜÖÈÉ®Áî®Êà∑Êèê‰æõÊäÄÊúØÊîØÊåÅ ÂÖ¨ÂºÄ‰ª£Á†ÅÔºåËá≥Â∞ëÊòØÂÜÖÈÉ®ÂºÄÊ∫êÔºåËÆ©Êõ¥Â§öÁöÑÁî®Êà∑ÂèÇ‰∏éÂÖ±‰∫´ ÂÜÖÈÉ®Ëê•ÈîÄÂπ≥Âè∞ Âà©Áî®ÊàêÂäüÊ°à‰æãËµ¢ÂæóÈ¢ÜÂØº‰ª¨ÁöÑÊîØÊåÅÔºåËææÂà∞ÈáèÂåñÂπ≥Âè∞‰ª∑ÂÄºÔºåËßÑÈÅøÈ£éÈô©ÔºåÊèêÂçá‰∏öÂä°Á†îÂèëÊïàÁéá ÂÜÖÈÉ®\u0026quot;Ë∑ØÊºî\u0026quot;ÊãõÂãüÂõ¢ÈòüÂπ∂Ëé∑ÂæóÂèçÈ¶à ËØÅÊòé‰Ω†ÁöÑÂÖ±‰∫´ÊúçÂä°Âπ≥Âè∞ÊñπÂºèÂèØÊâ©Â±ï ËÄÉËôëÊîØÊåÅÂá†ÂçÅ‰∏äÁôæÁöÑÂõ¢Èòü ÂèØË∑®Ë∂äÂêÑÁßçÂú∫ÊôØÔºå‰æãÂ¶ÇÔºåÂ∫îÁî®Á®ãÂ∫è‰∏≠Êñ≠ÔºåAWS ÊúçÂä°‰∫ã‰ª∂ÔºåËøÅÁßªÔºå0-day Ë°•‰∏Å ËÄÉËôëÂπ≥Âè∞Â§çÂà∂ÁöÑÁì∂È¢àÂú®Âì™ÈáåÔºü‰æãÂ¶ÇÔºåÈááÁî®ÈöæÂ∫¶ÔºåÂ≠¶‰π†Êõ≤Á∫øÔºåÈÉ®ÁΩ≤ÔºåÊîØÊåÅ / on call Á≠âÁ≠â 3. ÂΩ¢ÊàêÈ£ûËΩÆ ÂÖ±‰∫´Ëá™ÊúçÂä°Âπ≥Âè∞ÊúÄÁªàÂΩ¢Êàê‰∫ßÁîüËá™Ë∫´Â¢ûÈïøÂäøÂ§¥ÁöÑËâØÊÄßÂæ™ÁéØÔºå‰æãÂ¶ÇÔºå\nÊäïËµÑÂπ≥Âè∞Âõ¢Èòü --\u0026gt; Ê∑ªÂä†Âπ≥Âè∞ÂäüËÉΩ --\u0026gt; Â∫îÁî®ÂºÄÂèëËÄÖÊõ¥Âø´‰πê --\u0026gt; Â¢ûÂä†Âπ≥Âè∞‰ΩøÁî® --\u0026gt; ÊèêÂçáÁªÑÁªáÁöÑÊïàÁéá --\u0026gt; ÊäïËµÑÂπ≥Âè∞Âõ¢Èòü --\u0026gt; ...\nÈÄöËøá‰ª•‰∏ä‰∏Ä‰∏™È£ûËΩÆÈó≠ÁéØÔºåËææÊàêÈïøÊúüÊàêÂäü„ÄÇ\n","link":"https://kane.mx/posts/2021/shared-service-platform-for-decentralized-developer-teams/","section":"posts","tags":["SSP","DevOps","GitOps","Infrastructure as Code","AWS","Proton","Service Catalog"],"title":"AWS‰∏äÊûÑÂª∫ÂÖ±‰∫´Ëá™ÊúçÂä°Âπ≥Âè∞ÊúçÂä°Âéª‰∏≠ÂøÉÂåñÁ†îÂèëÂõ¢Èòü"},{"body":"","link":"https://kane.mx/tags/proton/","section":"tags","tags":null,"title":"Proton"},{"body":"","link":"https://kane.mx/tags/service-catalog/","section":"tags","tags":null,"title":"Service Catalog"},{"body":"","link":"https://kane.mx/tags/ssp/","section":"tags","tags":null,"title":"SSP"},{"body":"","link":"https://kane.mx/tags/amazon-builders-library/","section":"tags","tags":null,"title":"Amazon Builders' Library"},{"body":"","link":"https://kane.mx/series/amazon-builders-library/","section":"series","tags":null,"title":"Amazon-Builders-Library"},{"body":"","link":"https://kane.mx/tags/resilience-engineering/","section":"tags","tags":null,"title":"Resilience Engineering"},{"body":"","link":"https://kane.mx/tags/system-design/","section":"tags","tags":null,"title":"System Design"},{"body":"AWSÊû∂ÊûÑÁöÑÂÆåÂñÑ(AWS Well-Architected)Ê°ÜÊû∂Ê∂âÂèä‰∫Ü‰∫îÂ§ßÊîØÊü±Ôºå ÂÖ∂‰∏≠ÂèØÈù†ÊÄßÊîØÊü±Ë¶ÅÊ±Ç‰æßÈáç‰∫éÁ°Æ‰øùÂ∑•‰ΩúË¥üËΩΩÂú®È¢ÑÊúüÁöÑÊó∂Èó¥ÂÜÖÊ≠£Á°Æ„ÄÅ‰∏ÄËá¥Âú∞ÊâßË°åÂÖ∂È¢ÑÊúüÂäüËÉΩ„ÄÇ ËøôË¶ÅÊ±ÇÂ∫îÁî®Á®ãÂ∫èÁ≥ªÁªüÂÖ∑Â§áÂºπÊÄßËÆæËÆ°ÔºåÂèØ‰ªéÊïÖÈöú‰∏≠Âø´ÈÄüÊÅ¢Â§çÔºå‰ª•‰æøÊª°Ë∂≥‰∏öÂä°ÂíåÂÆ¢Êà∑ÈúÄÊ±Ç„ÄÇ ÁÑ∂ËÄåËÆæËÆ°„ÄÅÂºÄÂèë„ÄÅ‰∏îÈ™åËØÅÂÖ∑Â§áÂºπÊÄßËÆæËÆ°ÁöÑÂ∫îÁî®Á®ãÂ∫èÔºåÂØπÁªèÈ™åÂíåÂÆûË∑µËÉΩÂäõÈÉΩÊúâÂæàÈ´òÁöÑË¶ÅÊ±Ç„ÄÇ Âà©Áî®ÊàêÁÜüÁöÑÁªèÈ™åÂíåËâØÂ•ΩÁöÑÂ∑•ÂÖ∑Â∞ÜÂä†Âø´ÊûÑÂª∫Á¨¶ÂêàÈ¢ÑÊúüÁöÑÂºπÊÄßÂ∫îÁî®Á®ãÂ∫è„ÄÇ\nApplication Resilience WorkshopÊòØ‰∏ÄÂ•óËØæÁ®ãÂíåÂä®ÊâãÂÆûË∑µÂ≠¶‰π†Â¶Ç‰ΩïËøõË°åÂÆûÈ™åÊù•ËßÇÂØüÁ≥ªÁªüÁöÑË°å‰∏∫Ôºå ‰æãÂ¶ÇÔºåÊûÅÁ´ØÁ≥ªÁªüË¥üËΩΩÂíåÁΩëÁªú‰∏≠Êñ≠ÊÉÖÂÜµ‰∏ãÔºå‰ΩøÁî®‰∏çÂêåÁöÑËΩØ‰ª∂Ê®°ÂºèÊù•ÂáèËΩªËøô‰∫õÂÆûÈ™åÂØπÁ≥ªÁªüÁ®≥ÊÄÅÁöÑÂΩ±Âìç„ÄÇ Êï¥‰∏™ÂÆûÈ™å‰πüÊòØÂàÜ‰∏∫ÂÅáËÆæ„ÄÅÊñπÊ≥ï„ÄÅËßÇÊµãÂíåÁºìËß£Á≠âÊ≠•È™§ÔºåÂêåÊ∑∑Ê≤åÂ∑•Á®ãÊúâÂºÇÊõ≤ÂêåÂ∑•‰πãÂ§Ñ„ÄÇ\nÂ∫îÁî®Á®ãÂ∫èÂºπÊÄßÂÆûÈ™åÂÅáËÆæ‰∫Ü‰∏Ä‰∏™ÂæÆÊúçÂä°ÊûÑÂª∫ÁöÑÂ∫îÁî®Á®ãÂ∫èÔºåÈÄöËøáÂéãÂäõÊµãËØïÂ∑•ÂÖ∑Ê≥®ÂÖ•ÊûÅÁ´ØÁöÑÁ≥ªÁªüË¥üËΩΩÔºå ÈÄöËøáÂ∫îÁî®Á®ãÂ∫èÂêÑÊúçÂä°ÁöÑÂèØËßÇÊµãÊÄßÊù•ÁêÜËß£ÁõÆÊ†áÂ∫îÁî®ÁöÑÂª∂ËøüÔºåÂêûÂêê„ÄÅÂÆπÈáè„ÄÅRTOÁ≠âÊåáÊ†á„ÄÇ\nÂΩìÂèëÁé∞ÊúÄÂàùËÆæËÆ°ÁöÑÂ∫îÁî®Á®ãÂ∫èÂú®ÊûÅÁ´ØÂéãÂäõ‰∏ã‰ºöÊúâÁÅæÈöæÊÄßÁöÑÊïÖÈöúÔºåÊïôÁ®ã‰∏≠ÁªôÂá∫‰∫ÜÈòüÂàóÔºåË¥üËΩΩÂç∏ËΩΩÔºå ÈÄöËøáÈôç‰ΩéÊúçÂä°QoSÁöÑClient Deadline CutoffÔºåÊñ≠Ë∑ØÂô®Ôºå‰ª§ÁâåÊ°∂Á≠âÁ®ãÂ∫èËÆæËÆ°Ê®°ÂºèÊù•ÁºìËß£ÊûÅÁ´ØÂéãÂäõÂØπÁ≥ªÁªüÈÄ†ÊàêÁöÑÁÅæÈöæÊÄßÁöÑÊïÖÈöú„ÄÇ\n‰ΩÜÊòØÊó†ËÆ∫‰ΩøÁî®ÈòüÂàóËß£ËÄ¶ËøòÊòØË¥üËΩΩÂç∏ËΩΩÈÉΩ‰∏çÊòØÁªùÂØπÂÆåÁæéÁöÑËß£ÂÜ≥ÊñπÊ°àÔºåÂú®Amazon Builders' Library ‰∏≠ÁöÑÂá†ÁØáÊñáÁ´†‰∏∫Êàë‰ª¨ÂàÜ‰∫´‰∫ÜAmazon‰ªéËøêË°åÂ§ßËßÑÊ®°ÂàÜÂ∏ÉÂºèÁ≥ªÁªü‰∏≠Â≠¶‰π†Âà∞ÁöÑÂÆùË¥µ‰∏îÊàêÁÜüÁöÑÁªèÈ™åÔºå\nÈÅøÂÖçÊó†Ê≥ïÂÖãÊúçÁöÑÈòüÂàóÁßØÂéã ÈÄöËøáÂç∏Èô§Ë¥üËΩΩÊù•ÈÅøÂÖçËøáËΩΩ Ë∂ÖÊó∂„ÄÅÈáçËØïÂíåÊäñÂä®ÂõûÈÄÄ Âà©Áî®Ëøô‰∫õÊàêÁÜüÁöÑÁªèÈ™åÊàë‰ª¨ÂèØ‰ª•ÊùÉË°°Á≥ªÁªüÁöÑÈúÄÊ±ÇÂíåÊäÄÊúØÂÆûÁé∞ÔºåÈÄâÊã©ÂΩì‰∏ãÊúÄÂêàÁêÜ‰∏îÂèØË°åÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ\nÂú®‰ªäÂπ¥ÁöÑPre-re:Invent‰πãÈôÖÔºåAWSÂèëÂ∏É‰∫ÜAWS Resilience HubÊúçÂä°ÔºåÂ∞ÜÂ∫îÁî®Á®ãÂ∫è‰∫ë‰∏äËµÑÊ∫êÁä∂ÊÄÅÁöÑÊâ´ÊèèÔºå Á≥ªÁªüÂºπÊÄßÁöÑËØÑ‰º∞ÔºåÁ¨¶ÂêàRPO/RTOÁöÑÈÖçÁΩÆÂª∫ËÆÆÔºå‰ª•ÂèäÂü∫‰∫éÊ∑∑Ê≤åÂ∑•Á®ãÁöÑÂÆûÈ™åËøêË°åÈõÜÊàê‰∏∫‰∏Ä‰∏™Êï¥‰ΩìÁöÑÊúçÂä°ÔºåÈÄöËøá‰∏Ä‰∏™ÊéßÂà∂Èù¢ÊùøÂÆûÁé∞‰∫ÜÂ∫îÁî®Á®ãÂ∫èÂºπÊÄßÁöÑÁÆ°ÁêÜ„ÄÇ\nÊ≠§Â§ñÔºåÂ¶ÇÊûú‰Ω†ÁöÑÂ∫îÁî®ÂàöÂ•ΩÊòØ‰∏Ä‰∏™ÁîµÂïÜÊàñÂú®Á∫øÁ•®Âä°Á≥ªÁªüÔºåÁ≥ªÁªüÊ≠£Âú®Èù¢ÂØπÁßíÊùÄ„ÄÅÈªë‰∫îÁ≠âÂ§ßËßÑÊ®°Ë¥üËΩΩÂéãÂäõÁöÑËÄÉÈ™åÔºåÂèØ‰ª•ÂèÇËÄÉÁîöËá≥Áõ¥Êé•Â∞ùËØïAWSËß£ÂÜ≥ÊñπÊ°à AWS Virtual Waiting RoomÊù•Áõ¥Êé•ÊûÑÂª∫‰∏Ä‰∏™ÂºπÊÄßÁ≥ªÁªü„ÄÇ\n","link":"https://kane.mx/posts/2021/application-resilience/","section":"posts","tags":["AWS","resilience engineering","Amazon Builders' Library","System Design"],"title":"Â∫îÁî®Á®ãÂ∫èÂºπÊÄßËÆæËÆ°"},{"body":"","link":"https://kane.mx/tags/metaverse/","section":"tags","tags":null,"title":"Metaverse"},{"body":"","link":"https://kane.mx/tags/nft/","section":"tags","tags":null,"title":"Nft"},{"body":"","link":"https://kane.mx/tags/%E5%85%83%E5%AE%87%E5%AE%99/","section":"tags","tags":null,"title":"ÂÖÉÂÆáÂÆô"},{"body":"ÂÖÉÂÆáÂÆôÊòØËøëÊúüÁöÑÁÉ≠ÁÇπËØùÈ¢òÔºåËøëÊúüÂÅö‰∫Ü‰∫õÂ≠¶‰π†‰∫ÜËß£ÔºåÂ∞Ü‰∏Ä‰∫õÂ≠¶‰π†ÂÜÖÂÆπÊÄªÁªì‰∏∫‰∏Ä‰∏™deck„ÄÇÂàÜ‰∫´Â¶Ç‰∏ãÔºå\n","link":"https://kane.mx/posts/2021/metaverse/","section":"posts","tags":["metaverse","nft","ÂÖÉÂÆáÂÆô"],"title":"ÂÖÉÂÆáÂÆôÈ£éÂè£‰∏ãÁöÑÊú∫‰ºö"},{"body":"","link":"https://kane.mx/tags/aws-fault-injection-simulator/","section":"tags","tags":null,"title":"AWS Fault Injection Simulator"},{"body":"","link":"https://kane.mx/tags/aws-fis/","section":"tags","tags":null,"title":"AWS FIS"},{"body":"Ê∑∑Ê≤åÂ∑•Á®ãÊòØ‰∏ÄÁßçÂ∏ÆÂä©Á≥ªÁªüÊª°Ë∂≥ÂºπÊÄßÈúÄÊ±ÇÁöÑÊäÄÊúØÔºåÂÆÉËµ∑Ê∫ê‰∫éNetflixÁöÑÂ∑•Á®ãÂÆûË∑µÔºåËëóÂêçÁöÑÁå¥Â≠êÂÜõÂõ¢„ÄÇ\nAWS‰∏ÄÁõ¥ÊèêÂÄ°Êû∂ÊûÑÁöÑÂÆåÂñÑ(AWS Well-Architected)ÔºåÊ∑∑Ê≤åÂ∑•Á®ãÊ≠£ÊòØÂçìË∂äËøêËê•ÂíåÂèØÈù†ÊÄßÊîØÊü±ÁöÑÂÆûË∑µ„ÄÇ Âõ†Ê≠§Âú® re:Invent 2020 AWSÂèëÂ∏É‰∫ÜFault Injection SimulatorÊúçÂä°Êù•ÁÆÄÂåñÂºÄÂèëËÄÖÂú®AWS‰∏äÁöÑÊ∑∑Âä®Â∑•Á®ãÂÆûË∑µ„ÄÇ\nAWS FIS‰Ωú‰∏∫AWS‰∏äÂéüÁîüÁöÑÊ∑∑Ê≤åÂ∑•Á®ãÊúçÂä°ÔºåÁõÆÂâçÂ∑≤ÂêåEC2ÔºåECSÔºåEKSÔºåRDSÔºåCloudWatchÔºåÁîöËá≥ÊòØIAM Role APIÈõÜÊàêÔºåÂèØ‰ª•Ëß¶ÂèëËøô‰∫õÊúçÂä°‰∏≠ËµÑÊ∫êÁöÑÂèòÊõ¥Êù•ÂÅáËÆæÊïÖÈöúÔºå ‰æãÂ¶ÇÔºåÈáçÂêØÊàñÁªàÊ≠¢EC2ÂÆû‰æãÔºåÈáçÂêØRDSÂÆû‰æãÁ≠â„ÄÇ\nChaos Engineering on AWSÊòØ‰∏Ä‰ªΩÈùûÂ∏∏ËØ¶ÁªÜÁöÑÊ∑∑Ê≤åÂ∑•Á®ãÂú®AWS‰∏äÂä®ÊâãÂÆûÈ™å„ÄÇ ËØ•ÂÆûÈ™åÂ∞ÜÊåáÂØºÂèÇ‰∏éËÄÖÂø´ÈÄüËÆæÁΩÆÂÆûÈ™åÂàùÂßãÁéØÂ¢ÉÔºåÈÄöËøáÂèØËßÇÊµãÊÄßÂ∑•ÂÖ∑‰∫ÜËß£Á≥ªÁªüÁä∂ÊÄÅÔºåÁÑ∂ÂêéÂ∏¶È¢ÜÂÆûÈ™åÂèÇ‰∏éËÄÖÈÄöËøáËØ¶ÁªÜÁöÑÂÆûÈ™åÊ≠•È™§Â≠¶‰π†Â¶Ç‰Ωï‰ΩøÁî®FISÊúçÂä°Êù•ËææÂà∞ÂØπÁ≥ªÁªüÂèØÈù†ÊÄßÁöÑÈ™åËØÅÂíå‰ºòÂåñ„ÄÇ ÂÆûÈ™åÈ°πÁõÆÈô§‰∫ÜË¶ÜÁõñFISÊîØÊåÅÈõÜÊàêÁöÑEC2ÔºåECSÔºåRDSÁ≠âÊúçÂä°Â§ñÔºåËøòÊºîÁ§∫‰∫ÜSSMÈõÜÊàêÔºåÂπ∂‰∏îÈÄöËøáFISÂÜÖÁΩÆÁöÑSSMÊñáÊ°£ÊàñËá™ÂÆö‰πâÁöÑSSMÊñáÊ°£Êù•ÂÅáËÆæÁ≥ªÁªüÊïÖÈöú„ÄÇ Ê∑∑Ê≤åÂ∑•Á®ã‰Ωú‰∏∫‰∏ÄÁßçÊèêÂçáÁ≥ªÁªüÂºπÊÄßÁöÑË¥®ÈáèÊâãÊÆµÔºåÈúÄË¶ÅÈáçÂ§çÊÄßÁöÑÂú®Á≥ªÁªü‰∏≠ÂÆûÈ™åÔºåÂä®ÊâãÂÆûÈ™å‰πü‰∏∫ÂèÇ‰∏éËÄÖËÆæËÆ°‰∫ÜCI/CDÂÆûÈ™åÔºåÈÄöËøáGitopsÊñπÂºèÂ∞ÜÊ∑∑Âä®Â∑•Á®ãÊåÅÁª≠ÂÆûÈ™åÂà∞Á≥ªÁªüÁéØÂ¢É‰∏≠„ÄÇ ÊÄª‰πãÔºåÂØπÊ∑∑Ê≤åÂ∑•Á®ãÊúâÂÖ¥Ë∂£ÁöÑÂºÄÂèëËÄÖÔºåChaos Engineering on AWSÈùûÂ∏∏ÂÄºÂæó‰∏ÄÂÅöÁöÑÂä®ÊâãÂÆûÈ™åÔºåÂèØ‰ª•Âø´ÈÄüÁöÑÂ∏ÆÂä©ÊÇ®‰∫ÜËß£Ê∑∑Ê≤åÂ∑•Á®ãÔºåÂèäFISÂú®AWS‰∏äÁöÑÂÆûË∑µ„ÄÇ\nÊúÄÂêéÂÜçÂº∫Ë∞É‰∏ãÊúÄÈáçË¶ÅÁöÑ‰∫ãÔºåÊ∑∑Ê≤åÂ∑•Á®ã‰∏çÊòØ‰∏Ä‰∏™Â≠§Á´ãÁöÑÁ≥ªÁªüÂºπÊÄßÂÆûÈ™åÔºåÂÆÉÈúÄË¶ÅÁ≥ªÁªüÊú¨Ë∫´ÁöÑÂºπÊÄß„ÄÅÂèØÈù†ÊÄßËÆæËÆ°‰ª•ÂèäÂèØËßÇÊµãÊÄßÁöÑÂÆûÁé∞ÔºåÊòØ‰∏Ä‰∏™Á≥ªÁªüÊï¥‰ΩìÁöÑËÆæËÆ°ÂÆûË∑µ„ÄÇ\n","link":"https://kane.mx/posts/2021/chaos-engineering-on-aws/","section":"posts","tags":["AWS","chaos engineering","AWS Fault Injection Simulator","AWS FIS"],"title":"AWS‰∏äÁöÑÊ∑∑Ê≤åÂ∑•Á®ã"},{"body":"","link":"https://kane.mx/tags/chaos-engineering/","section":"tags","tags":null,"title":"Chaos Engineering"},{"body":"","link":"https://kane.mx/tags/filevault/","section":"tags","tags":null,"title":"Filevault"},{"body":"","link":"https://kane.mx/tags/macos-monterey/","section":"tags","tags":null,"title":"MacOS Monterey"},{"body":"","link":"https://kane.mx/tags/macosx/","section":"tags","tags":null,"title":"MacOSX"},{"body":"I'm trying to upgrade my Macbook Pro to macOS Monterey, however the installation can not be started due to the disk is encrypted by Filevault \u0026#x1f615; I have to turn off Filevault to disable disk encrpytion before installing macOS Monterey.\nI found this support article on how turning off Filevault, but it does not work at all. There is nothing hint or error message after clicking the option Turn off Filevault.\nAfter researching it for a while, I found this post via CLI command,\n1sudo fdesetup disable But above command also does not work, it exits with error code -69594.\n1sudo fdesetup disable 2Enter the user name:kane 3Enter the password for user \u0026#39;kane\u0026#39;: 4FileVault was not disabled (-69594). I found some articles said that the Filevault only can be disabled by the user whom enables it. I found below command to show the user whom enabled the Filevault, it's enabled by an unknown user! I don't have idea how enabling it.\n1sudo fdesetup list -extended 2ESCROW UUID TYPE USER 3 2D3F7CA5-4ED4-4537-8DA2-98B1E3637954 Unknown User Finally I found below command line to disable Filevault though I don't know which user enabled it.\n1diskutil apfs disableFileVault disk1s1 -user disk Input the disk password when booting the macOS. The disabling Filevault will be processed in backgroud, you can retrieve the progress by below command,\n1diskutil apfs list Happy Monterey!\n","link":"https://kane.mx/posts/2021/turn-off-filevault-on-macosx/","section":"posts","tags":["MacOSX","macOS Monterey","filevault","Tip"],"title":"Turn off Filevault on macOS"},{"body":"","link":"https://kane.mx/tags/aws-ecr/","section":"tags","tags":null,"title":"AWS-ECR"},{"body":"","link":"https://kane.mx/tags/helm/","section":"tags","tags":null,"title":"Helm"},{"body":"I met a case to mirror existing Helm charts to another repository. It might be caused by network availability or compliance requirements.\nThere are multiple ways to host a Helm repository, for example, Nexus OSS Repository, Github Pages, AWS ECR and so on.\nAmazon Elastic Container Registry (Amazon ECR) is a fully managed container registry that makes it easy to store, manage, share, and deploy your container images and artifacts anywhere. It's built with scale and secure. In my case I'm using this existing service to mirror the Helm charts.\nI created a script to mirror existing Helm chart to AWS ECR based on the official guide in ECR doc.\nFor example, below code snippet mirrors eks-charts/aws-load-balancer-controller with version 1.2.7 to Amazon ECR,\n1helm repo add eks-charts https://aws.github.io/eks-charts 2helm pull eks-charts/aws-load-balancer-controller --version 1.2.7 3./push-helm-chart-to-all-ecr-regions.sh aws-load-balancer-controller 1.2.7","link":"https://kane.mx/posts/2021/mirror-helm-chart-to-aws-ecr/","section":"posts","tags":["AWS","AWS-ECR","Helm","Kubernetes"],"title":"Mirror Helm Charts to AWS ECR"},{"body":"","link":"https://kane.mx/tags/amazon-neptune/","section":"tags","tags":null,"title":"Amazon Neptune"},{"body":"","link":"https://kane.mx/tags/graph-database/","section":"tags","tags":null,"title":"Graph Database"},{"body":"Amazon Neptune is a managed Graph database on AWS, whose compute and storage is decoupled like Amazon Aurora. Neptune leverages popular open-source APIs such as Gremlin and SPARQL, and easily migrate existing applications.\nAfter exploring Neptune few months in solution, I have below few learnings,\nBulk loading Always meet the ConcurrentModificationExceptions when concurrently loading vertices/edges into Neptune. Using neptune-python-utils with retry backoff can improve it, however it requires the expensive large Neptune instance.\nThe best way of batch loading the large vertices/edges into Neptune is using the bulk load feature, it works fine though the instance of Neptune is small. The loading time depends on the instance size of Neptune.\nproperties of vertice In my use case, I store the embedding as properties of vertices like relation database. There are almost 400 properties for every vertices, the query performance is bad with large number of properties. Due to the embedding properties will not be queried, consolidating the 400 properties as a single one properties to improve the query performance.\nstreams Neptune Streams logs every change to the graph. It's a Lab feature in 2019, and GA in 2020. However there is no Lambda integration now! It means you can not process the Neptune streams in Lambda functions!\nTools Neptune Tools Amazon Neptune Tools is a toolkit maintained by Neptune service team.\nNeptune sigv4 The script can connect Neptune to call control plane APIs with aswauthsigv4 and proxy support.\n","link":"https://kane.mx/posts/2021/the-practise-of-amazon-neptune/","section":"posts","tags":["graph database","Amazon Neptune","AWS"],"title":"The practise of Amazon Neptune"},{"body":"","link":"https://kane.mx/tags/amazon-eks/","section":"tags","tags":null,"title":"Amazon EKS"},{"body":"","link":"https://kane.mx/tags/sonatype-nexus/","section":"tags","tags":null,"title":"Sonatype Nexus"},{"body":"Last year I shared the production-ready, cloud native solution to deploy Sonatype Nexus Repository OSS on AWS.\nThe solution has an update with below notable changes,\nsupport specifying EKS version, v1.20, v1.19, and v1.18 are supported versions support provisioning to existing VPC support provisioning to existing EKS(require EKS v1.17+) update aws-efs-csi-driver to 1.3.1 update aws-load-balancer-controller to 2.2.0 See the solution page for detail usage.\n","link":"https://kane.mx/posts/2021/nexus-oss-on-aws-v110-update/","section":"posts","tags":["Amazon EKS","Kubernetes","Helm","AWS CDK","AWS","Sonatype Nexus"],"title":"The update of Sonatype Nexus repository OSS on AWS solution"},{"body":"","link":"https://kane.mx/tags/nat/","section":"tags","tags":null,"title":"NAT"},{"body":"","link":"https://kane.mx/tags/network/","section":"tags","tags":null,"title":"Network"},{"body":"Êú¨ÊñπÊ°àÁöÑËµ∑Âõ†ÊòØÔºå‰∏Ä‰∏™Ê∫ê‰ª£Á†ÅÊâòÁÆ°Âú®Github‰∏äÁöÑÈ°πÁõÆfix‰∏Ä‰∏™ÈáçË¶ÅÁöÑbugÂêéÔºåÂú®AWS‰∏äÁöÑÊåÅÁª≠ÈÉ®ÁΩ≤ÊµÅÊ∞¥Á∫ø‰∏ÄÁõ¥Â§±Ë¥•„ÄÇÂàÜÊûêÊó•ÂøóÂêéÔºåÂèëÁé∞ÊµÅÊ∞¥Á∫ø‰∏≠ÁöÑÊï∞‰∏™Ê≠•È™§ÈúÄË¶ÅÂÖãÈöÜÊ∫ê‰ª£Á†ÅÔºå‰ΩÜÊòØËÆøÈóÆGithubÁöÑÁΩëÁªúÈùûÂ∏∏‰∏çÁ®≥ÂÆöÔºåËøôÊï∞‰∏™ÊµÅÊ∞¥Á∫ø‰ªªÂä°ÊåÅÁª≠Âõ†ËøûÊé•Ë∂ÖÊó∂ÔºåËøûÊé•ÊãíÁªùÁ≠âÁΩëÁªúÈîôËØØËÄåÂ§±Ë¥•„ÄÇËÄåÊµÅÊ∞¥Á∫ø‰ªªÂä°Â§ßÈáè‰ΩøÁî®‰∫ÜCodeBuild, LambdaÁ≠âAWSÊâòÁÆ°ÊúçÂä°ÔºåÊó†Ê≥ï‰∏∫ÊâßË°åÁéØÂ¢ÉÈÖçÁΩÆÂèØÈù†ÁöÑÁΩëÁªúËøûÊé•„ÄÇ\nÊú¨ÊñπÊ°àÊÄùË∑ØÂ¶Ç‰∏ãÔºå\nÂú® VPC public subnets ‰∏≠ÂàõÂª∫ NAT instance Âç≥ EC2 ËôöÊãüÊú∫Ôºå ÈÖçÁΩÆ NAT instanceÔºå‰ΩøÁî® tunnel ÁΩëÁªúËÆøÈóÆ githubÔºå ‰øÆÊîπ private subnets ÁöÑË∑ØÁî±Ë°®ÔºåÊ∑ªÂä† github ÊúçÂä°ÁöÑ IP CIDRsÔºåÂ∞ÜÂØπËøô‰∫õIPÂú∞ÂùÄÁöÑËØ∑Ê±ÇÈÄöËøá NAT instance ËΩ¨Âèë„ÄÇ Áªº‰∏äÔºåÂÆûÁé∞‰∫Ü‰∏çÁî®ÂØπÁé∞ÊúâÊåÅÁª≠ÈÉ®ÁΩ≤ÊµÅÊ∞¥Á∫øÂÅö‰ªª‰Ωï‰øÆÊîπÔºåÊµÅÊ∞¥Á∫ø‰∏≠ËøêË°åÂú® VPC private subnet ÂÜÖÁöÑ‰ªªÂä°(ÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫éCodeBuild, Fargate, Lambda, GlueÁ≠â)ÔºåÂØπÂ§ñÁΩëÁöÑËØ∑Ê±ÇÁõÆÊ†áÂú∞ÂùÄÂ¶ÇÂú®Ë∑ØÁî±Ë°®ÁöÑÁâπÊÆäËßÑÂàô(IP CIDRs)‰∏≠ÔºåÁΩëÁªúËØ∑Ê±ÇÂ∞Ü‰ºöÈÄöËøá NAT instance Êù•ËΩ¨Âèë„ÄÇ\n‰∏∫Ê≠§ÔºåÂàõÂª∫‰∫Ü‰∏Ä‰∏™Âü∫‰∫é AWS CDK construct ÁöÑÂºÄÊ∫êÈ°πÁõÆ SimpleNAT Êù•Â∞ÅË£ÖÂíåÂ§çÁî®ÂàõÂª∫ÈÖçÁΩÆ NAT instancesÔºåÂπ∂‰∏îÂ∞ÜÊåáÂÆöÁöÑIPÂú∞ÂùÄÊÆµÊõ¥Êñ∞Âà∞Ë∑ØÁî±Ë°®ËÆæÁΩÆË∑ØÁî±ËßÑÂàô„ÄÇ\nËØ•È°πÁõÆÂêåÊó∂Êèê‰æõ‰∫Ü‰∏Ä‰∏™ÂÆåÊï¥Á§∫‰æãÂ∫îÁî®ÔºåÊºîÁ§∫‰∫ÜÂ¶Ç‰ΩïÈÖçÁΩÆ NAT instance ‰ΩøÁî® sshuttle Âª∫Á´ãÁΩëÁªúÈößÈÅìÔºåÂπ∂‰∏îÂ∞ÜÊåáÂÆöÁöÑIPÂú∞ÂùÄÊÆµËØ∑Ê±ÇÈÄöËøá NAT instance Êù•ËΩ¨Âèë„ÄÇ\n","link":"https://kane.mx/posts/2021/simple-nat-on-aws/","section":"posts","tags":["AWS","Tip","network","NAT","CDK Construct","AWS CDK"],"title":"Âú®AWS‰∏äÂø´ÈÄüÈÉ®ÁΩ≤‰∏ìÁî®ÁöÑNATÂÆû‰æã"},{"body":"Infrastructure as Code is the trend to manage the resources of application. AWS CloudFormation is the managed service offering the IaC capability on AWS since 2011. CloudFormation uses the declarative language to manage your AWS resources with the style what you get is what you declare.\nHowever there are cons of CloudFormation as a declarative language,\nthe readability and maintenance for applications involving lots of resources the reuseable of code, CloudFormation modules released in re:Invent 2020 might help mitigate it AWS CDK provides the programming way to define the infra in code by your preferred programming languages, such as Typescript, Javascript, Python, Java and C#. AWS CDK will synthesis the code to CloudFormation template, then deploying the stack via AWS CloudFormation service. It benefits the Devops engineers manage the infra on AWS as programming application, having version control, code review, unit testing, integration testing and CI/CD pipelines, the deployment still depends on the mature CloudFormation service to rolling update the resources and rollback when failing.\nFor solution development, using CDK indeed improves the productivity then publish the deployment assets as CloudFormation templates.\nThough CDK application can be synthesized to CloudFormation template, there are still some differences blocking the synthesized templates to be deployed across multiple AWS regions.\nThis post will share the tips on how effectively writing AWS CDK application then deploying the application by CloudFormation across multiple regions.\nGeneral Environment-agnostic stack Don‚Äôt specify env with account and region like below that will generate account/region hardcode in CloudFormation template.\n1new MyStack(app, \u0026#39;Stack1\u0026#39;, { 2 env: { 3 account: \u0026#39;123456789012\u0026#39;, 4 region: \u0026#39;us-east-1\u0026#39; 5 }, 6}); use CfnMapping/CfnCondition instead of if-else clause CloudFormation does not have logistic processing like programming language. Use CfnMapping or CfnCondition instead.\nNote: the CfnMapping does not support default value, you have to list all supported regions like below code snippet,\n1getAwsLoadBalancerControllerRepo() { 2 const albImageMapping = new cdk.CfnMapping(this, \u0026#39;ALBImageMapping\u0026#39;, { 3 mapping: { 4 \u0026#39;me-south-1\u0026#39;: { 5 2: \u0026#39;558608220178\u0026#39;, 6 }, 7 \u0026#39;eu-south-1\u0026#39;: { 8 2: \u0026#39;590381155156\u0026#39;, 9 }, 10 \u0026#39;ap-northeast-1\u0026#39;: { 11 2: \u0026#39;602401143452\u0026#39;, 12 }, 13 \u0026#39;ap-northeast-2\u0026#39;: { 14 2: \u0026#39;602401143452\u0026#39;, 15 }, 16 ... 17 \u0026#39;ap-east-1\u0026#39;: { 18 2: \u0026#39;800184023465\u0026#39;, 19 }, 20 \u0026#39;af-south-1\u0026#39;: { 21 2: \u0026#39;877085696533\u0026#39;, 22 }, 23 \u0026#39;cn-north-1\u0026#39;: { 24 2: \u0026#39;918309763551\u0026#39;, 25 }, 26 \u0026#39;cn-northwest-1\u0026#39;: { 27 2: \u0026#39;961992271922\u0026#39;, 28 }, 29 } 30 }); 31 return `${albImageMapping.findInMap(cdk.Aws.REGION, \u0026#39;2\u0026#39;)}.dkr.ecr.${cdk.Aws.REGION}.${cdk.Aws.URL_SUFFIX}/amazon/aws-load-balancer-controller`; 32 } never use Stack.region Don‚Äôt rely on stack.region to do the logistic for China regions. Use additional context parameter or CfnMapping like below snippet,\n1const partitionMapping = new cdk.CfnMapping(this, \u0026#39;PartitionMapping\u0026#39;, { 2 mapping: { 3 aws: { 4 nexus: \u0026#39;quay.io/travelaudience/docker-nexus\u0026#39;, 5 nexusProxy: \u0026#39;quay.io/travelaudience/docker-nexus-proxy\u0026#39;, 6 }, 7 \u0026#39;aws-cn\u0026#39;: { 8 nexus: \u0026#39;048912060910.dkr.ecr.cn-northwest-1.amazonaws.com.cn/quay/travelaudience/docker-nexus\u0026#39;, 9 nexusProxy: \u0026#39;048912060910.dkr.ecr.cn-northwest-1.amazonaws.com.cn/quay/travelaudience/docker-nexus-proxy\u0026#39;, 10 }, 11 } 12 }); 13partitionMapping.findInMap(cdk.Aws.PARTITION, \u0026#39;nexus\u0026#39;); Use core.Aws.region token referred to the region which region of the stack is deployed.\nexplicitly add dependencies on resources to control the creation/deletion order of resources For example, when deploying a solution with creating a new VPC with NAT gateway, then deploying EMR cluster in private subnets of VPC. The EMR cluster might fail on creation due to network issue. It‚Äôs caused by the NAT gateway is not ready when initializing the EMR cluster, you have to manually create the dependencies among EMR cluster and NAT gateway.\nAlways override the logical ID of CloudFormation resource when creating AWS resources with unique name UPDATED: 2023/10/24\nWhen creating an AWS resource via CDK with a friendly name, for example, you create a Glue Table named my-table in CDK. The logical ID will be generated by CDK constructs' name inheritance, however, you might refactor your constructs in the system design level. The default logical ID of the resource will be changed after your refactor or renaming the ID of construct. After the logical ID changes, the resource will be replaced when updating the CloudFormation stack to a new template. In the updating process of CloudFormation stack, a new resource will be created firstly, however, the resource creation will fail due to the conflict resource name. Currently, the workaround is that explicitly overriding the logical ID of the AWS resource created by CDK to avoid the replacement in stack updating. Explicitly override the logical ID will maintain the code readability and avoid the unintended the failure of stack update.\n1(table.node.defaultChild as CfnResource).overrideLogicalId(tableLogicId); EKS module(@aws-cdk/aws-eks) specify kubectl layer when creating EKS cluster NOTE: This tricky only applies for AWS CDK prior to 1.81.0. CDK will bundle kubectl, helm and awscli as lambda layer instead of SAR application since 1.81.0, it resolves below limitation.\nEKS uses a lambda layer to run kubectl/helm cli as custom resource, the @aws-cdk/aws-eks module depends on the Stack.region to check the region to be deployed in synthesizing phase. It violates the principle of Environment-agnostic stack! Use below workaround to create the EKS cluster,\n1const partitionMapping = new cdk.CfnMapping(this, \u0026#39;PartitionMapping\u0026#39;, { 2 mapping: { 3 aws: { 4 // see https://github.com/aws/aws-cdk/blob/60c782fe173449ebf912f509de7db6df89985915/packages/%40aws-cdk/aws-eks/lib/kubectl-layer.ts#L6 5 kubectlLayerAppid: \u0026#39;arn:aws:serverlessrepo:us-east-1:903779448426:applications/lambda-layer-kubectl\u0026#39;, 6 }, 7 \u0026#39;aws-cn\u0026#39;: { 8 kubectlLayerAppid: \u0026#39;arn:aws-cn:serverlessrepo:cn-north-1:487369736442:applications/lambda-layer-kubectl\u0026#39;, 9 }, 10 } 11}); 12 13const kubectlLayer = new eks.KubectlLayer(this, \u0026#39;KubeLayer\u0026#39;, { 14 applicationId: partitionMapping.findInMap(cdk.Aws.PARTITION, \u0026#39;kubectlLayerAppid\u0026#39;), 15}); 16const cluster = new eks.Cluster(this, \u0026#39;MyK8SCluster\u0026#39;, { 17 vpc, 18 defaultCapacity: 0, 19 kubectlEnabled: true, 20 mastersRole: clusterAdmin, 21 version: eks.KubernetesVersion.V1_16, 22 coreDnsComputeType: eks.CoreDnsComputeType.EC2, 23 kubectlLayer, 24}); If you're interested on this issue, see cdk issue for detail.\nmanage the lifecycle of helm chart deployment The k8s helm chart might create AWS resources out of CloudFormation scope. You have to manage the lifecycle of those resources by yourself.\nFor example, there is an EKS cluster with AWS load balancer controller, then you deploy a helm chart with ingress that will create ALB/NLB by the chart, you must clean those load balancers in deletion of the chart. Also the uninstall of Helm chart is asynchronous, you have to watch the deletion of resource completing before continuing to clean other resources.\nTHE END The tips will be updated when something new is found or the one is deprecated after CDK is updated.\nHAPPY CDK \u0026#x1f606;\n","link":"https://kane.mx/posts/2020/effective-aws-cdk-for-aws-cloudformation/","section":"posts","tags":["Infrastructure as Code","AWS CloudFormation","AWS CDK","AWS"],"title":"Effective AWS CDK for AWS CloudFormation"},{"body":"ËøëÊúüAmazon Builders LibraryÂèëÂ∏É‰∫ÜÊï∞ÁØáÊñáÁ´†‰ªãÁªç‰∫öÈ©¨ÈÄäÂ¶Ç‰ΩïÂÆûË∑µÊåÅÁª≠ÈÉ®ÁΩ≤ÔºåÂêåÊó∂ÂàÜ‰∫´‰∫Ü‰∫öÈ©¨ÈÄäÂú®ÈÉ®ÁΩ≤ÊñπÈù¢ÁöÑÊúÄ‰Ω≥ÂÆûË∑µ„ÄÇ\nËøôÈáåÂ∞ÜËøô‰∏âÁØáÊñáÁ´†Ê†∏ÂøÉÂÜÖÂÆπÂÅö‰∏™Ê¶ÇËø∞ÔºåÊñπ‰æøÂ§ßÂÆ∂ÊåâÈúÄÁªÜËØª„ÄÇ\nGoing faster with continuous delivery ËøôÁØáÊñáÁ´†ÂÖàÊòØÂàÜ‰∫´‰∫Ü‰∫öÈ©¨ÈÄäÊåÅÁª≠ÊîπËøõÂíåËΩØ‰ª∂Ëá™Âä®ÂåñÁöÑÊñáÂåñ(AmazonianÈöèÊó∂ÈÉΩÊÉ¶ËÆ∞ÁùÄÁöÑÈ¢ÜÂØºÂäõÂáÜÂàô)ÔºåÁÑ∂Âêé‰ªãÁªç‰∫Ü‰∫öÈ©¨ÈÄäÂÜÖÈÉ®ÁöÑÊåÅÁª≠ÈÉ®ÁΩ≤Â∑•ÂÖ∑Pipelines„ÄÇ‰ªé‰∏Ä‰∏™ËØïÁÇπÂ∑•ÂÖ∑ËøõÂåñ‰∏∫‰∫öÈ©¨ÈÄäÊ†áÂáÜ„ÄÅ‰∏ÄËá¥‰∏îÁÆÄÊ¥ÅÁöÑÂèëÂ∏ÉÂ∑•ÂÖ∑„ÄÇÂπ∂‰∏îÂ∞ÜÊûÑÂª∫ÂíåÂèëÂ∏ÉËΩØ‰ª∂ÁöÑÊúÄ‰Ω≥ÂÆûË∑µÊ£ÄÊü•‰πüËûçÂÖ•Âà∞Pipelines‰∏≠„ÄÇ\nÊé•‰∏ãÊù•ÊòØÂàÜ‰∫´Â¶Ç‰ΩïÂáèÂ∞èÊïÖÈöúÂΩ±ÂìçÂà∞ÂÆ¢Êà∑ÁöÑÈ£éÈô©„ÄÇÊúâËøáËΩØ‰ª∂ÂºÄÂèëÁªèÈ™åÁöÑÈÉΩÁü•ÈÅìÔºåËΩØ‰ª∂ÂèòÊõ¥ÂºïÂÖ•ÊïÖÈöúÊòØ‰∏çÂèØÈÅøÂÖçÁöÑÔºåÂ¶Ç‰ΩïÂ∞ÜÊïÖÈöúÂØπÂÆ¢Êà∑ÁöÑÂΩ±ÂìçÊéßÂà∂Âà∞ÊúÄÂ∞èÊòØÈùûÂ∏∏ÈáçË¶ÅÁöÑ„ÄÇËØ•Êñá‰ªé‰∏ãÈù¢Âá†‰∏™ÊñπÈù¢ÁªôÂá∫‰∫ÜÂª∫ËÆÆÔºå\nÈÉ®ÁΩ≤Âç´ÁîüÔºåÂ¶ÇÂØπÊñ∞ÈÉ®ÁΩ≤Á®ãÂ∫èÁöÑÂÅ•Â∫∑Ê£ÄÊü• ‰∏äÁîü‰∫ßÁ≥ªÁªü‰πãÂâçÁöÑÊµãËØïÔºåËá™Âä®ÂåñÂçïÂÖÉ„ÄÅÈõÜÊàê„ÄÅÈ¢ÑÁîü‰∫ßÊµãËØï Áîü‰∫ßÁ≥ªÁªü‰∏äÁöÑÈ™åËØÅÔºåÂàÜÊâπÁöÑÈÉ®ÁΩ≤ÔºåÊéßÂà∂ÊïÖÈöúÂΩ±ÂìçÂçäÂæÑ ÊéßÂà∂‰ΩïÊó∂ÂèëÂ∏ÉËΩØ‰ª∂ ÊúÄÂêé‰ΩúËÄÖ‰ªãÁªç‰∫Ü‰∫öÈ©¨ÈÄäÂ¶Ç‰ΩïÂø´ÈÄüÊâßË°å‰∏öÂä°ÂàõÊñ∞ -- ÈÄöËøáËá™Âä®Âåñ‰∏ÄÂàá‰∫ãÊÉÖ„ÄÇ\nAutomating safe, hands-off deployments ËøôÁØáÊñáÁ´†ÂæàÂ•ΩÁöÑÂëºÂ∫î‰∫ÜGoing faster with continuous delivery‰∏ÄÊñá‰∏≠Â¶Ç‰ΩïÈÅøÂÖçÊñ∞ÁöÑÈÉ®ÁΩ≤ÂØºËá¥ÊïÖÈöúÂΩ±ÂìçÔºåÈùûÂ∏∏ËØ¶ÁªÜÁöÑ‰ªãÁªç‰∫Ü‰∫öÈ©¨ÈÄäÂÖ≥‰∫éËá™Âä®ÂåñÂÆâÂÖ®ÈÉ®ÁΩ≤ÁöÑÂÆûË∑µ„ÄÇ\nÂØπ‰∫éÊåÅÁª≠ÈÉ®ÁΩ≤ÔºåÊ∫êÁ†Å -\u0026gt; ÊûÑÂª∫ -\u0026gt; ÊµãËØï -\u0026gt; Áîü‰∫ß Ëøô‰∏™ÊµÅÁ®ãÂ§ßÂÆ∂ÈÉΩÂæàÁÜüÊÇâ„ÄÇ\n‰ªé‰∏ãÂõæÁúãÔºå‰∫öÈ©¨ÈÄäÂØπ‰∫éÊ∫êÁ†ÅÂíåÊûÑÂª∫ÁöÑÁêÜËß£ÊòØÈùûÂ∏∏Ê∑±ÂÖ•ÂíåÂÖ®Èù¢ÁöÑ„ÄÇ\nÊ∫êÁ†ÅÂπ∂‰∏ç‰ªÖ‰ªÖÊòØÂ∫îÁî®Á®ãÂ∫èÊ∫ê‰ª£Á†ÅÔºåËøòÂèØ‰ª•ÂåÖÊã¨ËøêÁª¥Â∑•ÂÖ∑‰ª£Á†Å„ÄÅÊµãËØï‰ª£Á†Å„ÄÅÂü∫Á°ÄÊû∂ÊûÑ‰ª£Á†Å„ÄÅÈùôÊÄÅËµÑÊ∫ê„ÄÅ‰æùËµñÂ∫ì„ÄÅÈÖçÁΩÆÂíåÊìç‰ΩúÁ≥ªÁªüË°•‰∏Å„ÄÇ\n‰ª£Á†ÅÂÆ°Ê†∏ÊòØÂøÖÈ°ªÁöÑ„ÄÇÂØπ‰∫éÂÖ®Ëá™Âä®ÁöÑÊµÅÊ∞¥Á∫øÔºå‰ª£Á†ÅÂÆ°Ê†∏ÊòØÊúÄÂêé‰∏ÄÈÅì‰∫∫Â∑•Ê†∏È™å„ÄÇ‰ª£Á†ÅÂÆ°Ê†∏‰∏ç‰ªÖ‰ªÖÊòØÂÆ°Ê†∏‰ª£Á†ÅÁöÑÊ≠£Á°ÆÊÄßÔºåËøòÂ∫îËØ•Ê£ÄÊü•‰ª£Á†ÅÊòØÂê¶ÂåÖÊã¨Ë∂≥Â§üÁöÑÊµãËØïÔºåÊòØÂê¶ÊúâÂÆåÂñÑÁöÑÂ∑•ÂÖ∑Êù•ÁõëÊµãÈÉ®ÁΩ≤‰ª•ÂèäËÉΩÂê¶ÂÆâÂÖ®ÁöÑÂõûÈÄÄ„ÄÇ\nÂêåÊó∂ÊûÑÂª∫‰πü‰∏çÂÖâÊòØÁºñËØëÊ∫ê‰ª£Á†ÅÔºåÊâìÂåÖÂπ∂Â≠òÂÇ®ÊûÑ‰ª∂„ÄÇ‰πüÂåÖÂê´ÂçïÂÖÉÊµãËØïÔºåÈùôÊÄÅ‰ª£Á†ÅÂàÜÊûêÔºå‰ª£Á†ÅË¶ÜÁõñÁéáÊ£ÄÊü•Ôºå‰ª£Á†ÅÂÆ°Ê†∏Ê£ÄÊü•„ÄÇ\nÊµãËØïÂú®‰∫öÈ©¨ÈÄäÊòØ‰∏Ä‰∏™Â§öÈò∂ÊÆµÁöÑÈ¢ÑÁîü‰∫ßÁéØÂ¢ÉÔºåËØ¶ËßÅ‰∏ãÂõæ„ÄÇ\nÈõÜÊàêÊµãËØïÊòØËá™Âä®ÂåñÁöÑÊ®°ÊãüÂÆ¢Êà∑‰∏ÄÊ†∑‰ΩøÁî®ÊúçÂä°ÔºåÂÆûÁé∞Á´ØÂà∞Á´ØÁöÑÊµãËØï„ÄÇÈÉ®ÁΩ≤Âà∞Áîü‰∫ß‰πãÂâçÔºåËøòÈúÄË¶ÅÊâßË°åÂêëÂêéÂÖºÂÆπÊÄßÊµãËØï‰ª•ÂèäÂÄüÂä©Ë¥üËΩΩÂùáË°°ÂÆûÁé∞one-boxÊµãËØï„ÄÇ\nAWSÊúçÂä°ÊòØÈÉ®ÁΩ≤Âú®ÂÖ®ÁêÉÂ§ö‰∏™Âå∫ÂüüÂÜÖÁöÑÂ§ö‰∏™ÂèØÁî®Âå∫Ôºå‰∏∫‰∫ÜÂáèÂ∞ëÈÉ®ÁΩ≤ÊïÖÈöúÂØπÂÆ¢Êà∑ÁöÑÂΩ±ÂìçÔºåÁîü‰∫ßÈÄöËøáÊ≥¢Ê¨°ÈÉ®ÁΩ≤Êù•ÂàÜÊâπÂàÜÈò∂ÊÆµÁöÑÂÆâÂÖ®ÈÉ®ÁΩ≤„ÄÇ\nÈ¶ñÂÖàÈÉ®ÁΩ≤ÊòØÂú®ÂçïÂå∫ÂüüÁöÑÂçïÂèØÁî®Âå∫ÂÅöone-boxÈÉ®ÁΩ≤ÔºåÂ¶ÇÊûúÂºïËµ∑Ë¥üÈù¢ÈóÆÈ¢òÔºå‰ºöËá™Âä®ÂõûÈÄÄÂπ∂ÂÅúÊ≠¢Áîü‰∫ßÂêéÁª≠ÁöÑÈÉ®ÁΩ≤„ÄÇÁ≥ªÁªüÊåáÊ†áÁöÑÁõëÊéßÊòØÂÆûÁé∞Ëá™Âä®ÂåñÂÆâÂÖ®ÈÉ®ÁΩ≤ÁöÑÊ†∏ÂøÉÔºåÈúÄË¶ÅÈÄöËøáÁõëÊéßÁöÑÊåáÊ†áÊù•Ëá™Âä®Ëß¶ÂèëÈÉ®ÁΩ≤ÂõûÈÄÄ„ÄÇ\nBake time‰πüÊòØÂÆûË∑µÁªèÈ™åÊÄªÁªìÂá∫Êù•ÁöÑÁ≤æÈ´ì„ÄÇÊúâÊó∂ÊïÖÈöú‰∏çÊòØÂú®ÈÉ®ÁΩ≤ÂêéÈ©¨‰∏äÊòæÁé∞ÁöÑÔºåÈúÄË¶ÅÊó∂Èó¥Êâç‰ºöÈÄêÊ∏êÊòæÁé∞„ÄÇËÆæÁΩÆÂêàÁêÜÁöÑBake timeÔºåËÉΩÂ§üËÆ©ÊïÖÈöúÊúâË∂≥Â§üÊó∂Èó¥Ë¢´Êö¥Èú≤Âá∫Êù•Ôºå‰∏çËá≥‰∫éÁÖßÊàêÂ§ßËåÉÂõ¥ÂΩ±Âìç„ÄÇ\nEnsuring rollback safety during deployments Âõ†‰∏∫ÊïÖÈöúÊòØ‰∏çÂèØÈÅøÂÖçÁöÑÔºåÈÉ®ÁΩ≤ËÉΩÂ§üË¢´ÂÆâÂÖ®ÂõûÈÄÄÊòØÈùûÂ∏∏ÂøÖË¶ÅÁöÑ„ÄÇËøôÁØáÊñáÁ´†Â∞±ËØ¶ÁªÜ‰ªãÁªç‰∫ÜÂ¶Ç‰ΩïÂÆûÁé∞ÂèØÂÆâÂÖ®ÂõûÈÄÄÁöÑÈÉ®ÁΩ≤ -- ÈÄöËøá‰∏§Èò∂ÊÆµÈÉ®ÁΩ≤ÁöÑÊäÄÊúØÔºå‰ª•ÂèäÂ∫èÂàóÂåñÁöÑÊúÄ‰Ω≥ÂÆûË∑µ„ÄÇ\nËøô‰∏âÁØáÊñáÁ´†ÂàÜÂà´‰ªéÊúØÂíåÂô®ÁöÑËßíÂ∫¶ÂàÜ‰∫´‰∫Ü‰∫öÈ©¨ÈÄäÂú®ËΩØ‰ª∂ÈÉ®ÁΩ≤ÁöÑÂÆûË∑µÁªèÈ™åÔºåÂºÄÂèëËÄÖ‰ª¨ÂèØ‰ª•ÁªìÂêàËá™Ë∫´‰∏öÂä°ÊÉÖÂÜµÈõÜÊàêÈÄÇÂêàÁöÑÊúÄ‰Ω≥ÂÆûË∑µ„ÄÇ\n","link":"https://kane.mx/posts/2020/the-best-practise-of-deployment-at-amazon/","section":"posts","tags":["DevOps","Continuous Deployment","Amazon Builders' Library","System Design"],"title":"‰∫öÈ©¨ÈÄäÁöÑÈÉ®ÁΩ≤ÊúÄ‰Ω≥ÂÆûË∑µ"},{"body":"","link":"https://kane.mx/tags/aws-step-functions/","section":"tags","tags":null,"title":"AWS Step Functions"},{"body":"AWS CDKÊòØÁºñÊéíÈÉ®ÁΩ≤AWS‰∫ë‰∏äËµÑÊ∫êÊúÄ‰Ω≥ÁöÑÂ∑•ÂÖ∑‰πã‰∏Ä„ÄÇÂü∫‰∫éAWS CDKÁöÑÂ∫îÁî®Â∫îËØ•Â¶Ç‰ΩïÂÆûË∑µDevOpsÊåÅÁª≠ÈõÜÊàêÂíåÈÉ®ÁΩ≤Âë¢Ôºü\nÈÄöÂ∏∏Êàë‰ª¨Êúâ‰∏ãÈù¢Âá†ÁßçÊñπÊ≥ïÔºå\n‰ΩøÁî®AWS CodePipelineÊù•ÂÆåÊàêDevOps pipelineÊê≠Âª∫„ÄÇCodePipelineÊòØAWS CodeÁ≥ªÂàóÊúçÂä°‰∏≠ÁöÑÊåÅÁª≠ÈõÜÊàêÁºñÊéíÂ∑•ÂÖ∑ÔºåÂÆÉÂèØ‰ª•ÈõÜÊàêCodeBuildÈ°πÁõÆÔºåÂú®CodeBuildÈ°πÁõÆbuild‰∏≠ÂÆâË£ÖcdkÔºåÂπ∂ÊâßË°åcdk deployÂëΩ‰ª§Êù•ÂÆûÁé∞Â∫îÁî®ÈÉ®ÁΩ≤„ÄÇ ËøôÁßçÊñπÊ≥ïÁÆÄÂçïÁõ¥Êé•ÁöÑÂÆûÁé∞‰∫ÜDevOpsÈÉ®ÁΩ≤ÊµÅÊ∞¥Á∫ø„ÄÇ‰ΩÜÁº∫Â∞ëstagingÔºåÂ∞ÜÊúÄÊñ∞Êèê‰∫§Áõ¥Êé•ÈÉ®ÁΩ≤Âà∞Áîü‰∫ßÊòØ‰∏ÄÁßçÈùûÂ∏∏È´òÈ£éÈô©ÁöÑÂÅöÊ≥ï„ÄÇ\nCDKËøëÊúüÂèëÂ∏É‰∫Ü‰ΩìÈ™åÊÄßÁöÑÊñ∞ÁâπÊÄßCDK PipelinesÊù•Â∞ÅË£ÖCDKÂ∫îÁî®ÊåÅÁª≠ÈÉ®ÁΩ≤ÊµÅÊ∞¥Á∫øÁöÑÈÖçÁΩÆ„ÄÇCDK Pipelines‰πüÊòØÂü∫‰∫éAWS CodePipelineÊúçÂä°ÔºåÊèê‰æõÂø´ÈÄüÂàõÂª∫ÂèØË∑®Ë¥¶Âè∑Âå∫ÂüüÁöÑÊåÅÁª≠ÈÉ®ÁΩ≤ÊµÅÊ∞¥Á∫øÔºåÂêåÊó∂ÊîØÊåÅÈÉ®ÁΩ≤ÊµÅÊ∞¥Á∫øÈ°πÁõÆÁöÑËá™ÂçáÁ∫ßÊõ¥Êñ∞„ÄÇÊï¥‰∏™ÊµÅÊ∞¥Á∫øÊµÅÁ®ãÂ¶Ç‰∏ãÂõæÊâÄÁ§∫Ôºå CDK PipelinesÊòØÈùûÂ∏∏È´òÊïà‰∏îÁÅµÊ¥ªÁöÑÊåÅÁª≠ÈÉ®ÁΩ≤ÊµÅÊ∞¥Á∫øÂàõÂª∫ÁöÑÊñπÂºèÔºå‰ΩÜÁî±‰∫éÊòØ‰ΩìÈ™åÊÄßÁâπÊÄßÔºåÂú®Áîü‰∫ßÂ∫îÁî®‰∏≠ËøòÊúâ‰∏Ä‰∫õÂ±ÄÈôêÊÄß„ÄÇ‰æãÂ¶ÇÔºå\n‰∏çÊîØÊåÅcontext providerÊü•Êâæ„ÄÇ‰πüÂ∞±ÊòØËØ¥ÔºåÊó†Ê≥ïÊîØÊåÅCDKÂ∫îÁî®Êü•ÊâæË¥¶Êà∑‰∏≠Â≠òÂú®ÁöÑVPCÔºåR53 HostedZoneÁ≠â„ÄÇ Áî±‰∫éCDK PipelinesÂÆûÈôÖÊòØ‰ΩøÁî®CodePipelineÊù•ÁºñÊéíÈÉ®ÁΩ≤ÊµÅÊ∞¥Á∫øÔºåCodePipelineÁöÑÂ±ÄÈôêÊÄßÔºåCDK PipelinesÂêåÊ†∑Â≠òÂú®„ÄÇ CodePipelineÂú®Êüê‰∫õÂàÜÂå∫ÂíåÂå∫ÂüüËøò‰∏çÂèØÁî®„ÄÇ‰æãÂ¶ÇÔºåAWS‰∏≠ÂõΩÂå∫ÊöÇÊó∂ËøòÊ≤°ÊúâCodePipelineÊúçÂä°ÔºåCDK PipelinesÂú®AWS‰∏≠ÂõΩÂå∫‰πüÂ∞±Êó†Ê≥ï‰ΩøÁî®„ÄÇ ‰ΩøÁî®AWS Step FunctionsÊù•ÁºñÊéíCDKÂ∫îÁî®ÈÉ®ÁΩ≤ÁöÑÊµÅÊ∞¥Á∫ø„ÄÇÂú®Step FunctionsÁºñËØëÁöÑÈÉ®ÁΩ≤ÊµÅÊ∞¥Á∫ø‰∏≠ÔºåÂèØÁî®ÈÄöËøáCodeBuildÈ°πÁõÆÊù•ÂÆåÊàêcdk deployÊâßË°åÂÅöÂà∞ÂÆåÊï¥ÁöÑÊîØÊåÅCDKÁöÑÊâÄÊúâÂäüËÉΩ„ÄÇÂêåÊó∂Step FunctionsÂÖ∑Â§áÊúÄÂ§ßÁöÑÁÅµÊ¥ªÊÄßÊù•ÊîØÊåÅÊåÅÁª≠ÈÉ®ÁΩ≤ËøáÁ®ã‰∏≠ÁöÑÂêÑÁßçÁºñÊéíÈúÄÊ±ÇÔºå‰æãÂ¶ÇÔºåË∑®Ë¥¶Êà∑ÈÉ®ÁΩ≤Â∫îÁî®ÁöÑ‰∏çÂêåstageÔºåÂºïÂÖ•‰∫∫Â∑•ÂÆ°ÊâπÊµÅÁ®ãÔºåÈÄöËøáSlackÁ≠âchatopsÂ∑•ÂÖ∑Êù•ÂÆåÊàêÂÆ°Êâπ„ÄÇ OpentunaÈ°πÁõÆÂ∞±ÂÆûË∑µ‰∫ÜÁî®Step FunctionsÊù•ÁºñÊéíÊåÅÁª≠ÈÉ®ÁΩ≤ÊµÅÊ∞¥Á∫ø„ÄÇÊï¥‰∏™ÈÉ®ÁΩ≤ÊµÅÁ®ãÂ¶Ç‰∏ãÂõæÔºå\nÂ¶ÇÊûúÂØπÂü∫‰∫éStep FunctionsÂÆûÁé∞ÁöÑCDKÂ∫îÁî®ÊåÅÁª≠ÈÉ®ÁΩ≤ÊÑüÂÖ¥Ë∂£ÔºåÂèØ‰ª•ËÆøÈóÆOpenTUNAÈ°πÁõÆÂÆûÁé∞ÁöÑÊ∫êÁ†Å‰∫ÜËß£Êõ¥Â§öÁªÜËäÇ„ÄÇ\n","link":"https://kane.mx/posts/2020/deploy-aws-cdk-applications-cross-accounts/","section":"posts","tags":["AWS","AWS CDK","DevOps","AWS Step Functions"],"title":"Ë∑®Ë¥¶Âè∑Ë∑®Âå∫ÂüüÈÉ®ÁΩ≤AWS CDKÁºñÊéíÁöÑÂ∫îÁî®"},{"body":"Sonatype Nexus repository OSS is an artifact repository that supports most software repositories such as Maven, Pypi, Npmjs, Rubygems, Yum, Apt, Docker registry and etc. In the enterprise Nexus repository is widely used for storing proprietary artifacts and caching the artifacts for speeding up the devops.\nBuilding a production ready Nexus repository always is a requirement for devops team, it should satisfy below criterias at least,\nartifacts storage management It's difficult to predicate the storage usage of artifacts, allocating large volume is not cost optimized. the durability of nexus3 data storage We need a way to make sure data storage of nexus when updating Nexus OSS to newer version or recover the service from unhealthy status. self healing capability when the service is down A reliable way recovers the Nexus repository OSS when it's unhealth. There is a well-architected solution(maintained by AWS team) to quickly(~10 minutes) deploy Nexus OSS leveraging below capabilities,\nHost on EKS cluster using managed EC2 nodes with IRSA Expose service via AWS Application load balancer managed by AWS load balancer controller(former ALB Ingress Controller) Use dedicated S3 bucket for storing Nexus OSS blobstore with ulimited and on-demand storage Use EFS, EFS CSI Driver, PV and PVC storing nexus data Use Helm to deploy Sonatype Nexus chart Optional Use External DNS to registry the domain record of Nexus repository to Route 53 Optional Use AWS Certificate Manager to create SSL certificate of domain name of Nexus repository Enjoy it\u0026#x1f60f;\n","link":"https://kane.mx/posts/2020/deploy-sonatype-nexus-oss-on-eks/","section":"posts","tags":["Amazon EKS","Kubernetes","Helm","AWS CDK","AWS","Sonatype Nexus"],"title":"Deploy Sonatype Nexus repository OSS on EKS"},{"body":"","link":"https://kane.mx/tags/aws-athena/","section":"tags","tags":null,"title":"AWS Athena"},{"body":"","link":"https://kane.mx/tags/big-data/","section":"tags","tags":null,"title":"Big Data"},{"body":"","link":"https://kane.mx/tags/cloud-native/","section":"tags","tags":null,"title":"Cloud Native"},{"body":"","link":"https://kane.mx/tags/data-lakes/","section":"tags","tags":null,"title":"Data Lakes"},{"body":"","link":"https://kane.mx/tags/docker/","section":"tags","tags":null,"title":"Docker"},{"body":"","link":"https://kane.mx/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/","section":"tags","tags":null,"title":"‰∫ëËÆ°ÁÆó"},{"body":"ËøëÊúüÂØπDockerÈïúÂÉèÂÅö‰∫Ü‰∫õÊï∞ÊçÆÂàÜÊûêÔºåËøôÈáåÂàÜ‰∫´‰∏Ä‰∏ãÂà©Áî®‰∫ëÂéüÁîüÊäÄÊúØÂø´ÈÄü‰∏î‰ΩéÊàêÊú¨ÁöÑÂÆûÁé∞‰ªªÊÑèÊï∞ÈáèÁöÑÊï∞ÊçÆÂàÜÊûê„ÄÇ\n‰πãÂâçÈÄöËøáÊñáÁ´†‰ªãÁªç‰∫Ü‰∏çÁî®ÊãâÂèñDockerÈïúÂÉèÂ∞±ÂèØËé∑ÂèñÈïúÂÉèÁöÑÂ§ßÂ∞èÁöÑ‰∏ÄÁßçÊñπÊ≥ïÔºåÈÄöËøáÂÖ∂‰∏≠ÁöÑÁ§∫‰æãËÑöÊú¨ÔºåÊàë‰ª¨ÂèØ‰ª•Ëé∑ÂèñÂà∞ÂæÖÂàÜÊûêÁöÑÂéüÂßãÊï∞ÊçÆ„ÄÇ\nÊØîÂ¶ÇnginxÈïúÂÉèÁöÑÈÉ®ÂàÜÂéüÂßãÊï∞ÊçÆ(csvÊ†ºÂºè)Â¶Ç‰∏ãÔºå\n1 2 3 4 5 6 7 8 9 10 11 12 1.18.0-alpine,sha256:676b8117782d9e8c20af8e1b19356f64acc76c981f3a65c66e33a9874877892a,amd64,linux,null,null,\u0026#34;sha256:cbdbe7a5bc2a134ca8ec91be58565ec07d037386d1f1d8385412d224deafca08\u0026#34;,2813316 1.18.0-alpine,sha256:676b8117782d9e8c20af8e1b19356f64acc76c981f3a65c66e33a9874877892a,amd64,linux,null,null,\u0026#34;sha256:6ade829cd166df9b2331da48e3e60342aef9f95e1e45cde8d20e6b01be7e6d9a\u0026#34;,6477096 1.18.0-alpine,sha256:70feed62d5204358ed500463c0953dce6c269a0ebeef147a107422a2c78799a9,arm,linux,v6,null,\u0026#34;sha256:b9e3228833e92f0688e0f87234e75965e62e47cfbb9ca8cc5fa19c2e7cd13f80\u0026#34;,2619936 1.18.0-alpine,sha256:70feed62d5204358ed500463c0953dce6c269a0ebeef147a107422a2c78799a9,arm,linux,v6,null,\u0026#34;sha256:a03f81873d278ad248976b107883f0452d33c6f907ebcdd832a6041f1d33118a\u0026#34;,6080562 1.18.0-alpine,sha256:2ba714ccbdc4c2a7b5a5673ebbc8f28e159cf2687a664d540dcb91d325934f32,arm64,linux,v8,null,\u0026#34;sha256:29e5d40040c18c692ed73df24511071725b74956ca1a61fe6056a651d86a13bd\u0026#34;,2724424 1.18.0-alpine,sha256:2ba714ccbdc4c2a7b5a5673ebbc8f28e159cf2687a664d540dcb91d325934f32,arm64,linux,v8,null,\u0026#34;sha256:806787fcd4f9e2f814506fb53e81b6fb33f9eea04e5b537b31fa5fb601a497ee\u0026#34;,6423816 1.18.0-alpine,sha256:6d6f19360150548bbb568ecd3e1affabbdce0fcc39156e70fbae8a0aa656541a,386,linux,null,null,\u0026#34;sha256:2826c1e79865da7e0da0a993a2a38db61c3911e05b5df617439a86d4deac90fb\u0026#34;,2808418 1.18.0-alpine,sha256:6d6f19360150548bbb568ecd3e1affabbdce0fcc39156e70fbae8a0aa656541a,386,linux,null,null,\u0026#34;sha256:f2ab0e3b0ff04d1695df322540631708c42b0a68925788de2290c9497e44fef3\u0026#34;,6845295 1.18.0-alpine,sha256:c0684c6ee14c7383e4ef1d458edf3535cd62b432eeba6b03ddf0d880633207da,ppc64le,linux,null,null,\u0026#34;sha256:9a8fdc5b698322331ee7eba7dd6f66f3a4e956554db22dd1e834d519415b4f8e\u0026#34;,2821843 1.18.0-alpine,sha256:c0684c6ee14c7383e4ef1d458edf3535cd62b432eeba6b03ddf0d880633207da,ppc64le,linux,null,null,\u0026#34;sha256:30a37aac8b54a38e14e378f5122186373cf233951783587517243e342728a828\u0026#34;,6746511 1.18.0-alpine,sha256:714439fec7e1f55c29b57552213e45c96bbfeefddea2b3b30d7568591966c914,s390x,linux,null,null,\u0026#34;sha256:7184c046fdf17da4c16ca482e5ede36e1f2d41ac8cea9c036e488fd149d6e8e7\u0026#34;,2582859 1.18.0-alpine,sha256:714439fec7e1f55c29b57552213e45c96bbfeefddea2b3b30d7568591966c914,s390x,linux,null,null,\u0026#34;sha256:214dff8a034aad01facf6cf63613ed78e9d23d9a6345f1dee2ad871d6f94b689\u0026#34;,6569410 ÂêÑÂàóÁöÑÂê´‰πâÂàÜÂà´ÊòØÔºåÈïúÂÉètag, ÈïúÂÉèDigest, ÈïúÂÉèÂØπÂ∫îÂπ≥Âè∞ÁöÑArchitecture, ÈïúÂÉèÂØπÂ∫îÂπ≥Âè∞ÁöÑOS, ÈïúÂÉèÂØπÂ∫îÂπ≥Âè∞ÁöÑÂèòÁßçÔºà‰æãÂ¶ÇÔºåARMÁöÑv7, v8Á≠âÔºâ, ÈïúÂÉèÂØπÂ∫îÂπ≥Âè∞ÁöÑOSÁâàÊú¨, ÈïúÂÉèÁªÑÊàêÂ±ÇÁöÑDigest, ÈïúÂÉèÁªÑÊàêÂ±ÇÁöÑÂ§ßÂ∞è„ÄÇ\n‰∏äÈù¢nginxÈïúÂÉèÁöÑÁ§∫‰æãÊï∞ÊçÆÔºåÂëäËØâÊàë‰ª¨ÈïúÂÉèÂêçnginx‰∏îtag‰∏∫1.18.0-alpineÁöÑÈïúÂÉèÂåÖÂê´‰∫Üamd64-linux, arm-linux-v6, arm64-linux-v8, 386-linux, ppc64le-linux‰ª•Âèäs390x-linuxÂÖ±5ÁßçArchÂêàËÆ°6‰∏™ÁâàÊú¨ÁöÑÈïúÂÉè„ÄÇ‰∏îÊØè‰∏™Âπ≥Âè∞ÁöÑÂØπÂ∫îÈïúÂÉèÂåÖÂê´‰∫Ü‰∏§‰∏™Â±Ç‰ª•ÂèäËøô‰∏§‰∏™Â±ÇÁöÑÂ§ßÂ∞è„ÄÇ\nÂΩìÊàë‰ª¨Êúâ‰∫ÜÊàêÁôæÊï∞ÂçÉÁîöËá≥Êµ∑ÈáèÈïúÂÉèÁöÑÂéüÂßãÊï∞ÊçÆÂêéÔºåÂ¶Ç‰ΩïËÉΩÂø´ÈÄü‰∏î‰ΩéÊàêÊú¨ÁöÑÂàÜÊûêËøô‰∫õÊï∞ÊçÆÂë¢Ôºü\nÂú®AWS‰∏äÔºåÊàë‰ª¨ÂèØ‰ª•Âà©Áî®Êï∞ÊçÆÊπñÁõ∏ÂÖ≥ÁöÑÁ≥ªÂàó‰∫ßÂìÅÊù•ÂÆûÁé∞‰ΩéÊàêÊú¨ÁöÑ‰∫§‰∫íÂºèÂàÜÊûê„ÄÇ\nÂú®DockerÈïúÂÉèÂàÜÊûêËøô‰∏™Âú∫ÊôØ‰∏ãÔºåÊàëÂ∑≤ÁªèËé∑ÂèñÂà∞‰∫ÜÂæÖÂàÜÊûêÈïúÂÉèÁöÑÂπ≥Âè∞„ÄÅÂ±ÇÁ≠âÊï∞ÊçÆ„ÄÇÊàëÂ∞ÜËøô‰∫õÊï∞ÊçÆ‰∏ä‰º†Âà∞Amazon S3‰Ωú‰∏∫Êï∞ÊçÆÊπñÁöÑÊï∞ÊçÆÊ∫ê„ÄÇ Êé•‰∏ãÊù•‰ΩøÁî®AWS Glue‰ª•S3‰∏≠ÁöÑÊï∞ÊçÆÂàõÂª∫TableÂπ∂‰∏î‰ªé‰∏≠ÊèêÂâçÊï∞ÊçÆÁöÑmetadata„ÄÇÂêåÊó∂ÂÅöÊï∞ÊçÆÂàÜÂå∫Ôºå‰∏∫Êé•‰∏ãÊù•ÁöÑÊü•ËØ¢ÂÅöÊÄßËÉΩÂíåÊàêÊú¨‰ºòÂåñ„ÄÇ ÊâìÂºÄAmazon AthenaÔºåÊ†πÊçÆ‰∏öÂä°ÈúÄÊ±ÇÈÄöËøáSQLËØ≠Âè•Êü•ËØ¢ÂàÜÊûêDockerÈïúÂÉèÊï∞ÊçÆ„ÄÇ Â∞±ÊòØÈÄöËøá‰ª•‰∏ä3‰∏™ÁÆÄÂçïÊ≠•È™§ÔºåÊàëÂ∞±ÂæóÂà∞‰∫Ü‰∏Ä‰∏™Êó†ÊúçÂä°Âô®Êû∂ÊûÑÁöÑDockerÈïúÂÉèÊï∞ÊçÆÂàÜÊûêÂ∫îÁî®ÔºÅÊï¥‰∏™Â∫îÁî®ÂÆåÂÖ®ÊòØÊåâÈáèËÆ°Ë¥πÁöÑÔºå‰∏ªË¶ÅÊàêÊú¨ÂåÖÊã¨S3ÂØπË±°Â≠òÂÇ®Ë¥πÁî®ÔºåÂíåAthenaË¥πÁî®ÔºàÊ†πÊçÆÊØèÊ¨°Êü•ËØ¢Êâ´ÊèèÊï∞ÊçÆÁöÑÂ§ßÂ∞èÊù•ËÆ°ÁÆóÔºâ„ÄÇ\n‰ΩøÁî®ËØ•ÂàÜÊûêÂ∫îÁî®ÔºåÊàëÁªüËÆ°‰∫ÜDocker HubÂÆòÊñπÈïúÂÉè‰∏≠ÂåÖÂê´Â±ÇÊúÄÂ§öÁöÑ10‰∏™ÈïúÂÉè(ÂàÜÂπ≥Âè∞ÁªüËÆ°)Ôºå ÊúÄÂêéÔºåÂæóÂäõ‰∫éAWS Infra as CodeÁöÑÂº∫Â§ßËÉΩÂäõÔºåÊï¥‰∏™Â∫îÁî®‰πüÊòØÈÄöËøá‰ª£Á†ÅÁÆ°ÁêÜÁöÑ‰∏îÂºÄÊ∫êÁöÑÔºåÊúâÂÖ¥Ë∂£ÁöÑËØªËÄÖ‰πüÂèØ‰ª•ÈÉ®ÁΩ≤Ëá™Â∑±ÁöÑÂàÜÊûêÂ∫îÁî®„ÄÇ\n","link":"https://kane.mx/posts/2020/serverless-docker-images-analytics/","section":"posts","tags":["‰∫ëËÆ°ÁÆó","AWS","Big Data","Data Lakes","Analytics","AWS Athena","Cloud Native","docker"],"title":"Êó†ÊúçÂä°Âô®Êû∂ÊûÑÁöÑDockerÈïúÂÉèÊï∞ÊçÆÂàÜÊûêÂ∫îÁî®"},{"body":"Recently I had a requirement to stats the size of some Docker images. It would be waste if pulling them all firstly then calculating the size of each image. Also you know the docker image consists of some Docker layers that probably are shared by other images. It's hard to get the disk usage if only sum the size of each image.\nIs there any way to get the size of Docker image without pulling it?\nIt's definitely Yes. The docker images are hosted by Docker Registry, which is defined by a public specification. The latest V2 of Registry has API to fetch the manifest of an image that contains the size of every layer. Looks like it's very cool. Utilitying the manifest API of image will satisfie my requirement!\nOne more thing you should note, the v2 of Docker registry still is compatible with schema specification V1. You have to properly handle with the mixed responses of manifest when you query the manifest of an image.\nI created a simple shell script gracefully handling either v1 or v2 response of the image manifest, which can calculate the total layers size of a Docker image with specific tag, or the size of all tags of a Docker image.\nAbove script was inspired by this post. Hope you enjoy it.\n","link":"https://kane.mx/posts/2020/get-docker-image-size-without-pulling-image/","section":"posts","tags":["docker","Tip"],"title":"Get the size of Docker image without pulling image"},{"body":"","link":"https://kane.mx/tags/oh-my-zsh/","section":"tags","tags":null,"title":"Oh-My-Zsh"},{"body":"Z shellÊê≠ÈÖçoh-my-zshËá™ÂÆö‰πâÈÖçÁΩÆÂ∑≤Êàê‰∏∫‰ºóÂ§öLinux/MacosxÁî®Êà∑ÁöÑÊ†áÂáÜterminalÈÖçÁΩÆ„ÄÇ\nÊúÄËøëÈÅáÂà∞Âú®zsh‰∏≠ÊâßË°å‰ªªÊÑèÂëΩ‰ª§ÈÉΩÂèòÂæóÁâπÂà´ÊÖ¢(Âì™ÊÄïÁÆÄÂçïÊâßË°åls‰πüË¶ÅËä±Ë¥πËÇâÁúºÂèØËßÅÁöÑ1Ôºå2ÁßíÈíü)ÔºåËøôÈáåËÆ∞ÂΩï‰∏ãÂ¶Ç‰ΩïÊéíÊü•Z shell‰∏ãÂêØÁî®oh-my-zshÁöÑÊÄßËÉΩÈóÆÈ¢ò„ÄÇ\nÊÄßËÉΩÈóÆÈ¢òÁóáÁä∂ Á™ÅÁÑ∂ÊüêÂ§©Ëµ∑Âú®ÁªàÁ´Ø‰∏≠ÊâßË°å‰ªªÊÑèÂëΩ‰ª§ÔºåÈÉΩËá≥Â∞ëË¶ÅËä±Ë¥π1Ôºå2ÁßíÔºàËÇâÁúºËÆ°Êï∞ÔºâÔºåËØ•ÂëΩ‰ª§Êâç‰ºöÂÆåÊàêÊâßË°åÂπ∂ÈÄÄÂá∫Âà∞ÁªàÁ´ØÂºÄÂßãÊé•ÂèóÊñ∞ÁöÑËæìÂÖ•„ÄÇ\nÊàëÂΩìÂâç‰∏ªË¶Å‰ΩøÁî®ÁöÑÁªàÁ´ØÊòØiTerm2ÔºåÊâßË°åÂëΩ‰ª§ÂêéÔºåÂú®ÁªàÁ´ØTabÁöÑtitle bar‰∏äËÉΩÊòæÂºèÁöÑÁúãÂà∞gitÂëΩ‰ª§‰πüË¢´ÊâßË°å‰∫Ü„ÄÇ\nÂ∞ùËØï‰∫ÜÂÖ∂‰ªñÁöÑshellÔºåÊØîÂ¶ÇbashÔºåÊòØÊ≤°ÊúâËøô‰∏™ÈóÆÈ¢ò„ÄÇÂü∫Êú¨Êñ≠ÂÆöÈóÆÈ¢òÂêåzshÁõ∏ÂÖ≥„ÄÇÊõ¥Â§öÈóÆÈ¢òÊèèËø∞‰πÉËá≥Âä®ÁîªÊà™Â±èÔºåÂèØ‰ª•ÂèÇËßÅËøô‰∏™issue„ÄÇ\nzsh + oh-my-zsh ÊÄßËÉΩÈóÆÈ¢òÂàÜÊûê oh-my-zshÂÖ∂ÂÆûÂ∞±ÊòØÊèê‰æõzshÁöÑÂÆöÂà∂ÂåñÈÖçÁΩÆÔºå‰∏ªË¶ÅÂåÖÊã¨Theme‰∏ªÈ¢òÂíåÂêÑÁßçËΩØ‰ª∂ÁöÑÊèí‰ª∂„ÄÇ\noh-my-zsh Êèí‰ª∂ ÈÄöÂ∏∏oh-my-zsh‰∏≠ÂÜÖÁΩÆÊàñ‰∏âÊñπÁ§æÂå∫Êèê‰æõÁöÑÊèí‰ª∂ÊòØÂØºËá¥ÊÄßËÉΩÈôç‰ΩéÁîöËá≥‰∫íÁõ∏ÂÜ≤Á™ÅÁöÑ‰∏ªË¶ÅÂéüÂõ†„ÄÇÊéíÊü•ÊÄùË∑Ø‰πüÂæàÁÆÄÂçïÔºåÈÄöËøáÈÄê‰∏™Á¶ÅÁî®Â∑≤Âä†ËΩΩÁöÑÊèí‰ª∂Êù•ÊµãËØïÊòØÂê¶ÂèØ‰ª•Ëß£ÂÜ≥ÈóÆÈ¢ò„ÄÇ\nÁî®ÊñáÊú¨ÁºñËæëÂô®ÊâìÂºÄÂΩìÂâçÁî®Êà∑ÁöÑ~/.zshrcÈÖçÁΩÆÔºåÊâæÂà∞pluginsÂºÄÂ§¥ÁöÑÈÖçÁΩÆË°åÔºå‰æãÂ¶ÇÔºå\n1plugins=( 2 git 3 osx 4## gradle 5 brew 6## command-not-found 7 github 8# gnu-utils 9## mvn 10 python 11 pip 12# screen 13 vi-mode 14 docker 15## docker-compose 16 node 17## spring 18 mosh 19# httpie 20## sudo 21 tmux 22## kubectl 23## helm 24 golang 25 history 26 history-substring-search 27 zsh-autosuggestions 28 zsh-syntax-highlighting 29) ÈÄöËøáË°åÈ¶ñÊ∑ªÂä†#Êù•Á¶ÅÁî®oh-my-zshÊèí‰ª∂ÔºåÂêØÂä®Êñ∞ÁöÑÁªàÁ´ØÁ™óÂè£ÊàñtabÊù•È™åËØÅÊòØÂê¶ËØ•Êèí‰ª∂ÊòØÂºïËµ∑ÈóÆÈ¢òÁöÑÊ†πÊ∫ê„ÄÇ\nÂú®ÊàëÁöÑÈÖçÁΩÆ‰∏≠ÔºåÂá∫Áé∞ËøáÂõ†‰∏∫ÂêØÁî®ËøáÂ§öÊèí‰ª∂ÔºåÂØºËá¥Êñ∞Âª∫ÁªàÁ´ØÈúÄË¶Å10Êù•ÁßíÈíü„ÄÇ‰ΩÜÂõ†‰∏∫ÂàõÂª∫ÁªàÁ´Ø‰∏çÊòØ‰∏Ä‰∏™È´òÈ¢ëÁöÑÈúÄÊ±ÇÔºåËøô‰∏™ÊÄßËÉΩÈÄöÂ∏∏Êù•ËØ¥ËøòÊòØÂèØ‰ª•ÂøçÂèó„ÄÇ\noh-my-zsh ‰∏ªÈ¢ò Âú®ÊàëÁöÑËøô‰∏™ÈóÆÈ¢ò‰∏≠ÔºåÂç≥‰ΩøÂ∞ÜÊâÄÊúâÊèí‰ª∂ÈÉΩÁ¶ÅÁî®‰∫ÜÔºåÂëΩ‰ª§ÊâßË°åÂêéÈÄÄÂá∫ÈÄüÂ∫¶ËøòÊòØÊ≤°ÊúâÊîπÂñÑÔºågitÂëΩ‰ª§‰ªçÁÑ∂ÊúâË¢´ÊâßË°å„ÄÇËøôÊó∂ÊàëÂ∞ùËØïÊõ¥Êç¢‰∏çÂêåoh-my-zshÂÜÖÁΩÆ‰∏ªÈ¢òÊù•ÊéíÊü•ÈóÆÈ¢ò„ÄÇ‰ΩÜÊòØ‰ΩøÁî®‰∫ÜÂåÖÊã¨ÈªòËÆ§‰∏ªÈ¢òrobbyrussellÔºåÊûÅÁÆÄ‰∏ªÈ¢òysÂú®ÂÜÖÁöÑÂ§ö‰∏™‰∏ªÈ¢òÈÉΩÊó†Ê≥ïËß£ÂÜ≥ËØ•ÈóÆÈ¢ò„ÄÇ\nÊúÄÂêéÁõ¥Êé•Á¶ÅÊ≠¢oh-my-zsh‰ΩøÁî®‰∏ªÈ¢òÔºåÈóÆÈ¢òÊ≤°Êúâ‰∫ÜÔºÅ\nÁÑ∂ËÄåoh-my-zsh‰∏ªÈ¢òÊòØÂØπzshÁöÑÊûÅÂ§ßÂ¢ûÂº∫ÔºåÊîπÂñÑ‰∫ÜÈªòËÆ§ÁöÑÁî®Êà∑‰ΩìÈ™åÔºåÊ≤°Êúâ‰∏ªÈ¢òÊâ©Â±ï‰ΩøÁî®Ëµ∑Êù•‰ºöÈùûÂ∏∏‰∏ç‰π†ÊÉØ„ÄÇ\nÂ∞èÁªì ÊúÄÁªàËØïÁî®‰∫ÜÂè¶‰∏Ä‰∏™Á§æÂå∫Áª¥Êä§ÁöÑÁü•Âêçzsh‰∏ªÈ¢òPureÔºåÊÄßËÉΩÈóÆÈ¢òÂæóÂà∞‰∫ÜËß£ÂÜ≥\u0026#x270c;\u0026#xfe0f; ÂêåÊó∂‰πüÊª°Ë∂≥‰∫Ü‰∏ªÈ¢òÂØπzshËæìÂÖ•ËæìÂá∫Áî®Êà∑‰ΩìÈ™åÁöÑÂ¢ûÂº∫ \u0026#x1f60a;\nÂ∏åÊúõËøôÈáåÂàÜ‰∫´ÁöÑoh-my-zshÊÄßËÉΩÁöÑË∞É‰ºòÊÄùË∑ØÔºåÂèØ‰ª•Â∏ÆÂä©Âà∞ÊúâÁ±ª‰ººÈúÄË¶ÅÁöÑÂêÑ‰Ωç„ÄÇ\nÂ∞ÜÊù•Á§æÂå∫ÂØπËøô‰∏™ÈóÆÈ¢òÂ¶ÇÊúâËøõ‰∏ÄÊ≠•ÁöÑÂèçÈ¶àÔºåÂ∞Ü‰ºöÂÅöÊõ¥Êñ∞„ÄÇ\n","link":"https://kane.mx/posts/2020/zsh-performance-tuning/","section":"posts","tags":["zsh","oh-my-zsh","performance-tuning","trobule-shooting"],"title":"oh-my-zshÊÄßËÉΩË∞É‰ºòÊÄùË∑Ø"},{"body":"","link":"https://kane.mx/tags/performance-tuning/","section":"tags","tags":null,"title":"Performance-Tuning"},{"body":"","link":"https://kane.mx/tags/trobule-shooting/","section":"tags","tags":null,"title":"Trobule-Shooting"},{"body":"","link":"https://kane.mx/tags/zsh/","section":"tags","tags":null,"title":"Zsh"},{"body":"","link":"https://kane.mx/tags/codecommit/","section":"tags","tags":null,"title":"CodeCommit"},{"body":"Github/GitlabÂ∑≤ÁªèÊàê‰∏∫‰ºóÂ§öÂºÄÂèëËÄÖÈùûÂ∏∏ÁÜüÊÇâÁöÑ‰ª£Á†ÅÂçè‰ΩúÂπ≥Âè∞ÔºåÈÄöËøá‰ªñ‰ª¨ÂèÇ‰∏éÂºÄÊ∫êÈ°πÁõÆÊàñÂÆûÊñΩ‰ºÅ‰∏öÂÜÖÈÉ®È°πÁõÆÂçè‰Ωú„ÄÇ\nAWS‰πüÊèê‰æõ‰∫ÜÊâòÁÆ°ÁöÑ„ÄÅÂü∫‰∫éGit„ÄÅÂÆâÂÖ®‰∏îÈ´òÂèØÁî®ÁöÑ‰ª£Á†ÅÊúçÂä°CodeCommit„ÄÇCodeCommit‰∏ªË¶ÅÈíàÂØπ‰ºÅ‰∏öÁî®Êà∑Âú∫ÊôØÔºåÊâÄ‰ª•‰ªñÂπ∂Ê≤°ÊúâÁ§æ‰∫§ÂäüËÉΩ‰ª•Âèä‰ª£Á†Å‰ªìÂ∫ìforkÂäüËÉΩÔºåÊòØÂê¶CodeCommitÂ∞±Êó†Ê≥ïÂÆûÁé∞GithubÂü∫‰∫éPull RequestÁöÑÂçèÂêåÂ∑•‰ΩúÊ®°ÂºèÂë¢Ôºü\nÁ≠îÊ°àÊòØÔºåCodeCommitÂÆåÂÖ®ÂèØ‰ª•ÂÆûÁé∞Âü∫‰∫éPull RequestÁöÑ‰ª£Á†ÅÂçè‰Ωú„ÄÇÁî±‰∫éGitÁöÑÂàÜÂ∏ÉÂºè‰ª£Á†ÅÁÆ°ÁêÜÁâπÊÄßÔºåÈ¶ñÂÖàfork‰∏äÊ∏∏È°πÁõÆ‰ªìÂ∫ìÔºåÂ∞Ü‰øÆÊîπÂêéÁöÑ‰ª£Á†ÅÊèê‰∫§Âà∞fork‰ªìÂ∫ìÔºåÈÄöËøáPull RequestÁî≥ËØ∑‰øÆÊîπËØ∑Ê±ÇÂêàÂπ∂„ÄÇGithubÂ∞ÜËøôÂ•óÂçè‰ΩúÊµÅÁ®ãÊé®ÂπøÂºÄÊù•Âπ∂Ë¢´ÂºÄÊ∫êÈ°πÁõÆÂπøÊ≥õÈááÁî®„ÄÇÂÖ∂ÂÆûËøòÊúâÂè¶Â§ñÁöÑGit‰ªìÂ∫ìÂçèÂêåÊñπÂºèÊù•ÂÆåÊàêÂ§ö‰∫∫ÁöÑÂçè‰ΩúÂºÄÂèëÔºå‰æãÂ¶ÇGerrit Code Review„ÄÇÁõÆÂâçAndroid„ÄÅEclipse Foundation‰∏ãÈù¢ÁöÑÂêÑÁßçÈ°πÁõÆÈÉΩÂú®‰ΩøÁî®Gerrit‰Ωú‰∏∫ÂçèÂêåÂºÄÂèëÂ∑•ÂÖ∑„ÄÇGerritÈÄöËøáÊéßÂà∂Âêå‰∏Ä‰∏™‰ª£Á†Å‰ªìÂ∫ì‰∏≠‰∏çÂêåËßíËâ≤ÁöÑÁî®Êà∑ÂèØÊèê‰∫§‰ª£Á†ÅÂàÜÊîØÁöÑÊùÉÈôêÊù•ÂÆûÁé∞‰ª£Á†ÅË¥°ÁåÆ„ÄÅReview„ÄÅÊåÅÁª≠ÈõÜÊàê‰ª•ÂèäÂçèÂêåÂºÄÂèëÁöÑ„ÄÇ\nCodeCommit‰Ωú‰∏∫AWSÊâòÁÆ°ÁöÑÊúçÂä°ÔºåÂêåIAMËÆ§ËØÅÂíåÊéàÊùÉÁÆ°ÁêÜÂÅö‰∫ÜÂæàÂ•ΩÁöÑÈõÜÊàê„ÄÇÂÆåÂÖ®ÂèØ‰ª•ÈÄöËøáIAM PolicyÁöÑËÆæÁΩÆÔºå‰∏∫Âêå‰∏Ä‰∏™‰ª£Á†Å‰ªìÂ∫ì‰∏≠‰∏çÂêåÁî®Êà∑ËßíËâ≤ËÆæÁΩÆ‰∏çÂêåÁöÑÊùÉÈôê„ÄÇ‰ΩøÁî®Á±ª‰ººGerritÁöÑÊùÉÈôêÊéßÂà∂ÊÄùË∑ØÔºå\n‰ªªÊÑè‰ª£Á†Å‰ªìÂ∫ìÂçè‰ΩúËÄÖÂèØ‰ª•Êèê‰∫§‰ª£Á†ÅÂà∞ÁâπÂÆöÂê´‰πâÁöÑÂàÜÊîØÔºå‰æãÂ¶ÇÔºåfeatures/*, bugs/*„ÄÇÂèØ‰ª•ÂÖÅËÆ∏Â§ö‰∫∫ÂçèÂêåÂ∑•‰ΩúÂú®Êüê‰∏ÄÁâπÂÆöÂàÜÊîØ‰∏ä„ÄÇÂçè‰ΩúËÄÖÂêåÊó∂ÂèØ‰ª•ÂàõÂª∫Êñ∞ÁöÑPull RequestËØ∑Ê±ÇÂêàÂπ∂‰ª£Á†ÅÂà∞‰∏ªÂàÜÊîØÔºå‰æãÂ¶ÇmasterÊàñËÄÖmainline„ÄÇ ‰ª£Á†Å‰ªìÂ∫ìMaster/OwnerÊúâÊùÉÈôêÂêàÂπ∂Pull Request„ÄÇ ÊãíÁªù‰ªª‰Ωï‰∫∫Áõ¥Êé•Êé®ÈÄÅ‰ª£Á†ÅÂà∞‰ªìÂ∫ì‰∏ªÂàÜÊîØÔºåÂåÖÊã¨‰ªìÂ∫ìOwner/Admin„ÄÇ ÁõëÂê¨‰ªìÂ∫ìPull RequestÂàõÂª∫ÂíåPRÊ∫êÂàÜÊîØÊõ¥Êñ∞‰∫ã‰ª∂ÔºåËá™Âä®Ëß¶ÂèëËØ•PRÂØπÂ∫îÂàÜÊîØÁöÑautomation buildÔºåÁºñËØë„ÄÅÊµãËØïÁ≠âÈÄöËøáÂêéÔºåËá™Âä®‰∏∫PRÁöÑÈÄöËøáÊäïÁ•®+1„ÄÇÂèç‰πãËã•Â§±Ë¥•ÔºåÂàôÂèñÊ∂àÊäïÁ•®„ÄÇ ‰∏∫‰ª£Á†Å‰ªìÂ∫ìËÆæÁΩÆPR ReviewËßÑÂàôÔºåËá≥Â∞ëÈúÄË¶ÅÊî∂Âà∞PR automation buildÂíå‰ªìÂ∫ìMaster/OwnerÂêàËÆ°‰∏§Á•®ÈÄöËøáÊâçÂÖÅËÆ∏ÂêàÂπ∂‰ª£Á†Å„ÄÇ ÁõëÂê¨‰ª£Á†Å‰ªìÂ∫ì‰∏ªÂàÜÊîØÔºå‰ªªÊÑèÊñ∞Êèê‰∫§Â∞ÜËß¶ÂèëËá™Âä®ÂåñÂèëÂ∏ÉBuild„ÄÇÂ∞ÜÊúÄÊñ∞ÂèòÊõ¥Âú®Êï¥‰∏™Á≥ªÁªü‰∏äÂÅöÈõÜÊàê„ÄÇ ÊòØ‰∏çÊòØÂæàÊ£íÔºÅÂÆåÂÖ®ÂÅöÂà∞‰∫ÜGithub„ÄÅGithub Pull Request„ÄÅGithub Action/Travis CIÊï¥Â•ódevopsÂçèÂêåÂºÄÂèëÁöÑÊµÅÁ®ã„ÄÇ\nÂçè‰ΩúÊµÅÁ®ãÂ¶Ç‰∏ãÂõæÔºå ÂêåÊó∂Ôºå‰ª•‰∏äÊï¥Â•óÂü∫‰∫éCodeCommit‰ª£Á†ÅÁÆ°ÁêÜÁöÑdevopsÂ∑•‰ΩúÊµÅÁ®ãÂèØ‰ª•Âà©Áî®CloudFormationÂÆûÁé∞AWSËµÑÊ∫êÁºñÊéíÔºåÂ∞ÜDevops‰æùËµñÁöÑInfra‰ΩøÁî®‰ª£Á†ÅÊù•ÂÅöÁÆ°ÁêÜÔºÅËøôÊ†∑ÁöÑÂ•ΩÂ§ÑÊòØÔºå‰ºÅ‰∏öÂÜÖÈÉ®Âç≥‰ΩøÊúâÊï∞ÁôæÊï∞ÂçÉÁîöËá≥Êõ¥Â§ö‰ª£Á†Å‰ªìÂ∫ìÈÉΩÂèØ‰ª•Áªü‰∏ÄÁÆ°ÁêÜÔºåÊñ∞‰ªìÂ∫ìÁöÑÁî≥ËØ∑‰πüÂèØ‰ª•ÈÄöËøáInfra‰ª£Á†ÅÁöÑPRÔºåÂú®ÈÄöËøáÂÆ°ÊâπÂêàÂπ∂ÂêéËá™Âä®‰ªéAWS provisioningÂàõÂª∫Âá∫Á¨¶Âêà‰ºÅ‰∏öÁÆ°ÁêÜË¶ÅÊ±ÇÁöÑÂÆâÂÖ®‰ª£Á†Å‰ªìÂ∫ì„ÄÇÂæàÈÖ∑Âêß\u0026#x1f606;\nËøôÈáåÊúâ‰∏ÄÂ•óÂÆåÊï¥ÁöÑÂàõÂª∫‰ª•‰∏äÂ∑•‰ΩúÊµÅÁöÑÊºîÁ§∫ÔºåÊúâÂÖ¥Ë∂£ÁöÑËØªËÄÖÂèØ‰ª•Âú®Ëá™Â∑±ÁöÑË¥¶Êà∑ÂÜÖ‰ΩìÈ™å„ÄÇÊï¥Â•óÊñπÊ°àÂÆåÂÖ®‰ΩøÁî®ÁöÑÊòØAWSÊâòÁÆ°ÊúçÂä°Ôºå‰ªÖÊåâÂÆûÈôÖ‰ΩøÁî®Èáè(Â¶Ç‰ΩøÁî®CodeBuildÁºñËØë‰∫Ü‰ª£Á†Å)ËÆ°Ë¥π„ÄÇ\n","link":"https://kane.mx/posts/2020/codecommit-devops-model/","section":"posts","tags":["‰∫ëËÆ°ÁÆó","AWS","Devops","CodeCommit","Git"],"title":"Âü∫‰∫éCodeCommit‰ª£Á†ÅÁÆ°ÁêÜÁöÑÊó†ÊúçÂä°Âô®Êû∂ÊûÑDevops"},{"body":"","link":"https://kane.mx/tags/aws-api-gateway/","section":"tags","tags":null,"title":"AWS API Gateway"},{"body":"AWSÂú®3Êúà12Êó•Ê≠£ÂºèÂèëÂ∏É‰∫ÜÊñ∞‰∏Ä‰ª£ÁöÑAPIÁΩëÂÖ≥ -- HTTP APIs„ÄÇAWSÂèëÂ∏ÉÁöÑÁ¨¨‰∏Ä‰ª£API GatewayÊúçÂä°Â∑≤ÁªèÂø´5Âπ¥‰∫ÜÔºåÈÄöËøáËøô‰∫õÂπ¥Êù•Â§ßËßÑÊ®°ÊúçÂä°ÂÆ¢Êà∑ÁöÑÂøÉÂæó‰ª•ÂèäÂÆ¢Êà∑ÂèçÈ¶àÔºåÁî±Ê≠§ÈáçÊñ∞ÊûÑÂª∫‰∫ÜÊõ¥Âø´ÔºàÁõ∏ÊØîÁ¨¨‰∏Ä‰ª£ÁΩëÂÖ≥60%ÁöÑÂª∂ËøüÂáèÂ∞ëÔºâ„ÄÅÊõ¥‰æøÂÆúÔºàËá≥Â∞ëËäÇÁúÅ71%ÁöÑË¥πÁî®Ôºâ„ÄÅÊõ¥ÊòìÁî®ÁöÑÁ¨¨‰∫å‰ª£ÁΩëÂÖ≥ÊúçÂä°„ÄÇ\nÈô§‰∫ÜÊÄßËÉΩ„ÄÅË¥πÁî®„ÄÅÊòìÁî®ÊÄßÁöÑÂ§ßÂπÖÂ∫¶ÊîπËøõ‰πãÂ§ñÔºåÂú®HTTP APIsÂèëÂ∏ÉÂçöÂÆ¢‰∏≠ÁùÄÈáç‰ªãÁªç‰∫Ü‰ª•‰∏ãÊñ∞ÁâπÊÄßÔºå\nHTTP APIsÁΩëÂÖ≥ÂèØÂêåÁßÅÊúâVPCÂÜÖÁöÑË¥üËΩΩÂùáË°°(ALB/NLB)ÔºåÊúçÂä°ÂèëÁé∞(Cloup Map)ÈõÜÊàê„ÄÇÊÑèÂë≥ÁùÄÂèØÂ∞ÜÁõÆÂâçÊúÄÊµÅË°å‰∏îÊôÆÈÅçÂ∫îÁî®ÁöÑÂÆπÂô®ÊúçÂä°‰Ωú‰∏∫APIÂêéÁ´Ø ÂèØ‰ª•Â∞ÜËá™ÂÆö‰πâÂüüÂêçÁöÑAPIË∑ØÂæÑÊ∑∑ÂêàÊò†Â∞ÑÂà∞Á¨¨‰∏Ä‰ª£ÁöÑREST APIsÂíåÊúÄÊñ∞ÁöÑHTTP APIs ËØ∑Ê±ÇÈôêÊµÅÁöÑÊîπËøõ„ÄÇÊîØÊåÅÂØπ‰∏çÂêåstage‰ª•ÂèäËØ∑Ê±ÇË∑ØÁî±ÂàÜÂà´ËÆæÁΩÆ‰∏çÂêåÁöÑÈôêÊµÅ StageÂèòÈáè„ÄÇÂèØ‰ª•Â∞ÜStageÂèòÈáè‰º†ÈÄíÁªôAPIÁΩëÂÖ≥ÂêéÁ´ØÁöÑÊúçÂä°„ÄÇÂêåÊó∂ÊîØÊåÅË∑ØÁî±Âú®‰∏çÂêåÁöÑstageÂä®ÊÄÅÈõÜÊàê‰∏çÂêåÁöÑÂêéÁ´ØLambdaÂáΩÊï∞ LambdaÈõÜÊàêÊó∂‰ΩøÁî®Payload version 2.0„ÄÇVersion 2.0Ê†ºÂºèÊèê‰æõ‰∫ÜÊõ¥Â§öÁöÑÁÅµÊ¥ªÊÄßÂèäÁÆÄÂåñ‰∫ÜÊï∞ÊçÆÊ†ºÂºè ÊîØÊåÅÂØºÂÖ• Swagger / OpenAPI ÈÖçÁΩÆÊñá‰ª∂ Â¶ÇÊûúÂØπHTTP APIsÊÑüÂÖ¥Ë∂£ÔºåÂèØ‰ª•Â∞ùËØïÂú®Ëá™Â∑±ÁöÑË¥¶Êà∑ÂÜÖÈÉ®ÁΩ≤Ëøô‰∏™Á§∫‰æã„ÄÇËøô‰∏™Á§∫‰æãÊºîÁ§∫‰∫ÜÂ¶Ç‰ΩïÊåâÈúÄ‰ΩøÁî®AWS BatchÊúçÂä°ËøõË°åÊâπÈáè‰ªªÂä°ËÆ°ÁÆóÔºåÂêåÊó∂Â∞Ü‰ªªÂä°Êèê‰∫§ÂíåÊü•ËØ¢Áä∂ÊÄÅÈÄöËøáHTTPÊé•Âè£Êèê‰æõÂá∫Êù•„ÄÇËØ•Á§∫‰æãÊîØÊåÅÈÉ®ÁΩ≤Êó∂ÈÄâÁî®‰∏çÂêåÁöÑAWSÊúçÂä°ÔºàALB„ÄÅREST APIsÊàñHTTP APIsÔºâÊù•Êèê‰æõËøô‰∫õAPIÊé•Âè£ËÆøÈóÆ„ÄÇÊï¥‰∏™Á§∫‰æãÈÉΩÊòØÂü∫‰∫éÊó†ÊúçÂä°Âô®Êû∂ÊûÑÂÆûÁé∞ÁöÑÔºå‰∏çËøõË°åÊâπÈáèËÆ°ÁÆóÊòØ‰∏ç‰∫ßÁîü‰ªª‰ΩïË¥πÁî®ÁöÑÂì¶\u0026#x1f604;„ÄÇ\n","link":"https://kane.mx/posts/2020/new-http-apis-of-api-gateway/","section":"posts","tags":["‰∫ëËÆ°ÁÆó","FaaS","AWS","AWS API Gateway","Serverless Computing"],"title":"AWSÂèëÂ∏ÉÊõ¥Âø´„ÄÅÊõ¥‰æøÂÆú„ÄÅÊõ¥ÊòìÁî®ÁöÑHTTP APIs"},{"body":"","link":"https://kane.mx/tags/faas/","section":"tags","tags":null,"title":"FaaS"},{"body":"Âú®re:Invent 2019‰πãÂâçÔºåAWS ToolkitÂèëÂ∏É‰∫ÜCloud Debugging betaÂäüËÉΩ„ÄÇËØ•ÂäüËÉΩÊîØÊåÅÂú®IntelliJ IDEs(IntelliJ, PyCharm, Webstorm, ‰ª•Âèä Rider)‰∏≠ËøúÁ®ãË∞ÉËØï ECS Fargate ÂÆπÂô®‰∏≠ÊâßË°åÁöÑÂ∫îÁî®Á®ãÂ∫è„ÄÇ\n\bÂØπECS Fargate demoÂêØÁî®‰∫ÜËøúÁ®ãË∞ÉËØïÂπ∂Ë∞ÉËØïÊàêÂäüÂêéÔºåËøôÈáåËÆ∞ÂΩï‰∏Ä‰∏ãËØ•ÂäüËÉΩÁöÑ‰ΩøÁî®‰ΩìÈ™åÂπ∂‰∏îÂàÜ‰∫´‰ΩìÈ™åËøáÁ®ã‰∏≠ÊéâËøõÂéªËøáÁöÑ‰∏Ä‰∫õÂùë„ÄÇ\nËØïÁî®‰ΩìÈ™å È¶ñÂÖàÔºåËØ•ÂäüËÉΩ‰∏çÈÄÇÁî®‰∫éÁîü‰∫ßÁéØÂ¢É„ÄÇÂõ†‰∏∫ÂØπECS FargateÁ±ªÂûãÁöÑServiceÂêØÁî®Cloud DebuggingÂäüËÉΩ‰ºöÂ∞ÜÂéüÂßãÁöÑECS ServicesÊî∂Áº©‰∏∫0‰∏™taskÂâØÊú¨ÔºåÂêåÊó∂ÂàõÂª∫‰∏Ä‰∏™Êñ∞ÁöÑServiceÂπ∂ÂêØÁî®Êñ∞ÁöÑTask DefinitionÔºåÊñ∞ÁöÑTask Definition‰∏≠‰ºöÂä†ÂÖ•Cloud Debug SidecarÂÆπÂô®Êù•ËæÖÂä©ÂÆûÁé∞ËøúÁ®ãË∞ÉËØï„ÄÇÊï¥‰∏™ËøáÁ®ã‰ºöÂØπÁîü‰∫ßÁéØÂ¢ÉÈÄ†ÊàêÂèòÊõ¥„ÄÇ Â¶ÇÊûúECSÈõÜÁæ§ÊòØÈÄöËøáCI/CDÊåÅÁª≠ÈÉ®ÁΩ≤ÔºåÂπ∂‰∏îÊòØÂ§ö‰∫∫ÂçèÂêå‰ΩøÁî®ÁöÑÁéØÂ¢ÉÔºåËØ•ÂäüËÉΩ‰πü‰∏çÈÄÇÁî®„ÄÇÂõ†‰∏∫ÔºåÂØπÊüê‰∫õÂÆπÂô®ÊúçÂä°ÂêØÁî®Cloud DebuggingÂ∞ÜÂØºËá¥‰ªñ‰∫∫ÁöÑÊåÅÁª≠ÈÉ®ÁΩ≤Â§±Ë¥•Êàñ‰∏çÁîüÊïà„ÄÇ ÂêØÁî®Cloud DebuggingÊìç‰ΩúÊØîËæÉÈ∫ªÁÉ¶Ôºå‰∏îÂêØÁî®Áä∂ÊÄÅ‰∏ãÊó†Ê≥ïÊõ¥Êñ∞ECS‰∏≠ÈÉ®ÁΩ≤ÁöÑÁâàÊú¨„ÄÇÈúÄË¶ÅÂÖàÂÅúÁî®Cloud DebuggingÔºåÈÉ®ÁΩ≤Êñ∞ÁâàÊú¨‰ª£Á†ÅÔºåÁÑ∂ÂêéÂÜçÊ¨°ÂêØÁî®Cloud DebuggingÊâçËÉΩË∞ÉËØïÊñ∞‰ª£Á†Å„ÄÇÂ∞ΩÂèØËÉΩÁöÑ‰∏çË¶Å‰æùËµñCloud DebuggingÊù•Ë∞ÉËØïÁ®ãÂ∫èÔºåËä±ÂäüËÉΩÂÅöÂ•ΩÂçïÂÖÉÊµãËØïÔºåÈõÜÊàêÊµãËØï‰ª•ÂèäE2EÊµãËØïÊù•ÈÅøÂÖçË∞ÉËØï‰∫ëÁ´ØÁéØÂ¢É„ÄÇ ËØïÁî®ÁªèÈ™å ÊåâÁÖßÂÆòÊñπÊñáÊ°£ÂêØÁî®Cloud DebuggingÂêéÔºåÂàõÂª∫Cloud Debugging Launch ConfigurationÂπ∂ÊâßË°åË∞ÉËØïÔºåÈÅáÂà∞**Retrieve execution role finished exceptionally**ÈîôËØØ„ÄÇÈîôËØØÁöÑÂéüÂõ†ÊòØÔºåÊñáÊ°£‰∏≠Ê≤°ÊúâÊèêÂà∞Cloud Debug SidecarÈúÄË¶Ålogs:CreateLogStreamÊùÉÈôêÂàõÂª∫CloudWatch Logs Stream„ÄÇËß£ÂÜ≥ÊñπÊ°àÊòØÔºå‰∏∫ECS Task Execution RoleÊ∑ªÂä†logs:CreateLogStreamÊùÉÈôê„ÄÇ Âú®AWS Toolkit JetbrainsÂΩìÂâçÁöÑÁâàÊú¨1.9-193‰∏çÊîØÊåÅÂêØÁî®‰∫ÜAppMeshÊàñX-RayÁöÑTask„ÄÇËß£ÂÜ≥ÊñπÊ°àÊòØÔºåÂØπÈúÄË¶ÅÂêØÁî®Cloud DebuggingÁöÑTaskÊöÇÊó∂Á¶ÅÁî®App MeshÂíåX-Ray„ÄÇ Cloud DebuggingÊòØ‰∏Ä‰∏™‰∏çÈîôÁöÑÂºÄÂèëÂ∑•ÂÖ∑Â∞ùËØïÊÄùË∑ØÔºåÂ∏ÆÂä©ÂºÄÂèëËÄÖÊõ¥Â•ΩÁöÑÂÅöÂá∫Cloud NativeÂ∫îÁî®„ÄÇ‰ΩÜÊòØËØ•È°πÁõÆ‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™Êó©ÊúüÈ°πÁõÆÔºåÊúâËÆ∏Â§öÈóÆÈ¢òÈúÄË¶Å‰øÆÂ§çÂíåÊîπËøõ„ÄÇ\n","link":"https://kane.mx/posts/2019/aws-cloud-debugging/","section":"posts","tags":["AWS","AWS Toolkit","AWS ECS","AWS Fargate","IntelliJ IDEs"],"title":"AWS Cloud DebuggingÂàùÊé¢"},{"body":"","link":"https://kane.mx/tags/aws-ecs/","section":"tags","tags":null,"title":"AWS ECS"},{"body":"","link":"https://kane.mx/tags/aws-fargate/","section":"tags","tags":null,"title":"AWS Fargate"},{"body":"","link":"https://kane.mx/tags/aws-toolkit/","section":"tags","tags":null,"title":"AWS Toolkit"},{"body":"","link":"https://kane.mx/tags/intellij-ides/","section":"tags","tags":null,"title":"IntelliJ IDEs"},{"body":"AWS BatchÊòØ‰∏Ä‰∏™ÂÖ®ÊâòÁÆ°ÁöÑÊâπÂ§ÑÁêÜË∞ÉÂ∫¶ÊúçÂä°ÔºåÂÆÉÂèØ‰∏∫Áî®Êà∑ÁÆ°ÁêÜÊâÄÊúâÂü∫Á°ÄËÆæÊñΩÔºå‰ªéËÄåÈÅøÂÖç‰∫ÜÈ¢ÑÁΩÆ„ÄÅÁÆ°ÁêÜ„ÄÅÁõëÊéßÂíåÊâ©Â±ïÊâπÂ§ÑÁêÜËÆ°ÁÆó‰Ωú‰∏öÊâÄÂ∏¶Êù•ÁöÑÂ§çÊùÇÊÄß„ÄÇÂΩìÁÑ∂AWS BatchÂ∑≤‰∏é AWS Âπ≥Âè∞ÂéüÁîüÈõÜÊàêÔºåËÆ©Áî®Êà∑ËÉΩÂ§üÂà©Áî® AWS ÁöÑÊâ©Â±ï„ÄÅËÅîÁΩëÂíåËÆøÈóÆÁÆ°ÁêÜÂäüËÉΩ„ÄÇËÆ©Áî®Êà∑ËΩªÊùæËøêË°åËÉΩÂ§üÂÆâÂÖ®Âú∞‰ªé AWS Êï∞ÊçÆÂ≠òÂÇ®ÔºàÂ¶Ç Amazon S3 Âíå Amazon DynamoDBÔºâ‰∏≠Ê£ÄÁ¥¢Êï∞ÊçÆÂπ∂ÂêëÂÖ∂‰∏≠ÂÜôÂÖ•Êï∞ÊçÆÁöÑ‰Ωú‰∏ö„ÄÇAWS BatchÂèØÊ†πÊçÆÊâÄÊèê‰∫§ÁöÑÊâπÂ§ÑÁêÜ‰Ωú‰∏öÁöÑÊï∞ÈáèÂíåËµÑÊ∫êË¶ÅÊ±ÇÈ¢ÑÁΩÆËÆ°ÁÆóËµÑÊ∫êÂπ∂‰ºòÂåñ‰Ωú‰∏öÂàÜÈÖç„ÄÇËÉΩÂ§üÂ∞ÜËÆ°ÁÆóËµÑÊ∫êÂä®ÊÄÅÊâ©Â±ïËá≥ËøêË°åÊâπÂ§ÑÁêÜ‰Ωú‰∏öÊâÄÈúÄÁöÑ‰ªª‰ΩïÊï∞ÈáèÔºå‰ªéËÄå‰∏çÂøÖÂèóÂõ∫ÂÆöÂÆπÈáèÈõÜÁæ§ÁöÑÈôêÂà∂„ÄÇAWS BatchËøòÂèØ‰ª•Âà©Áî® Spot ÂÆû‰æãÔºå‰ªéËÄåËøõ‰∏ÄÊ≠•Èôç‰ΩéËøêË°åÊâπÂ§ÑÁêÜ‰Ωú‰∏ö‰∫ßÁîüÁöÑË¥πÁî®„ÄÇ\nAWS BatchÊúçÂä°Êú¨Ë∫´ÊòØÂÖçË¥πÁöÑÔºå‰ªÖÊî∂ÂèñÂÆûÈôÖ‰ΩøÁî®ÁöÑ EC2 ÂÆû‰æãË¥πÁî®„ÄÇ\nÊàëÂàõÂª∫‰∫Ü‰∏Ä‰∏™Batch App demoÊù•ÊºîÁ§∫AWS BatchÁõ∏ÂÖ≥‰ΩøÁî®ÊñπÊ≥ï„ÄÇËØ•Á§∫‰æãÈÄöËøá‰∏Ä‰∏™Restful APIÊé•Âè£Êù•Êèê‰∫§ÊâπÂ§ÑÁêÜ‰ªªÂä°ÔºåRestful APIÈÄöËøáALB + LambdaÂáΩÊï∞Êù•Êö¥Èú≤ÊúçÂä°„ÄÇLambdaÂáΩÊï∞Ë¢´Ëß¶ÂèëÂêéÔºåÂ∞ÜÊñ∞‰ªªÂä°ËØ∑Ê±ÇÂèëÈÄÅÂà∞SQSÊúçÂä°„ÄÇÈöèÂêéÂè¶‰∏Ä‰∏™LambdaÂ∞ÜÊ∂àË¥πËøô‰∏™SQSÔºåÂπ∂Â∞ÜË∞ÉÁî®AWS Batch APIÊù•Êèê‰∫§Êñ∞ÁöÑÊâπÂ§ÑÁêÜ‰ªªÂä°ÔºåÂêåÊó∂Â∞Ü‰ªªÂä°‰ø°ÊÅØÂÇ®Â≠òÂà∞DynamoDB‰∏≠„ÄÇÂêåÊó∂DemoÂàõÂª∫‰∫ÜBatch‰ªªÂä°‰ºö‰ΩøÁî®Âà∞ÁöÑDocker ImageÔºåÂπ∂‰∏îÈ¢ÑÂÖàÊèê‰∫§Âà∞ECR‰∏≠„ÄÇÂêåÊó∂Batch‰ªªÂä°ÂÆö‰πâ‰∫Ü‰ΩøÁî®ÁöÑEC2ÂÆû‰æãÁ±ªÂûã(c5Á≥ªÂàóÂÆû‰æãÔºå‰∏îÂåÖÊã¨SpotÂíåÊåâÈúÄ‰∏§ÁßçËÆ°Ë¥πÊñπÂºèÁöÑÂÆû‰æãÔºå‰∏î‰ºòÂÖà‰ΩøÁî®SpotÂÆû‰æã)ÔºåÂÆû‰æãÈªòËÆ§‰º∏Áº©Êï∞Èáè‰∏∫0(Ê≤°ÊúâÂèØÊâßË°å‰ªªÂä°Êó∂Â∞Ü‰∏≠Ê≠¢ÂÆû‰æã)„ÄÇÂπ∂‰∏îÊèê‰∫§ÁöÑ‰ªªÂä°ÂàÜ‰∏∫ËÆ°ÁÆó‰ªªÂä°ÂíåÁªüËÆ°ÂΩíÂπ∂‰ªªÂä°ÔºåÁªüËÆ°ÂΩíÂπ∂‰ªªÂä°‰ºö‰æùËµñÊâÄ‰ª•ËÆ°ÁÆó‰ªªÂä°ÊâßË°åÂÆåÊØïÊâçÂºÄÂßãÊâßË°å„ÄÇÊúÄÂêéÈÄöËøáÂè¶‰∏ÄRestfulÊé•Âè£Êü•ËØ¢ËÆ°ÁÆó‰ªªÂä°ÁöÑÊúÄÁªàÁªìÊûúÔºåËØ•Êé•Âè£ÂêåÊ†∑‰ΩøÁî®ALB + LambdaÂáΩÊï∞Êù•ÂÆûÁé∞„ÄÇ\nEnjoy this Batch App demo orchestrated by AWS CDK.\n","link":"https://kane.mx/posts/2019/aws-batch/","section":"posts","tags":["AWS","Batch","Infrastructure as Code"],"title":"AWS BatchÁÆÄ‰ªã"},{"body":"","link":"https://kane.mx/tags/batch/","section":"tags","tags":null,"title":"Batch"},{"body":"Âú®Êã•ÊúâÂüüÂêçÂêéÔºåÈÄöÂ∏∏Â∏åÊúõÂàõÂª∫‰∏Ä‰∫õËá™ÊúâÂüüÂêç‰∏ãÁöÑÈÇÆÁÆ±Êù•Êî∂Âèñ‰∏çÂêåÁî®ÈÄîÁöÑÈÇÆ‰ª∂ÔºåÂêåÊó∂‰∏çÂ∏åÊúõ‰∏∫ËøôÈÉ®ÂàÜÂäüËÉΩ‰ªòË¥π\u0026#x1f603;„ÄÇ‰ΩøÁî®ÂÖçË¥πÁöÑ‰ºÅ‰∏öÈÇÆÁÆ±(ÊØîÂ¶ÇÁΩëÊòì‰ºÅ‰∏öÈÇÆÁÆ±„ÄÅÈòøÈáå‰∫ë‰ºÅ‰∏öÈÇÆÁÆ±)ÊòØ‰∏ÄÁßçÈÄâÊã©„ÄÇËøôÊó∂Â∞±ÈúÄË¶ÅÈÖçÁΩÆÈÇÆ‰ª∂Âú∞ÂùÄÂíåÈÇÆ‰ª∂ÂÆ¢Êà∑Á´ØÊù•Êî∂ÂèñÈÇÆ‰ª∂ÔºåÂ¶ÇÊûúÊúâÂ§ö‰∏™ÈÇÆÁÆ±Âú∞ÂùÄÔºåÈÖçÁΩÆ‰ºöÁâπÂà´È∫ªÁÉ¶„ÄÇÊúâÊó∂ÔºåËøô‰∫õ‰ºÅ‰∏öÈÇÆÁÆ±ÁöÑÊî∂‰ª∂ÊúçÂä°‰ºöËé´ÂêçÂÖ∂Â¶ôÁöÑ‰∏¢Â§±‰∏Ä‰∫õÈÇÆ‰ª∂„ÄÇ\nËøôÁßçÂú∫ÊôØ‰∏ãÔºåÈÇÆ‰ª∂ËΩ¨ÂèëÊúçÂä°ÊòØ‰∏ÄÁßçÈùûÂ∏∏Â•ΩÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇÊó†ÈúÄÊê≠Âª∫ÈÇÆ‰ª∂ÊúçÂä°Âô®ÊàñÁî≥ËØ∑ÂÖçË¥πÈÇÆ‰ª∂ÊúçÂä°ÔºåÂè™ÈúÄË¶ÅÈÖçÁΩÆÂüüÂêçÁöÑÈÇÆ‰ª∂MXËß£ÊûêÂà∞ËΩ¨ÂèëÈÇÆ‰ª∂Êî∂Âèë‰ª∂ÊúçÂä°ÔºåÂêåÊó∂‰ΩøÁî®DNS TXT recordÈÖçÁΩÆËΩ¨ÂèëËßÑÂàôÔºåÂç≥ÂèØÂ∞ÜÊâÄ‰ª•ÂèëÈÄÅÁöÑËá™ÊúâÂüüÂêç‰∏ãÁöÑÈÇÆ‰ª∂ËΩ¨ÂèëÂà∞Â∑≤ÊúâÁöÑÈÇÆÁÆ±Âú∞ÂùÄÔºÅ\u0026#x1f192;\nÁâπÂà´ÂÆâÂà©Forward EmailÊúçÂä°Ôºå‰∏Ä‰∏™ÂÖçË¥πËÄå‰∏îÊòØÂºÄÊ∫êÁöÑÈÇÆ‰ª∂ËΩ¨ÂèëÊúçÂä°„ÄÇ\nÂ¶Ç‰∏äÈù¢‰ªãÁªçÁöÑÔºåÂè™ÈúÄË¶Å‰∏∫ÂüüÂêçmydomain.comÂàõÂª∫Â¶Ç‰∏ã3Êù°DNSËß£ÊûêËÆ∞ÂΩïÔºå\nÂêçÁß∞ TTL ËÆ∞ÂΩïÁ±ªÂûã ‰ºòÂÖàÁ∫ß ËÆ∞ÂΩïÁöÑÂÜÖÂÆπ @ ÊàñËÄÖ Á©∫ÁôΩ 3600 MX 10 mx1.forwardemail.net @ ÊàñËÄÖ Á©∫ÁôΩ 3600 MX 20 mx2.forwardemail.net @ ÊàñËÄÖ Á©∫ÁôΩ 3600 TXT 20 forward-email=niftylettuce@gmail.com ÊâÄÊúâÂèëÂæÄ@mydomain.comÁöÑÈÇÆ‰ª∂Â∞ÜË¢´ËΩ¨ÂèëÂà∞ÈÇÆÁÆ±niftylettuce@gmail.com„ÄÇ\u0026#x270c;\u0026#xfe0f;\nÊõ¥Â§öÈÖçÁΩÆÈÄâÈ°πËØ∑Êü•ÁúãÊñáÊ°£„ÄÇ\n‰Ωú‰∏∫ Forward Email Êï∞Âπ¥ÁöÑ‰ªòË¥πÁöÑÁî®Êà∑ÂêéÔºåÊääËØ•ÈÖçÁΩÆËøÅÁßªÂà∞‰∫ÜÂÖçË¥πÁöÑ Cloudflare Email RoutingÔºåÂÆûÁé∞‰∫ÜÂêåÊ†∑ÁöÑÂäüËÉΩ„ÄÇÂõ†‰∏∫ËØ•ÂäüËÉΩÈúÄË¶ÅÂÖàÂ∞ÜÂüüÂêç DNS Ëß£ÊûêÊîæÂà∞ CloudflareÔºåÊâÄ‰ª•Âè™ÈúÄË¶ÅÁÇπÁÇπÈº†Ê†áÂ∞±ÈÖçÁΩÆÂÆåÊàê‰∫Ü„ÄÇüòä‚úåÔ∏è\n","link":"https://kane.mx/posts/2019/email-forwarding/","section":"posts","tags":["ÈÇÆ‰ª∂ËΩ¨Âèë","ÊäÄÂ∑ß"],"title":"ÂÖçË¥πÈÇÆ‰ª∂ËΩ¨ÂèëÊúçÂä°"},{"body":"","link":"https://kane.mx/tags/%E6%8A%80%E5%B7%A7/","section":"tags","tags":null,"title":"ÊäÄÂ∑ß"},{"body":"","link":"https://kane.mx/tags/%E9%82%AE%E4%BB%B6%E8%BD%AC%E5%8F%91/","section":"tags","tags":null,"title":"ÈÇÆ‰ª∂ËΩ¨Âèë"},{"body":"","link":"https://kane.mx/tags/edas/","section":"tags","tags":null,"title":"EDAS"},{"body":"","link":"https://kane.mx/tags/microservice/","section":"tags","tags":null,"title":"Microservice"},{"body":"","link":"https://kane.mx/tags/migration/","section":"tags","tags":null,"title":"Migration"},{"body":"ËøëÊúüÂÆûË∑µ‰∫ÜÂ∞ÜÈòøÈáå‰∫ëEDASÂæÆÊúçÂä°Â∫îÁî®ËøÅÁßªÂà∞AWS‰∏äÔºåÂú®ËøôÈáåÂàÜ‰∫´‰∏Ä‰∏ãËøÅÁßªÊñπÊ°à„ÄÇ\nËØ•ÊñπÊ°àÊ∂âÂèä‰∫Ü‰ª•‰∏ã‰∏â‰∏™ÊñπÈù¢Ôºå\nÂæÆÊúçÂä°Â∫îÁî®ÈõÜÁæ§„ÄÇÂú®AWS‰∏äÈááÁî®ÁöÑECSÈõÜÁæ§ÈÉ®ÁΩ≤ÂæÆÊúçÂä°Â∫îÁî®ÔºåÈÄöËøáCloudmapÂÆûÁé∞ÊúçÂä°Ê≥®ÂÜåÂèëÁé∞ÔºåApp MeshÂÆûÁé∞ÊúçÂä°Èó¥ÊµÅÈáèÊéßÂà∂„ÄÇÊõ¥Âä†ËØ¶Â∞ΩÁöÑÂæÆÊúçÂä°ËøÅÁßªË¶ÅÁÇπÂíåÂØπÂ∫îÊñπÊ°àÔºåËØ¶ËßÅ‰∏ãÈù¢ÁöÑdeck„ÄÇ Devops pipeline„ÄÇ‰ΩøÁî®ÊâòÁÆ°ÁöÑCodePipelineÔºåCodeBuildÂÆûÁé∞CI/CD„ÄÇ Infra as Code„ÄÇÂà©Áî®AWSÂº∫Â§ßÁöÑInfra as CodeËÉΩÂäõÔºåÂ∞Ü‰∫ë‰∏äÁöÑÂü∫Á°ÄËÆæÊñΩÂíåÂæÆÊúçÂä°Â∫îÁî®ÁºñÊéíÈÄöËøáCDK‰ª£Á†ÅÂÆûÁé∞„ÄÇ ‰∏ãÈù¢ÊòØËøÅÁßªÊñπÊ°àÁöÑdeck„ÄÇÂÆåÊï¥‰∏îÂèØÈÉ®ÁΩ≤ÁöÑPoC‰ª£Á†ÅÔºåÁÇπËøôÈáå„ÄÇ\n","link":"https://kane.mx/posts/2019/aliyun-edas-migration-in-action/","section":"posts","tags":["AWS","EDAS","Migration","Microservice","Infrastructure as Code"],"title":"ÂÆûÊàòAliyun EDASÂ∫îÁî®ËøÅÁßªAWS"},{"body":"","link":"https://kane.mx/tags/analysis/","section":"tags","tags":null,"title":"Analysis"},{"body":"ÊâòÁÆ°ÁöÑRDSÊï∞ÊçÆÂ∫ìÂ∑≤ÁªèÊòØ‰∫ëËÆ°ÁÆóÊúçÂä°‰∏≠ÈùûÂ∏∏ÊàêÁÜüÁöÑÊúçÂä°ÔºåÁªùÂ§ßÈÉ®ÂàÜÁöÑ‰∫ëËÆ°ÁÆóÁî®Êà∑‰ºöÈááÁî®RDSÊúçÂä°Êù•ÊèêÂçáÊï∞ÊçÆÂ∫ìÊúçÂä°ÁöÑÂèØÁî®ÊÄßÂêåÊó∂ÂáèÂ∞ëÊï∞ÊçÆÂ∫ìÁöÑÂêÑÁ±ªËøêÁª¥‰∫ãÂä°„ÄÇ\nAWS RDSÊúçÂä°ÊîØÊåÅÂºÄÂêØÂíåÊü•ËØ¢ÂêÑÁ±ªÁöÑÊï∞ÊçÆÂ∫ìÊó•ÂøóÔºåÂåÖÊã¨Â∏∏ËßÑÊó•Âøó„ÄÅÊÖ¢Êó•Âøó„ÄÅÈîôËØØÊó•ÂøóÂíåÂÆ°ËÆ°Êó•Âøó„ÄÇ‰ΩÜRDSÊúçÂä°ÈªòËÆ§Êèê‰æõÁöÑÊó•ÂøóÊü•ÁúãÂ∑•ÂÖ∑‰ªÖ‰ªÖÁ±ª‰ººÊñáÊú¨Êü•ÁúãÂô®ÔºåÊó†Ê≥ïÈíàÂØπÊó•ÂøóÊï∞ÊçÆÂÅöÁªüËÆ°ÂíåÊü•ÁúãÂéÜÂè≤ÊªöÂä®ÁöÑÂ≠òÊ°£„ÄÇ\nÊú¨ÊñáÂ∞Ü‰ªãÁªçÂ¶Ç‰Ωï‰ΩøÁî®AWS‰∏ä‰∫ëÂéüÁîüÁöÑÊúçÂä°Êê≠Âª∫Êó†ÊúçÂä°Êû∂ÊûÑÁöÑÂÆûÊó∂Êó•ÂøóÂàÜÊûêÊä•Ë°®Á≥ªÁªü„ÄÇËØ•Á≥ªÁªüÁöÑÂÆûÁé∞ÊÄùË∑ØÊù•Ëá™‰∫éAWS‰∏≠ÂõΩÁöÑ‰∏ÄÁØáÂçöÂÆ¢ÔºåËØ•Êñá‰ªãÁªç‰∫Ü‰ΩøÁî® CloudWatch LogsÔºåKinesis FirehoseÔºåAthena Âíå Quicksight ÂÆûÁé∞ÂÆûÊó∂ÂàÜÊûê Amazon Aurora Êï∞ÊçÆÂ∫ìÂÆ°ËÆ°Êó•Âøó„ÄÇ\nËøôÈáåÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÂÆåÊï¥ÁöÑAWS CDKÂ∫îÁî®ÂÆûÁé∞‰∫ÜÂçöÂÆ¢‰∏≠‰ªãÁªçÁöÑÊúçÂä°Êê≠Âª∫ÊÄùË∑ØÔºåRDSÂÆ°ËÆ°Êó•ÂøóÈÄöËøá CloudWatch Log -\u0026gt; Kinesis Firehose -\u0026gt; S3 ËøôÊ†∑‰∏Ä‰∏™Êï∞ÊçÆÁÆ°ÈÅìË¢´ËøáÊª§ÔºåËΩ¨Êç¢ÔºåÂéãÁº©ÊúÄÁªà‰øùÂ≠òÂà∞S3‰∏äÔºåÂèØË¢´Êó†ÊúçÂä°ÂàÜÊûêÊúçÂä°Athena‰ΩøÁî®„ÄÇÂêåÊó∂ÂàõÂª∫‰∫Ü‰∏Ä‰∏™LambdaÂáΩÊï∞Ê®°ÊãüÂ∫îÁî®ËÆøÈóÆÊï∞ÊçÆÂ∫ìÔºåÂÆÉÂë®ÊúüÊÄßÁöÑËøûÊé•‰∏äÂ∫îÁî®‰∏≠ÂàõÂª∫ÁöÑRDS AuroraÊï∞ÊçÆÂ∫ìÂπ∂ÊâßË°åÊü•ËØ¢ÊàñÂèòÊõ¥Sql„ÄÇÂú®Êï¥‰∏™Â∫îÁî®Âú®Ë¢´ÈÉ®ÁΩ≤ÊàêÂäüÂêéÊï∞ÂàÜÈíüÔºåÂèäÂèØÈÄöËøáAthenaÊï∞ÊçÆË°®Êü•ËØ¢ÁªüËÆ°AuroraÂÆ°ËÆ°Êó•Âøó„ÄÇEnjoy it\u0026#x1f606;\u0026#x1f606;\n","link":"https://kane.mx/posts/2019/rds-log-analysis/","section":"posts","tags":["AWS","Serverless","Analysis"],"title":"AWS RDSÊï∞ÊçÆÂ∫ìÊó•ÂøóÂàÜÊûêÂèäÂ±ïÁ§∫"},{"body":"","link":"https://kane.mx/tags/aws-vpn/","section":"tags","tags":null,"title":"AWS VPN"},{"body":"","link":"https://kane.mx/tags/openswan/","section":"tags","tags":null,"title":"Openswan"},{"body":"","link":"https://kane.mx/tags/site-to-site-vpn/","section":"tags","tags":null,"title":"Site-to-Site VPN"},{"body":"‰∏öÂä°‰∏ä‰∫ë‰πãÂêéÔºåÁªèÂ∏∏‰πüÊúâÈúÄÊ±ÇÂ∞ÜÂ§ö‰∫ë„ÄÅÊï∞ÊçÆ‰∏≠ÂøÉÊàñÂäûÂÖ¨ÂÆ§ÁöÑÁßÅÊúâÁΩëÁªúÂêå‰∫ëÁ´ØÁöÑÁßÅÊúâÁΩëÁªúÂª∫Á´ãËøûÊé•„ÄÇAWS Site-to-Site VPNÊ≠£ÊòØAWSÊèê‰æõÁöÑÊâòÁÆ°VPNÊúçÂä°ÔºåÊàë‰ª¨ÂèØ‰ª•Âú®Âè¶‰∏ÄÁ´ØÁöÑÁßÅÊúâÁΩëÁªúÈÄöËøáOpenswanÂêåAWS VPCÁΩëÁªúÂª∫Á´ãÂü∫‰∫éIPSecÂçèËÆÆÁöÑÂÆâÂÖ®ËøûÊé•„ÄÇ\n‰∏ãÈù¢ÊòØÈÖçÁΩÆÁöÑËØ¶ÁªÜÊ≠•È™§Ôºå\nÂ¶ÇÊûúÊòØÂàõÂª∫Êï∞ÊçÆ‰∏≠ÂøÉÊàñÂäûÂÖ¨ÂÆ§ÁöÑËøûÊé•ÔºåÊï∞ÊçÆ‰∏≠ÂøÉÊàñÂäûÂÖ¨ÂÆ§ÈúÄË¶ÅÊúâÂÖ¨ÁΩëIP„ÄÇÂ¶ÇÊûúÊòØÂú®ÂÖ∂‰ªñÂÖ¨Êúâ‰∫ë‰∏äÔºåÈúÄË¶ÅÂàõÂª∫Â∏¶ÂÖ¨ÁΩëIPÁöÑEC2ÔºåÊàñ‰ΩøÁî®EIP„ÄÇ Â¶ÇÊûú‰ΩøÁî®AWS EC2ÈÖçÁΩÆOpenswanÔºåÈúÄË¶ÅÁ¶ÅÁî® EC2 ÁöÑ Source/Destination Check„ÄÇ Âú®AWS‰∏äÂàõÂª∫Virtual Private Gateway Âíå Customer Gateway(ÊåáÂÆöÂØπÁ´ØÁöÑÂÖ¨ÁΩëIP‰Ωú‰∏∫ÈùôÊÄÅË∑ØÁî±)„ÄÇ Âú®AWS‰∏äÂàõÂª∫Site-to-Site VPNËøûÊé•Ôºå‰ΩøÁî®Á¨¨‰∏ÄÊ≠•ÂíåÁ¨¨‰∫åÊ≠•ÂàõÂª∫ÁöÑVirtual Private GatewayÂíåCustomer Gateway„ÄÇ Âú®ÂØπÁ´ØÊú∫Âô®‰∏äÂÆâË£Öopenswan„ÄÇ 1sudo yum install openswan ÁºñËæë/etc/sysctl.confÊñá‰ª∂ÔºåÁ°Æ‰øùÊúâ‰ª•‰∏ãÈÖçÁΩÆÔºå 1net.ipv4.ip_forward = 1 2net.ipv4.conf.default.rp_filter = 0 3net.ipv4.conf.default.accept_source_route = 0 ÈáçÊñ∞Âä†ËΩΩsysctlÈÖçÁΩÆÂπ∂ÈáçÂêØnetworkÊúçÂä°„ÄÇ 1sudo sysctl -p 2sudo service network restart ÁºñËæë/etc/ipsec.confÁ°Æ‰øùinclude /etc/ipsec.d/*.confÊ≤°ÊúâË¢´Ê≥®Èáä„ÄÇ ÂàõÂª∫/etc/ipsec.d/aws.confÊñá‰ª∂ÔºåÂÜÖÂÆπÊã∑Ë¥ùÊù•Ëá™Á¨¨‰∏âÊ≠•ÂàõÂª∫ÁöÑËøûÊé•OpenswanÂª∫ËÆÆÈÖçÁΩÆ„ÄÇ ÂàõÂª∫/etc/ipsec.d/aws.secretsÊñá‰ª∂ÔºåÂÜÖÂÆπÊã∑Ë¥ùÊù•Ëá™Á¨¨‰∏âÊ≠•ÂàõÂª∫ÁöÑËøûÊé•OpenswanÈÖçÁΩÆ„ÄÇ ÂêØÂä®ipsecÊúçÂä°„ÄÇ 1# Start the ipsec service. 2sudo service ipsec start 3 4# Check the logs. 5sudo service ipsec status 6sudo ipsec auto --status ‰ª•‰∏äÈÖçÁΩÆÂú®Amazon Linux, Centos 6.9‰∏äÈ™åËØÅÈÄöËøá„ÄÇ‰ΩÜÊòØÂú®Amazon Linux 2„ÄÅCentos 7Á≠âËæÉÊñ∞ÁöÑLinuxÂèëË°åÁâàÊú¨‰∏äÔºåÂêØÂä®ipsecÊúçÂä°ÈÅáÂà∞Â¶Ç‰∏ãÈîôËØØÔºå 1Starting Internet Key Exchange (IKE) Protocol Daemon for IPsec... 2ERROR: /etc/ipsec.d/aws.conf: 12: keyword auth, invalid value: esp Ëß£ÂÜ≥ÊñπÊ≥ïÊòØÔºå‰ªé AWS Site-to-Site VPN ‰∏ãËΩΩÁöÑ Openswan ÈÖçÁΩÆ‰∏≠Âà†Êéâ‰∏çÊîØÊåÅÁöÑÈÖçÁΩÆË°åauth=esp„ÄÇ\n","link":"https://kane.mx/posts/2019/using-openswan-connect-aws-vpn/","section":"posts","tags":["AWS","AWS VPN","Site-to-Site VPN","Openswan"],"title":"‰ΩøÁî®OpenswanËøûÊé•AWS VPC"},{"body":"Infrastructure as Code(Êû∂ÊûÑÂç≥‰ª£Á†Å)‰∏ÄÁõ¥ÊòØË°°ÈáèÂÖ¨Êúâ‰∫ëÊòØÂê¶ÊîØÊåÅËâØÂ•ΩËøêÁª¥ËÉΩÂäõÁöÑÈáçË¶ÅÊåáÊ†á„ÄÇ‰Ωú‰∏∫‰∫ëËÆ°ÁÆóÈ¢ÜÂÖàÁöÑAWSÔºåÈÄöËøáÊúçÂä°CloudFormationÊù•ÁºñÊéí‰∫ëÁéØÂ¢É‰∏≠ÁöÑÂü∫Á°ÄËÆæÊñΩËµÑÊ∫ê„ÄÇ‰∏çËøáÁî±‰∫éCloudFormationÊòØ‰ΩøÁî®YAML/JSONÁºñÂÜôÁöÑÂ£∞ÊòéÂºèËØ≠Ë®ÄÔºå‰∏çÂñÑ‰∫éÂ§ÑÁêÜÈÄªËæëÔºåÁºñÂÜôÁπÅÁêê‰∏î‰∏çÂà©‰∫éË∞ÉËØïÊéíÈîôÔºåÂØπ‰∫éÊñ∞‰∏äÊâãÁöÑDevopsÂ∑•Á®ãÂ∏àÊù•ËØ¥‰πüÊúâ‰∏çÂ∞èÁöÑÂ≠¶‰π†Êõ≤Á∫ø„ÄÇ‰∏âÊñπÂºÄÊ∫êÁöÑÂ∑•ÂÖ∑TerraformÂêåÊ†∑Ê≤°ÊúâÂæàÂ•ΩËß£ÂÜ≥CloudFormationÂ≠òÂú®ÁöÑËøô‰∫õÈóÆÈ¢ò„ÄÇ\nAWS CDKÁöÑÂá∫Áé∞Ëß£ÂÜ≥‰∫ÜÁõÆÂâçCloudFormationÂ≠òÂú®ÁöÑÁªùÂ§ßÈÉ®ÂàÜÈóÆÈ¢òÔºåÊûÅÂ§ßÁöÑÊèêÂçáÂü∫Á°ÄËÆæÊñΩÁºñÊéí‰ª£Á†ÅÁöÑÂºÄÂèëÂíåÁª¥Êä§ÊïàÁéá„ÄÇ\nAWS CDKÊòØ‰∏ÄÁßçÂºÄÊ∫êËΩØ‰ª∂ÂºÄÂèëÊ°ÜÊû∂ÔºåÂºÄÂèëËÄÖÂèØ‰ª•Áî®Ëá™Â∑±‰ΩøÁî®ÁÜüÊÇâÁöÑÁºñÁ®ãËØ≠Ë®ÄÊ®°ÊãüÂíåÈ¢ÑÁΩÆ‰∫ëÂ∫îÁî®Á®ãÂ∫èËµÑÊ∫êÔºåÁõÆÂâçÊîØÊåÅTypescript/Javascript„ÄÅPython„ÄÅJavaÂíå.Net„ÄÇAWS CDKÂ∞Ü‰∫ë‰∏≠ËµÑÊ∫êÊäΩË±°ÂØπË±°ÂåñÔºåÈÄöËøáÊûÅÂÖ∂ÁÆÄÂçïËØ≠Ê≥ïÊèèËø∞ËµÑÊ∫êÂØπË±°ÊàñËÆæÁΩÆÂÖ∂ÂêÑÁßçÂ±ûÊÄß(ÈáçËΩΩCDKÈªòËÆ§Â±ûÊÄßËÆæÁΩÆ)Êù•ÂàõÂª∫ÊàñÊõ¥Êñ∞‰∫ë‰∏≠ËµÑÊ∫ê„ÄÇ\n‰æãÂ¶ÇÔºå‰∏ãÈù¢ÁÆÄÂçïÂá†Ë°åÂ∞ÜÂàõÂª∫‰∏Ä‰∏™Êñ∞ÁöÑÂêç‰∏∫GamedayÁöÑVPCÁΩëÁªúÔºåÂπ∂‰∏îË∑®‰∫Ü‰∏§‰∏™ÂèØÁî®Âå∫ÂàÜÂà´ÂàõÂª∫‰∫ÜÂÖ¨ÊúâÂ≠êÁΩëÂíåÁßÅÊúâÂ≠êÁΩë„ÄÇ\n1 this.vpc = new ec2.Vpc(this, \u0026#39;Gameday\u0026#39;, { 2 cidr: \u0026#39;10.0.0.0/16\u0026#39;, 3 maxAzs: 2, 4 subnetConfiguration: [ 5 { 6 cidrMask: 24, 7 name: \u0026#39;Public\u0026#39;, 8 subnetType: SubnetType.PUBLIC 9 }, 10 { 11 cidrMask: 24, 12 name: \u0026#39;Private\u0026#39;, 13 subnetType: SubnetType.PRIVATE 14 } 15 ] 16 }); ÊàëÂàõÂª∫‰∫Ü‰∏§‰∏™Á§∫‰æãÈ°πÁõÆ‰ΩøÁî®‰∫ÜAWS CDKÂø´ÈÄüÂàõÂª∫Â∫îÁî®ÁéØÂ¢É‰∏îÈÉ®ÁΩ≤Â∫îÁî®Ôºå\nGameday ‰∏∫‰∏Ä‰∏™ECS‰∏äËøêË°åÁöÑWebÂ∫îÁî®ÁºñÊéí‰∫ÜÂÆåÊï¥ÁöÑÁéØÂ¢ÉÔºåÂåÖÊã¨VPC„ÄÅRDS Aurora„ÄÅNAT Gateway„ÄÅÂÆâÂÖ®ÁªÑ„ÄÅECSÈõÜÁæ§„ÄÅECS TaskÂÆö‰πâ„ÄÅALBË¥üËΩΩÂùáË°°„ÄÇ Serverlss Domain Redirect Âü∫‰∫éAWSÊê≠Âª∫‰∫ÜÊó†ÊúçÂä°Âô®Êû∂ÊûÑÁöÑÂüüÂêçÈáçÂÆöÂêëÊúçÂä°„ÄÇÂü∫‰∫é‰∏çÂêåÁöÑÈÖçÁΩÆÂèÇÊï∞ÔºåÊèê‰æõ‰∫ÜÂü∫‰∫é S3 + CloudFront + Route 53 ÊàñÊòØ Lambda + API Gateway + Route 53 ‰∏§ÁßçËß£ÂÜ≥ÊñπÊ°à„ÄÇ ÊÄª‰ΩìÁöÑÊù•ËØ¥ÔºåAWS CDKÊòØ‰∏Ä‰∏™ÈùûÂ∏∏ÂÄºÂæóÈááÁî®ÁöÑ‰∫ë‰∏≠ËµÑÊ∫êÁºñÊéíÂíåÁÆ°ÁêÜÊñπÂºèÔºåÈ´òÊïàÁöÑÁÆ°ÁêÜ‰∫ÜAWS‰∏äÁöÑËµÑÊ∫ê„ÄÇ\nÁî±‰∫éCDKËøòÂú®Áõ∏ÂØπÊó©ÊúüÔºåÊàêÁÜüÂ∫¶Ëøò‰∏çÊòØÈÇ£‰πàÂÆåÁæé„ÄÇÊàëÂú®‰ΩøÁî®‰∏≠ÂèëÁé∞‰∏ãÈù¢‰∏Ä‰∫õÂÄºÂæóÊ≥®ÊÑèÁöÑÈóÆÈ¢ò„ÄÇ\nCDKÁ®ãÂ∫èÊúÄÁªàËøòÊòØÂàõÂª∫‰∫ÜCloudFormationÈÖçÁΩÆÔºåÊèê‰∫§Âà∞CloudFormationÂÆåÊàêËµÑÊ∫êÂèòÊõ¥„ÄÇÊ†∏ÂøÉÁöÑÁî®Êà∑‰ΩìÈ™åÔºåÈúÄË¶Å‰æùËµñCloudFormationÁöÑËÉΩÂäõ„ÄÇCloudFormationÁöÑÂàõÂª∫ÊàñÂõûÈÄÄË∂ÖÊó∂ËøáÈïøÔºåÊó∂Â∏∏ÂΩ±ÂìçËµÑÊ∫êÈÉ®ÁΩ≤‰ΩìÈ™å„ÄÇÂè¶Â§ñÔºåÊ∏ÖÁêÜËµÑÊ∫êÁöÑÊó∂ÂÄôÔºåÈÅáÂà∞ÈÉ®ÂàÜËµÑÊ∫êÊó†Ê≥ïÊ∏ÖÁêÜ‰∏îÁº∫Â∞ëÊòéÁ°ÆÊèêÁ§∫„ÄÇÊØîÂ¶ÇAuroraÈõÜÁæ§„ÄÇ CDKÁ±ªÂ∫ìÁº∫Â∞ëÈÖçÁΩÆÊ†°È™å„ÄÇËøôÁ±ªÈîôËØØÂè™ËÉΩÈÄöËøáCloudFormationÈÉ®ÁΩ≤ÂêéÔºåÊâç‰ºöË¢´ËµÑÊ∫êÊñπÂèëÁé∞Âπ∂ËøîÂõûÈîôËØØ„ÄÇÂØºËá¥Êï¥‰∏™ÂàõÂª∫ÁöÑÂ†ÜÊ†àÂõûÈÄÄÔºåË∞ÉËØïÂ§ßÂûãÁöÑÈÉ®ÁΩ≤Ê†àÂ∞ÜËä±Ë¥πÊØîËæÉÈïøÁöÑÊó∂Èó¥„ÄÇÂª∫ËÆÆÂ∞ÜÊï¥‰∏™ÈÉ®ÁΩ≤ÊãÜÂàÜ‰∏∫Â§ö‰∏™Â∞èÁöÑÂ†ÜÊ†àÔºåÂáèÂ∞èÊØèÊ¨°ÈÉ®ÁΩ≤Êó∂Èó¥ÔºåÊñπ‰æøË∞ÉËØï„ÄÇ ÊñáÊ°£ËøòÊØîËæÉÁÆÄÈôã„ÄÇÁº∫Â∞ëËæÉ‰∏∫Ê∑±ÂÖ•ÁöÑÁ§∫‰æã„ÄÇÂ¢ûÂä†‰∫ÜÂºÄÂèë‰∫∫ÂëòÁöÑÂ≠¶‰π†Êõ≤Á∫ø„ÄÇ Êñ∞ÁâàÊú¨ÂêëÂêéÂÖºÂÆπÊÄß‰∏çÂ§üÂ•ΩÔºåÊó∂Â∏∏Êñ∞ÁâàÊú¨Êúâbreak changes„ÄÇÂú®1.0GA‰πãÂêéÂèëÂ∏ÉÁöÑÁâàÊú¨break changesÁõ∏ÂØπÂáèÂ∞ëÔºå‰ΩÜ‰ªçÁÑ∂ÊúâÂá∫Áé∞„ÄÇ ","link":"https://kane.mx/posts/2019/aws-cdk/","section":"posts","tags":["AWS","AWS CDK","Infrastructure as Code"],"title":"AWS CDKÁÆÄ‰ªã"},{"body":"","link":"https://kane.mx/tags/aws-s3/","section":"tags","tags":null,"title":"AWS S3"},{"body":"‰∏öÂä°Êó∂Â∏∏ÊúâÈúÄÊ±ÇÂ∞ÜÊüê‰∏™ÂüüÂêç(A)ÁöÑËÆøÈóÆÈáçÂÆöÂêëÂà∞ÂÖ∂‰ªñÂüüÂêç(B)ÔºåÂç≥‰ΩøÂÆûÁé∞ËøôÊ†∑‰∏Ä‰∏™ÂæàÁÆÄÂçïÁöÑÈúÄÊ±ÇÈÄöÂ∏∏‰πüÈúÄË¶ÅÈÉ®ÁΩ≤WebÊúçÂä°Âô®Ôºà‰æãÂ¶ÇNginxÔºâÔºå‰∏∫ÂüüÂêçAÁöÑËØ∑Ê±ÇËøîÂõû302ÂìçÂ∫îÔºåÂπ∂Êèê‰æõÊñ∞ÁöÑLocationÂú∞ÂùÄÈáçÂÆöÂêëÂà∞ÂüüÂêçB„ÄÇÁé∞Âú®Âü∫‰∫é‰∫ëËÆ°ÁÆóÊúçÂä°ÔºåÊàë‰ª¨ÂèØ‰ª•‰ΩøÁî®‰∏Ä‰∫õÊâòÁÆ°ÊúçÂä°Êù•ÂÆûÁé∞ÂêåÊ†∑ÁöÑ‰∫ãÊÉÖÔºåÊó†ÈúÄÁÆ°ÁêÜÊúçÂä°Âô®ÂíåÁª¥Êä§Â∫îÁî®ÔºåÂêåÊó∂ÂÅöÂà∞ÊúÄ‰ΩéÊàêÊú¨ÂÆûÁé∞ËØ•ÈúÄÊ±Ç„ÄÇ\nÊé•‰∏ãÊù•Â∞Ü‰ªãÁªçÂ¶Ç‰ΩïÂà©Áî®AWS‰∏äÁöÑÊúçÂä°ÂÆûÁé∞ËØ•ÈúÄÊ±Ç„ÄÇ\n‰ΩøÁî®AWS S3ÂíåAWS CloudFrontÂÆûÁé∞ÂüüÂêçÈáçÂÆöÂêë ÂàõÂª∫‰∏Ä‰∏™Êñ∞ÁöÑS3 bucketÔºå‰æãÂ¶Ç redirect.domain.com ÈÖçÁΩÆÊñ∞bucketÂ±ûÊÄßÔºåÂºÄÂêØÈùôÊÄÅÁΩëÁ´ôÊâòÁÆ°ÔºåÂêåÊó∂ÈÖçÁΩÆ‰∏∫ÈáçÂÆöÂêëËØ∑Ê±ÇÂà∞ÊúüÊúõÁöÑÂüüÂêç redirected-host.domain.com ÂàõÂª∫Êñ∞ÁöÑCloudFrontÂàÜÂèëÔºåËÆæÁΩÆÁ¨¨‰∏ÄÊ≠•ÂàõÂª∫ÁöÑS3 bucket‰Ωú‰∏∫Ëá™ÂÆö‰πâÊ∫êÁ´ô(‰∏çÂèØ‰ª•ÈÖçÁΩÆÊ∫êÁ´ô‰∏∫S3 bucket)„ÄÇÂπ∂‰∏îÈÖçÁΩÆ‰ΩøÁî®Ëá™ÂÆö‰πâÂüüÂêç redirect.domain.com„ÄÇÊ≥®ÊÑèÔºåÈÖçÁΩÆËá™ÂÆö‰πâCNamesÈúÄË¶ÅÊèê‰æõÂüüÂêçÂØπÂ∫îÁöÑSSLËØÅ‰π¶ÔºåÂèØ‰ª•‰ΩøÁî®AWS Certificate ManagerÂàõÂª∫ÂÖçË¥πÁöÑSSL/TLSËØÅ‰π¶ Âú®ÂüüÂêçdomain.comËß£ÊûêÊúçÂä°ÂïÜ‰∏∫ÂüüÂêçredirect.domain.comÂàõÂª∫Êñ∞ÁöÑËß£ÊûêËÆ∞ÂΩï ‰ΩøÁî®AWS LambdaÂíåAPI GatewayÂÆûÁé∞ÂüüÂêçÈáçÂÆöÂêë ÂàõÂª∫‰∏Ä‰∏™LambdaÂáΩÊï∞Êù•ËøîÂõû302ËØ∑Ê±ÇÊàñËÄÖHTMLÈ°µÈù¢ÔºåÂú®È°µÈù¢‰∏≠ÈÄöËøáJavascriptÂÆûÁé∞ÈáçÂÆöÂêëÈ°µÈù¢ ‰∏∫ËØ•LambdaÂáΩÊï∞ÂàõÂª∫API GatewayËß¶ÂèëÂô® ‰∏∫ËØ•API GatewayÊé•Âè£ÂàõÂª∫Ëá™ÂÆö‰πâÂüüÂêç Âú®ÂüüÂêçdomain.comËß£ÊûêÊúçÂä°ÂïÜ‰∏∫ÂüüÂêçredirect.domain.comÂàõÂª∫Êñ∞ÁöÑËß£ÊûêËÆ∞ÂΩï ÊàëÂàõÂª∫‰∫Ü‰∏Ä‰∏™Âü∫‰∫éAWS CDKÁöÑGithubÈ°πÁõÆÔºåÂà©Áî®AWS Infrastructure as CodeÁöÑÂº∫Â§ßËÉΩÂäõ‰∏ÄÈîÆÈÉ®ÁΩ≤‰ª•‰∏ä‰∏§ÁßçÊó†ÊúçÂä°Âô®ÁéØÂ¢ÉÔºåÊúâÈúÄË¶ÅÁöÑÂèØ‰ª•‰Ωú‰∏∫ÂÆûÁé∞ÂèÇËÄÉ„ÄÇ\n","link":"https://kane.mx/posts/effective-cloud-computing/serverless-domain-redirect/","section":"posts","tags":["‰∫ëËÆ°ÁÆó","AWS","AWS S3","AWS Lambda","AWS CDK"],"title":"Êó†ÊúçÂä°Âô®Êû∂ÊûÑÁöÑÂüüÂêçÈáçÂÆöÂêëÊúçÂä°"},{"body":"","link":"https://kane.mx/tags/amazon-alexa/","section":"tags","tags":null,"title":"Amazon Alexa"},{"body":"ËøëÊúüÈúÄË¶ÅÂÅö‰∏Ä‰∫õAlexa‰∏äÁöÑÂºÄÂèëÔºåÂú®ÊâãÊú∫‰∏äÂÆâË£Ö‰∫ÜAmazon AlexaÔºå‰∏ÄÁõ¥ÂæóÂà∞‰∏ãÈù¢ËøôÊ†∑ÁöÑÈîôËØØÊèêÁ§∫ËÄåÊó†Ê≥ïÁôªÂΩï„ÄÇ\nConnection Timed Out.\nÂÖàÂêéÂ∞ùËØï‰∫ÜÁøªÂ¢ô„ÄÅÊõ¥ÊîπËØ≠Ë®ÄÁ≠âÊñπÊ≥ï‰ªçÁÑ∂‰∏çÂèØÁôªÂΩï„ÄÇÂπ∂‰∏îÂú®ÁΩëÁªú‰∏ä‰πüÊ≤°ÊúâÊâæÂà∞ÂèØÁî®ÁöÑÊñπÊ°àÔºåÂÜ≥ÂÆöÊäìÂåÖÁ†îÁ©∂‰∏ã‰∏∫‰ªÄ‰πàÊàëÁöÑË¥¶Âè∑ÂßãÁªàÊó†Ê≥ïÁôªÂΩï„ÄÇ\nÈÄöËøáÊäìÂèñAlexaÁôªÂΩïÊó∂ÂèëÈÄÅÁöÑÊï∞ÊçÆÂåÖÔºåÂèëÁé∞‰ªñËÆøÈóÆ‰∫Üamazon.cnÁ≠âÊï∞‰∏™cnÂüüÂêç‰∏ãÁöÑ‰∏ÄÁ≥ªÂàóÊúçÂä°ÔºåÁúãÊù•Ëøô‰∫õÊúçÂä°‰∏≠ÈÉ®ÂàÜÂ∑≤Êó†Ê≥ïÊèê‰æõÊ≠£Â∏∏ÊúçÂä°ÔºåÂØºËá¥ÁôªÂΩï‰∏ÄÁõ¥Âá∫Áé∞‰∏äÈù¢ÁöÑÈîôËØØ„ÄÇ\nAmazon Alexa‰Ωú‰∏∫‰∏Ä‰∏™ÊúçÂä°ÂÖ®ÁêÉÁî®Êà∑ÁöÑappÔºåÂ∫îËØ•ÊòØÂà§Êñ≠ÊâãÊú∫Áî®Êà∑Âú®Â§ßÈôÜÂú∞Âå∫Âêé‰ΩøÁî®‰∫ÜÈÖçÁΩÆÂú®Â§ßÈôÜÂú∞Âå∫ÁöÑËøô‰∫õÊúçÂä°„ÄÇ\n‰∏¥Êó∂Ëß£ÂÜ≥ÊñπÊ°àÁöÑÊÄùË∑ØÂ∞±ÊòØËÆæÁΩÆÁ≥ªÁªüÊàñappÔºåËÆ©‰ªñÊó†Ê≥ïËé∑ÂèñÂà∞ÊâãÊú∫ÁúüÂÆûÊâÄÂú®ÁöÑÂú∞ÁêÜ‰ΩçÁΩÆÔºåÈÇ£‰πàAlexa app‰ºö‰ΩøÁî®ÈªòËÆ§ÁöÑÂÖ®ÁêÉÊúçÂä°Âô®Êù•ËØ∑Ê±ÇÊï∞ÊçÆ„ÄÇ\n‰ª•‰∏ãÊòØ‰∏¥Êó∂Ëß£ÂÜ≥ÊñπÊ°àÁöÑÊ≠•È™§Ôºå\n‰ªéPlayÂ∏ÇÂú∫ÂÆâË£ÖAlexa app„ÄÇÂ¶ÇÊûúÂ∑≤ÂÆâË£ÖÊ∏ÖÁ©∫appÊï∞ÊçÆ„ÄÇ Á¶ÅÁî®app LocationÊùÉÈôê(ÈªòËÆ§Â∞±ÊòØÁ¶ÅÁî®ÁöÑ)„ÄÇ Êõ¥ÊîπÁ≥ªÁªüËØ≠Ë®Ä‰∏∫Ëã±ÊñáÔºåËÆæÁΩÆÊó∂Âå∫‰∏∫‰ªªÊÑèÁæéÂõΩÊó∂Âå∫„ÄÇ ÊãîÊéâSIMÂç°ÔºåÊàñËÄÖÁ¶ÅÁî®ÊâÄÊúâSIMÂç°„ÄÇ ÊâìÂºÄAlexa appÔºå‰ΩøÁî®Â∑≤ÊúâÊàñÊñ∞Ê≥®ÂÜåAmazonË¥¶Âè∑Âç≥ÂèØÁôªÂΩï„ÄÇ\n","link":"https://kane.mx/posts/2019/alexa-login-issue/","section":"posts","tags":["AWS","Amazon Alexa","Troubleshoot"],"title":"Amazon Alexa AndroidÁâàÊú¨ÂõΩÂÜÖÁôªÂΩïÈóÆÈ¢ò"},{"body":"","link":"https://kane.mx/tags/troubleshoot/","section":"tags","tags":null,"title":"Troubleshoot"},{"body":"‰∏™‰∫∫ÁîµËÑëÊï∞ÊçÆÂ§á‰ªΩ‰∏ÄÁõ¥ÈÉΩÊòØ‰∏Ä‰∏™Âº∫ÁÉàÁöÑÈúÄÊ±Ç„ÄÇ‰ΩøÁî®ÁΩëÁõòÁ≠â‰∫ëÂ≠òÂÇ®‰∫ßÂìÅÂèØ‰ª•ÈÉ®ÂàÜÊª°Ë∂≥Êï∞ÊçÆÁöÑÂ§á‰ªΩÈúÄÊ±ÇÔºå‰ªçÁÑ∂Êó†Ê≥ïÂÅöÂà∞‰ΩøÁî®‰æøÂà©ÊÄßÂíåÂæàÈ´òÁöÑÊï∞ÊçÆÂÆâÂÖ®‰øùÈöú„ÄÇ\nMacOSX‰∏äÁ≥ªÁªüÂÜÖÁΩÆ‰∫ÜÂ§á‰ªΩËß£ÂÜ≥ÊñπÊ°à -- Êó∂Èó¥Êú∫Âô®(Time Machine)„ÄÇTime MachineÊîØÊåÅAirPort Time CapsuleÔºåNASÂ≠òÂÇ®ÊàñËÄÖÂ§ñÁΩÆÁöÑÂ≠òÂÇ®ËÆæÂ§á„ÄÇÁÑ∂ËÄåËøô‰∫õÂ§á‰ªΩÊñπÊ°àÈÉΩ‰æùËµñ‰∫éÁ°¨‰ª∂ËÆæÂ§áÔºåÊúâÂÆπÈáèÈôêÂà∂Êàñ‰∏ç‰æø‰∫éÁßªÂä®„ÄÇÂú®‰∫ëËÆ°ÁÆóÂ∑≤ÁªèÂ§ßË°åÂÖ∂ÈÅìÁöÑ‰ªäÂ§©ÔºåÊúâÊ≤°Êúâ‰ΩøÁî®‰∫ëËÆ°ÁÆóÂéÇÂïÜÂØπË±°Â≠òÂÇ®‰Ωú‰∏∫ÁõÆÊ†áÂ≠òÂÇ®ÁöÑÂ§á‰ªΩÊñπÊ°àÔºå‰∏∫MacOSXÊï∞ÊçÆÂ§á‰ªΩÊèê‰æõÊó†ÈôêÂÆπÈáè„ÄÅÈ´òÂ∫¶ÁöÑÂÆâÂÖ®ÊÄßÁöÑ‰∫ëÊñπÊ°àÔºüÁªèËøá‰∏ÄÁï™ÊêúÁ¥¢ÔºåÊó¢ÊâæÂà∞‰∫ÜÂºÄÊ∫êÂÖçË¥πÁöÑÂ∑•ÂÖ∑ResticÔºå‰πüÊúâ‰ªòË¥πËΩØ‰ª∂Arq„ÄÇÊó†ËÆ∫ResticËøòÊòØArqÊèê‰æõÁöÑÊòØÁã¨Á´ãÁöÑ‰∏âÊñπÂ∑•ÂÖ∑Êù•ÂÆûÁé∞Â§á‰ªΩÂà∞‰∫ëÁ´ØÂ≠òÂÇ®Êàñ‰ªé‰∫ëÁ´ØÊÅ¢Â§çÔºåÊúâÊ≤°ÊúâÂ∞ÜTime MachineÂíå‰∫ëÁ´ØÂÇ®Â≠òÁªìÂêàÂú®‰∏ÄËµ∑ÁöÑÊñπÊ°àÂë¢Ôºü\nTimeMchineÊîØÊåÅÂ∞ÜÂ§ñÁΩÆÂ≠òÂÇ®‰Ωú‰∏∫Â§á‰ªΩËÆæÂ§áÔºåËøôÈáå‰ªãÁªçÁöÑÊñπÊ≥ïÂ∞±ÊòØÂ∞ÜËøúÁ´Ø‰∫ëËÆ°ÁÆóÂéÇÂïÜÁöÑÂØπË±°Â≠òÂÇ®ÊåÇËΩΩ‰∏∫Êú¨Âú∞ËÆæÂ§áÔºåËÆæÁΩÆTime MachineÂ∞ÜÂÆÉ‰Ωú‰∏∫ÁõÆÊ†áÂ§á‰ªΩËÆæÂ§áÔºåÂÆûÁé∞Â∞ÜÂ§á‰ªΩÊîæÂà∞‰∫ëÂéÇÂïÜÁöÑÂØπË±°ÂÇ®Â≠ò„ÄÇ\nÊé•‰∏ãÊù•ÊàëÂ∞Ü‰∏ÄÊ≠•Ê≠•ÊºîÁ§∫Â¶Ç‰ΩïÂ∞ÜAWS S3ÂØπË±°Â≠òÂÇ®ÁöÑbucket‰Ωú‰∏∫Time MachineÂ§á‰ªΩÁöÑËÆæÂ§á„ÄÇ\nÊ≠§ÊñπÊ≥ïÈÄÇÁî®‰∫éÂ∞Ü‰ªªÊÑè‰∫ëÂéÇÂïÜÁöÑÂØπË±°Â≠òÂÇ®‰Ωú‰∏∫Â§á‰ªΩÂ≠òÂÇ®ÔºåÂè™Ë¶ÅËØ•ÂéÇÂïÜÁöÑÂØπË±°Â≠òÂÇ®ÊîØÊåÅË¢´MacOSXÊåÇËΩΩ‰∏∫Êú¨Âú∞Á£ÅÁõò„ÄÇ\nÊúâÂæàÂ§öÊàêÁÜüÁöÑÊñπÊ°àÂ∞ÜAWS S3ÊåÇËΩΩ‰∏∫MacOSXÁ£ÅÁõòÔºå‰æãÂ¶ÇS3fs„ÄÅGoofys„ÄÇÊú¨ÊñáÊé®ËçêÁöÑÊñπÊ°àÊòØJuicefsÔºåJuicefs‰∏∫ÂØπË±°Â≠òÂÇ®ÁöÑÂÖÉÊï∞ÊçÆÊèê‰æõ‰∫ÜÁºìÂ≠òÔºåËÉΩÊûÅÂ§ßÁöÑ‰ºòÂåñÂØπÊåÇËΩΩÁ£ÅÁõòÁöÑlistÔºågetÁ≠âÊìç‰Ωú„ÄÇ\nÈ¶ñÂÖàÊåâÁÖßJuicefs ÊñáÊ°£ÂÆâË£ÖÂøÖË¶ÅÁöÑ‰æùËµñÂíåJuicefsÂÆ¢Êà∑Á´Ø„ÄÇÊé•‰∏ãÊù•Âú®JuicefsÊ≥®ÂÜåÂÆåÊàêÂêéÔºåÂàõÂª∫‰∏Ä‰∏™Êñá‰ª∂Á≥ªÁªü‰øùÂ≠òÂ§á‰ªΩÊï∞ÊçÆ„ÄÇÊ≥®ÊÑèÔºöËøôÈáåÁöÑbucketÂêçÁß∞ÈúÄË¶ÅÂêåÈöèÂêéÂàõÂª∫ÊàñÂ∑≤ÊúâÁöÑbucketÂêçÁß∞‰∏ÄËá¥„ÄÇ ÂàõÂª∫Êñ∞ÁöÑAWS S3 bucket(ÊàñËÄÖ‰ΩøÁî®Â∑≤ÊúâÁöÑbucket)ÔºåÂêåÊó∂‰∏∫ËØ•bucket‰∏ìÈó®ÂàõÂª∫Áî®‰∫éJuicefsÂÆ¢Êà∑Á´Ø‰ΩøÁî®ÁöÑIAMÁî®Êà∑„ÄÇÂº∫ÁÉàÂª∫ËÆÆ‰∏çË¶Å‰ΩøÁî®‰∫ëÂ∏êÂè∑ÁöÑaccess tokenÁî®‰∫éÊåÇËΩΩÔºåÊúÄ‰Ω≥ÂÆûË∑µÊòØ‰∏∫‰∏çÂêåÁöÑÁî®ÈÄîÂàõÂª∫ÂçïÁã¨ÁöÑIAMÁî®Êà∑„ÄÇÊõ¥Â§öIAMÁî®Êà∑ÂÆûË∑µÔºåËØ∑ÂèÇËÄÉÊñáÁ´†IAMÊúÄ‰Ω≥ÂÆûË∑µ„ÄÇ‰∏ãÈù¢ÊòØ‰ΩøÁî®AWS CLIÂàõÂª∫Êñ∞S3 bucketÂèäIAMÁî®Êà∑ÁöÑÂèÇËÄÉÂëΩ‰ª§Ôºå 1# ÂàõÂª∫S3 bucket 2aws s3 mb s3://my-bucket-for-mac-backup 3 4# ÂàõÂª∫IAMÁî®Êà∑ 5aws iam create-user --user-name juicefs 6# ‰∏∫juicefsÁî®Êà∑Êéà‰∫àËØªÂÜôÂ§á‰ªΩS3 bucketÊùÉÈôê 7echo \u0026#39;{ 8 \u0026#34;UserName\u0026#34;: \u0026#34;juicefs\u0026#34;, 9 \u0026#34;PolicyName\u0026#34;: \u0026#34;mac-backup-bucket-all-permissions\u0026#34;, 10 \u0026#34;PolicyDocument\u0026#34;: \u0026#34;{ \\\u0026#34;Version\\\u0026#34;: \\\u0026#34;2012-10-17\\\u0026#34;, \\\u0026#34;Statement\\\u0026#34;: [ { \\\u0026#34;Effect\\\u0026#34;: \\\u0026#34;Allow\\\u0026#34;, \\\u0026#34;Action\\\u0026#34;: \\\u0026#34;s3:*\\\u0026#34;, \\\u0026#34;Resource\\\u0026#34;: [ \\\u0026#34;arn:aws-cn:s3:::my-bucket-for-mac-backup/*\\\u0026#34;, \\\u0026#34;arn:aws-cn:s3:::my-bucket-for-mac-backup\\\u0026#34; ] } ] }\u0026#34; 11 1 { 12}\u0026#39; \u0026gt; policy.json 13aws iam put-user-policy --cli-input-json file://policy.json 14# ‰∏∫juicefsÁî®Êà∑ÂàõÂª∫access tokenÁî®‰∫éjuicefsÂÆ¢Êà∑Á´ØÊåÇËΩΩbucket 15aws iam create-access-key --user-name juicefs 16{ 17 \u0026#34;AccessKey\u0026#34;: { 18 \u0026#34;UserName\u0026#34;: \u0026#34;juicefs\u0026#34;, 19 \u0026#34;AccessKeyId\u0026#34;: \u0026#34;\u0026lt;key id\u0026gt;\u0026#34;, 20 \u0026#34;Status\u0026#34;: \u0026#34;Active\u0026#34;, 21 \u0026#34;SecretAccessKey\u0026#34;: \u0026#34;\u0026lt;access key\u0026gt;\u0026#34;, 22 \u0026#34;CreateDate\u0026#34;: \u0026#34;2019-06-30T15:25:41Z\u0026#34; 23 } 24} ÊåâÁÖßJuicefsÊñáÊ°£ÊåÇËΩΩÊåÇËΩΩS3 bucket„ÄÇ ËøõÂÖ•ÊåÇËΩΩÂêéÁöÑÁõÆÂΩï(Â¶Ç/jfs)ÔºåÂàõÂª∫Sparse ImageÁî®‰∫éTime MachineÂÜôÂÖ•Â§á‰ªΩ„ÄÇ 1cd /jfs 2hdiutil create -size 600g -type SPARSEBUNDLE -fs \u0026#34;HFS+J\u0026#34; Time Machine.sparsebundle ‰∏äÈù¢ÂëΩ‰ª§Â∞ÜÂàõÂª∫‰∏Ä‰∏™Âêç‰∏∫TimeMachine600 GBÂ§ßÂ∞èÁöÑÈïúÂÉè(ÂàùÂßã‰ªÖÂç†Áî®Êï∞ÁôæMBÔºåÂÆûÈôÖÊñá‰ª∂Á£ÅÁõòÁ©∫Èó¥Âè™ÊúâÂΩìÊñá‰ª∂ÂÜôÂÖ•ÂêéÊâç‰ºöÂç†Áî®)„ÄÇÊ†πÊçÆ‰Ω†ÁöÑÈúÄË¶ÅÈöèÊÑèË∞ÉÊï¥ÈïúÂÉèÂ§ßÂ∞èÔºåÈÄöÂ∏∏Âª∫ËÆÆËÆæÁΩÆ‰∏∫MacÁ£ÅÁõòÂ§ßÂ∞èÁöÑ‰∏§ÂÄç„ÄÇ ‰∏çÁÜüÊÇâÂëΩ‰ª§Ë°åÁöÑÁî®Êà∑Ôºå‰πüÂèØ‰ª•‰ΩøÁî®Á£ÅÁõòÂ∑•ÂÖ∑(Disk Utility)ÂàõÂª∫„ÄÇ ÈÄöËøáFinderÊåÇËΩΩ‰πãÂâçÂàõÂª∫ÁöÑSparse Image Áé∞Âú®ÊòØÈ≠îÊúØÊ≠•È™§ÔºåÂëäËØâTime Machine‰ΩøÁî®‰πãÂâçÂàõÂª∫ÁöÑËôöÊãüËÆæÂ§á‰Ωú‰∏∫Â§á‰ªΩÁ£ÅÁõò„ÄÇ 1sudo tmutil setdestination /Volumes/Time MachineDisk Áî±‰∫éS3 BucketÁî®‰∫éÂ§á‰ªΩÊï∞ÊçÆÔºåÂª∫ËÆÆÂºÄÂêØS3 Êô∫ËÉΩÂàÜÂ±ÇÂ≠òÂÇ®ÊàñËÄÖIAÂÇ®Â≠òÔºåÈôç‰ΩéËä±Ë¥π„ÄÇÂêåÊó∂ÂèØ‰ª•ÂêØÁî®S3 KMSÂä†ÂØÜ‰∫ëÁ´Ø‰øùÂ≠òÁöÑÊï∞ÊçÆÔºåÊèêÂçáÊï∞ÊçÆÂÆâÂÖ®ÊÄß„ÄÇ\n","link":"https://kane.mx/posts/2019/using-s3-as-device-for-mac-time-machine-backup/","section":"posts","tags":["MacOSX","AWS","AWS S3","Tip"],"title":"‰ΩøÁî®AWS S3‰Ωú‰∏∫MacOSXÊó∂Èó¥Êú∫Âô®(Time Machine)ÁöÑÂ§á‰ªΩÂ≠òÂÇ®"},{"body":"","link":"https://kane.mx/tags/dingtalk/","section":"tags","tags":null,"title":"Dingtalk"},{"body":"","link":"https://kane.mx/tags/spring/","section":"tags","tags":null,"title":"Spring"},{"body":"","link":"https://kane.mx/tags/spring-cloud-function/","section":"tags","tags":null,"title":"Spring Cloud Function"},{"body":"Âü∫‰∫éserverlessÊ°ÜÊû∂ÁöÑÈíâÈíâÂõûË∞ÉÂáΩÊï∞‰∏≠‰ªãÁªç‰∫Üserverless frameworkÔºå‰∏ÄÊ¨æÊîØÊåÅË∑®‰∫ëÂéÇÂïÜ/ServerlessÂπ≥Âè∞ÁöÑÈÉ®ÁΩ≤Â∑•ÂÖ∑„ÄÇ‰ΩÜÊòØÂáΩÊï∞‰ª£Á†ÅËøòÊòØÈúÄË¶ÅÈíàÂØπ‰∏çÂêåÁöÑserverlessÂπ≥Âè∞‰ΩúÂØπÂ∫îÁöÑÈÄÇÈÖç„ÄÇËÄåSpring Clound FunctionÂ∞±ÊòØÈíàÂØπËøôÁßçÊÉÖÂÜµ‰∏ìÈó®ÂºÄÂèëÁöÑË∑®serverlessÂπ≥Âè∞ÁöÑÊ°ÜÊû∂ÔºåÂÆûÁé∞‰∏ÄÂ•ó‰ª£Á†ÅÈÄöËøá‰∏çÂêåÁöÑÊâìÂåÖÂÆûÁé∞Ë∑®serverlessÂπ≥Âè∞„ÄÇSpring Clound FunctionÁõÆÂâçÊîØÊåÅAWS Lambda, Microsoft Azure Function‰ª•ÂèäApache OpenWhisk„ÄÇ\nËøôÈáåÊàë‰ª¨ÁªßÁª≠‰ΩøÁî®Êó†ÂáΩÊï∞ÁâàÊú¨ÁöÑÈíâÈíâÂõûÊéâÂáΩÊï∞Êù•ÊºîÁ§∫Spring Clound Function for AWSÁöÑ‰ΩøÁî®„ÄÇ\nÈ¶ñÂÖàÂ∞Üspring cloud function for aws adapterÊ∑ªÂä†Âà∞È°πÁõÆ‰æùËµñÔºå\n1implementation(\u0026#34;org.springframework.cloud:spring-cloud-function-adapter-aws:${springCloudFunctionVersion}\u0026#34;) ÂÖ∂Ê¨°ÂàõÂª∫ÂáΩÊï∞HandlerÔºåÂÆûÁé∞Spring Cloud FunctionË∑®ÂáΩÊï∞ËÆ°ÁÆóÂÆûÁé∞ÊäΩË±°ÁöÑSpringBootRequestHandlerÁ±ªÔºåÊàñËÄÖÊòØÁªßÊâøËá™ÂÆÉÁöÑtriggerÁ±ªÔºå‰æãÂ¶ÇSpringBootApiGatewayRequestHandler\n1import org.springframework.cloud.function.adapter.aws.SpringBootApiGatewayRequestHandler 2 3class Handler : SpringBootApiGatewayRequestHandler() Êé•‰∏ãÊù•ÂàõÂª∫Spring BootÂ∫îÁî®Á®ãÂ∫èÔºåÂπ∂Â∞ÜserverlessÂÆûÁé∞ÂáΩÊï∞Ê≥®ÂÜå‰∏∫Spring BeanÔºåÂáΩÊï∞ÁöÑÂÆûÁé∞ÈÉ®ÂàÜÂ∞±ÊòØserverlessÂáΩÊï∞ÂÖ∑‰ΩìÂÅöÁöÑ‰∏öÂä°ÈÄªËæë„ÄÇ\n1@SpringBootApplication 2open class DingtalkCallbackApplication { 3 4 @Bean 5 open fun dingtalkCallback(): Function\u0026lt;Message\u0026lt;EncryptedEvent\u0026gt;, Map\u0026lt;String, String\u0026gt;\u0026gt; { 6 val callback = Callback() 7 return Function { 8 callback.handleRequest(it) 9 } 10 } 11} 12fun main(args: Array\u0026lt;String\u0026gt;) { 13 SpringApplication.run(DingtalkCallbackApplication::class.java, *args) 14} ÊúÄÂêéÂ∞ÜÂáΩÊï∞ÊâìÂåÖ‰∏∫fat jarÔºàÂ¶ÇÊûúÂ∞Ü‰æùËµñÊâìÂåÖ‰∏∫lambda layerÔºåÂèØ‰∏çÁî®ÊâìÂåÖ‰∏∫fat jarÔºâ‰Ωú‰∏∫lambdaÁöÑ‰ª£Á†Å„ÄÇ\nÂáΩÊï∞ÁöÑÈÉ®ÁΩ≤ÂêåÂÖ∂‰ªñÁöÑlambdaÂáΩÊï∞Ê≤°Êúâ‰ªª‰ΩïÂå∫Âà´ÔºåËøô‰∏™Á§∫‰æã‰∏≠Ê≤øÁî®‰∫Ü‰πãÂâçÁöÑSAM/CloudFormationÈÖçÁΩÆÊàñËÄÖserverless frameworkÈÉ®ÁΩ≤ÈÖçÁΩÆ„ÄÇ\nÂÆåÊï¥ÁöÑÂèØËøêË°å„ÄÅÈÉ®ÁΩ≤‰ª£Á†ÅËØ∑ËÆøÈóÆËøô‰∏™ÂàÜÊîØ„ÄÇ\nÊÄª‰ΩìÊù•ËØ¥ÔºåSpring Clound FunctionÁöÑÂÆûÁé∞ÂéüÁêÜÂπ∂‰∏çÂ§çÊùÇÔºåÂÆö‰πâÁªü‰∏ÄÁöÑÂáΩÊï∞ÂÆûÁé∞ÂÖ•Âè£ÔºåÈÄöËøá‰∏çÂêåserverlessÂπ≥Âè∞ÁöÑadapterÂØπÊé•‰∏çÂêåÂπ≥Âè∞ÁöÑAPIÊé•Âè£ÔºåÂÅöÂà∞ÁºñÂÜô‰∏ÄÊ¨°ÂáΩÊï∞ÂÆûÁé∞ÔºåÈÄöËøáÊâìÂåÖ‰∏çÂêåÁöÑadapterÂÅöÂà∞Ë∑®serverlessÂπ≥Âè∞ËøêË°å„ÄÇ\n‰ΩÜ‰∏™‰∫∫ËÆ§‰∏∫Áé∞ÂÆû‰∏≠ËøôÊ†∑ÁöÑÂú∫ÊôØÂπ∂‰∏çÂ§ö„ÄÇÂπ∂‰∏îserverlessÂáΩÊï∞Ëß¶ÂèëÊñπÂºèÂæàÂ§öÔºå‰æãÂ¶ÇAWS‰∏äÁöÑAPIGateway„ÄÅKinesis„ÄÅCloudWatch„ÄÅIoTÁ≠âÊúçÂä°Ôºå‰∏éËøô‰∫õÊúçÂä°ÂØπÊé•ÊàñAPIË∞ÉÁî®ÂÖ∂ÂÆû‰πü‰∫ßÁîü‰∫ÜËÄ¶ÂêàÔºåÂπ∂‰∏çËÉΩÁÆÄÂçïÁöÑËøÅÁßªÂà∞‰∏âÊñπÁöÑserverlessÂπ≥Âè∞ÂéªÊâßË°å„ÄÇÂêåÊó∂ÔºåÂºÄÂèëËÄÖÈúÄË¶ÅÂºïÂÖ•spring/spring boot/spring cloudÁõ∏ÂÖ≥ÁöÑ‰æùËµñÔºåÂ¢ûÂä†‰∫ÜÁ®ãÂ∫èÁöÑÂ§çÊùÇÂ∫¶ÔºåÂèàÂª∂Èïø‰∫ÜlambdaÂáΩÊï∞clod startÈúÄË¶ÅÁöÑÊó∂Èó¥„ÄÇÂè¶Â§ñÔºåÂºÄÂèëËÄÖÈúÄË¶ÅÂ≠¶‰π†spring cloud functionÁõ∏ÂÖ≥ÁöÑÁü•ËØÜÔºåÊó†ÂΩ¢‰∏≠Â¢ûÂä†‰∫ÜÂ§çÊùÇÂ∫¶„ÄÇÊÄª‰πã‰ΩøÁî®spring cloud function‰Ωú‰∏∫ÂáΩÊï∞ËÆ°ÁÆóÊ°ÜÊû∂Êî∂ÁõäÂπ∂‰∏çÈ´òÔºåÊï¥‰∏™È°πÁõÆÁªô‰∫∫ÊÑüËßâÊØîËæÉÈ∏°ËÇã„ÄÇ\n","link":"https://kane.mx/posts/effective-cloud-computing/spring-cloud-function-for-aws/","section":"posts","tags":["‰∫ëËÆ°ÁÆó","FaaS","ÂáΩÊï∞ËÆ°ÁÆó","AWS","AWS Lambda","ÈíâÈíâ","dingtalk","Serverless Computing","Spring","Spring Cloud Function"],"title":"Spring Cloud Function -- Ë∑®ServerlessÂπ≥Âè∞ÁöÑÂáΩÊï∞ËÆ°ÁÆóÊ°ÜÊû∂"},{"body":"","link":"https://kane.mx/tags/%E5%87%BD%E6%95%B0%E8%AE%A1%E7%AE%97/","section":"tags","tags":null,"title":"ÂáΩÊï∞ËÆ°ÁÆó"},{"body":"","link":"https://kane.mx/tags/%E9%92%89%E9%92%89/","section":"tags","tags":null,"title":"ÈíâÈíâ"},{"body":"AWSÊòØÂÖ®ÁêÉ‰∫ëËÆ°ÁÆóÈ¢ÜÂüüÁöÑÈ¢ÜË∑ëËÄÖÔºåÂÆÉÂú®ËÆ°ÁÆó„ÄÅÂ≠òÂÇ®„ÄÅÁΩëÁªúÁ≠âÊñπÈù¢ÈÉΩÂÅöÂá∫‰∫ÜÂæàÂ§öÂàõÊñ∞ÔºåÂêåÊó∂‰πüÊòØÂÖ∂‰ªñ‰∫ëËÆ°ÁÆóÂéÇÂïÜÂ≠¶‰π†ÂèäÊ®°‰ªøÁöÑÂØπË±°„ÄÇ\nÈòøÈáå‰∫ëÊòØÁõÆÂâçÂõΩÂÜÖÂ∏ÇÂú∫‰ªΩÈ¢ùÊúÄÂ§ßÁöÑ‰∫ëËÆ°ÁÆóÂéÇÂïÜÔºåÂÖ∂‰ªΩÈ¢ùË∂ÖËøá‰∫ÜÁ¨¨‰∫åËá≥‰∫î‰ΩçÂéÇÂïÜÁöÑÊÄªÂíåÔºå‰ªΩÈ¢ùÈ¢ÜÂÖà‰ºòÂäøÊØîAWSÂú®ÂÖ®ÁêÉËøòË¶ÅÊòæËëóÔºåÂêåÊó∂ÂÖ®ÁêÉ‰ªΩÈ¢ù‰πüË∂ÖËøáIBMÊù•Âà∞Á¨¨Âõõ„ÄÇ\nÊú¨ÊñáÂ∞ÜÂØπAWSÂíåÈòøÈáå‰∫ëÊ†∏ÂøÉÊúçÂä°ÂÅö‰∏Ä‰∏™ÁÆÄË¶ÅÂØπÊØîÔºå‰ª•ÂèäËøô‰∏§ÂÆ∂ÂéÇÂïÜÂèëÂ±ïÊñπÂêëÁöÑ‰∏Ä‰∫õ‰∏™‰∫∫ËßÅËß£„ÄÇ\n‰∫ëËÆ°ÁÆóÔºåÂÖ∂Ê†∏ÂøÉÊúçÂä°Â∞±ÊòØËÆ°ÁÆó„ÄÅÂ≠òÂÇ®ÂèäÁΩëÁªú„ÄÇËøô‰∫õÂü∫Êú¨ËÉΩÂäõÁöÑÁ®≥ÂÆöÊÄßÔºåÂäüËÉΩÂÆåÂñÑÊÄßÂÜ≥ÂÆö‰∫Ü‰∫ëËÆ°ÁÆóÂéÇÂïÜËÉΩÂäõÁöÑ‰∏ãÈôê„ÄÇ\nÈô§‰∫Ü‰∏äÈù¢ÊèêÂà∞ÁöÑ‰∏âÂ§ßËÆ°ÁÆóÊú∫Ê†∏ÂøÉÁªÑ‰ª∂ËÉΩÂäõÔºå‰∏ãÈù¢Ëøô‰∫õËÉΩÂäõ‰πüÊòØ‰∫ëËÆ°ÁÆó‰∏≠ÈùûÂ∏∏ÈáçË¶ÅÁöÑÁªÑÊàêÈÉ®ÂàÜÔºå\nÊåâÈáèËÆ°Ë¥π ËµÑÊ∫êÁºñÊéíÔºà‰πüÂ∞±ÊòØÂπ≥Âè∞‰Ωú‰∏∫‰ª£Á†ÅÔºâ ‰∫ëËµÑÊ∫êÁöÑËÆ§ËØÅÂèäÊéàÊùÉ API Âü∫‰∫é‰∏äÈù¢Âàó‰∏æÁöÑ‰∫ëËÆ°ÁÆóÊ†∏ÂøÉÊúçÂä°ÂíåÂÖ≥ÈîÆËÉΩÂäõÔºåÊàë‰ª¨Êù•ÁúãÁúãÂì™‰∫õÊñπÈù¢ÊòØAWSÁöÑÂº∫È°π„ÄÇ\nAWS‰Ωú‰∏∫‰∫ëËÆ°ÁÆóÁöÑÈ¢ÜÂÜõÂéÇÂïÜÔºåÂú®ËÆ°ÁÆó„ÄÅÂ≠òÂÇ®„ÄÅÁΩëÁªúËøô‰∏âÂ§ßÊ†∏ÂøÉ‰∏ÄÁõ¥Âú®‰∏çÂÅúÁöÑÂàõÊñ∞‰∏≠Ôºå‰∏îË¢´ÂèãÂïÜÂú®‰∏çÂÅúÁöÑÊ®°‰ªø„ÄÇËÆ°ÁÆóÊñπÈù¢ÔºåAWSÈ¶ñÂÖàÊé®Âá∫‰∫ÜLambdaÊó†ÊúçÂä°Âô®ËÆ°ÁÆóÂºïÊìéÂÆûÁé∞ÊåâÈáè‰ΩøÁî®ÁöÑÂÖ®ÊâòÁÆ°ÊúçÂä°ÔºåÁîü‰∫ßÂèØÁî®ÁöÑGPUÂÆû‰æã(ÂçïËôöÊú∫ÂèØÈÖçÁΩÆÊúÄÈ´ò64ÂùóGPUÂç°ÔºåËÄåÈòøÈáå‰∫ëÈªòËÆ§‰ªÖÂîÆÂçñ2ÂùóGPUÂç°)ÔºåÂü∫‰∫éNitroÊû∂ÊûÑÁöÑEC2ÂÆû‰æã‰∏∫ÂÆ¢Êà∑ÈÄÅ‰∏ä‰∫ÜÂçáÊÄßËÉΩÈôç‰ª∑ÁöÑÂ•Ω‰∫ã„ÄÇ\nS3‰Ωú‰∏∫AWSÊúÄÊó©Êé®Âá∫ÁöÑ‰∫ëËÆ°ÁÆóÊúçÂä°Ôºå‰ªçÁÑ∂Âú®‰∏çÂÅúÁöÑÂàõÊñ∞ÊºîÂåñ‰∏≠„ÄÇÁõÆÂâçS3ËææÂà∞‰∫Ü11‰∏™9ÁöÑÊåÅ‰πÖÊÄßÔºå‰∏∫Êª°Ë∂≥ÂÆ¢Êà∑‰∏çÂêåÁöÑÂ≠òÂÇ®ÈúÄË¶ÅÔºåÂèàÊé®Âá∫‰∫ÜS3 Glacier„ÄÅGlacier Deep ArchiveÁ≠âÂ≠òÂÇ®ÊñπÊ°à„ÄÇÊåÅÁª≠Êé®Âá∫‰∫ÜAmazon Athena, Redshift, S3 selectÁ≠âÊúçÂä°ÂèäÂ∑•ÂÖ∑Ëß£ÂÜ≥Êµ∑ÈáèÊï∞ÊçÆÁöÑÂ§ßÊï∞ÊçÆÂ§ÑÁêÜ„ÄÇ\nAWS‰∏ÄÁõ¥Â∞ÜPAYG(Pay-As-You-Go)ÁöÑÊåâÈáèËÆ°Ë¥πÊ®°ÂûãË¥ØÁ©øÂú®ÂêÑÁßçÊúçÂä°‰∏≠„ÄÇÊó†ËÆ∫ÊòØEC2(ÂåÖÊã¨GPUÂÆû‰æã)ÔºåELBÔºåNATÁΩëÂÖ≥Á≠âÁ≠âÈÉΩÊèê‰æõÂ∞èÊó∂Á∫ßÁöÑÊåâÈáèËÆ°Ë¥π„ÄÇÈòøÈáå‰∫ëÂú®ËøôÊñπÈù¢ËøòÊúâËæÉÂ§öÁöÑÊîπËøõÁ©∫Èó¥Ôºå‰æãÂ¶ÇGPUÂÆû‰æãÊúÄÂ∞èÂîÆÂçñÊó∂Èïø‰∏∫‰∏ÄÂë®ÔºåSLBÈ¶ñÂÖàÊåâËßÑÊ†ºÂîÆÂçñÔºåNATÁΩëÂÖ≥ÊåâËá™ÁÑ∂Êó•ËÆ°Ë¥π„ÄÇ\nIAM‰∏∫‰∫ë‰∏äÁöÑËµÑÊ∫êÊèê‰æõ‰∫ÜÊúÄÁªÜÁ≤íÂ∫¶ÁöÑÊéàÊùÉÁÆ°ÁêÜÔºåAWSÂêÑ‰∏™ÊúçÂä°‰∏•Ê†ºÊåâÊúÄÁªÜÁ≤íÂ∫¶ÊéßÂà∂ÊéàÊùÉÔºåÊª°Ë∂≥‰ºÅ‰∏öÁöÑÊùÉÈôêÁÆ°ÁêÜ„ÄÇÂú®Êàë‰ΩøÁî®ËøáÁöÑÊï∞‰∏™ÈòøÈáå‰∫ëÊúçÂä°‰∏≠ÔºåÂ§öÊ¨°ÈÅáÂà∞ËæÉÊñ∞ÁöÑÊúçÂä°IAMËÆæËÆ°‰∏çÂë®ÔºåÊùÉÈôêÁ≤íÂ∫¶ËøáÂ§ßÔºåÁîöËá≥ÂäüËÉΩÊó†Ê≥ïÂ∑•‰ΩúÁöÑÊÉÖÂÜµ‰∏ãÂ∞±‰∏äÁ∫øÂèëÂ∏É‰∫Ü„ÄÇ\nAWS CloudFormationÊèê‰æõ‰∫Ü‰∫ë‰∏äËµÑÊ∫êÁºñÊéíÁÆ°ÁêÜÔºåÂÆûÁé∞‰∫ÜËµÑÊ∫êÁöÑ‰ª£Á†ÅÂåñÔºåÁâàÊú¨Âåñ(ÈÄöÂ∏∏Áß∞‰∏∫ÁöÑInfrastrucure as Code)„ÄÇÂ∞Ü‰∫ëÁ´ØËµÑÊ∫êÁöÑÁÆ°ÁêÜËøêÁª¥ÊèêÂçáÂà∞‰∏Ä‰∏™Êñ∞ÁöÑÂ±ÇÊ¨°„ÄÇ\nAWSÊèê‰æõ‰∫Ü‰∏âÁßçÊñπÂºèÁÆ°ÁêÜ‰∫ë‰∏äËµÑÊ∫êÔºåWeb Console, CLI‰ª•ÂèäAPI„ÄÇËøô‰∏âÁßçÊñπÂºèÔºåÂ∞ΩÊúÄÂ§ßÂä™ÂäõÊèê‰æõ‰∏ÄËá¥ÁöÑÂäüËÉΩ„ÄÇ\nAWSÂêåÊó∂ÊòØ‰∏Ä‰∏™‰∫ëËÆ°ÁÆóÁöÑÁîüÊÄÅÔºåÂêÑÁ±ª‰∏âÊñπ‰∫ëÊúçÂä°ÂéÇÂïÜÈÄöËøáMarketplaceÂîÆÂçñÂêÑÁ±ªSaaSÔºåPaaSÊúçÂä°ÔºåÂΩ¢Êàê‰∏Ä‰∏™‰∫ëËÆ°ÁÆóÁî®Êà∑Ôºå‰∏âÊñπÊúçÂä°VendorÔºåAWS‰∏âÊñπÂÖ±Ëµ¢ÁöÑÂ±ÄÈù¢„ÄÇ\nÊÄªÂæóËØ¥Êù•ÔºåAWSÊåÅÁª≠ÁöÑÂú®‰∫ëËÆ°ÁÆóÊ†∏ÂøÉÊúçÂä°ÂíåÂÖ≥ÈîÆÊúçÂä°ÊäïÂÖ•Ôºå‰∏çÂÅúÁöÑÂàõÊñ∞Ôºå‰øùËØÅ‰∫ÜAWSÊï¥‰ΩìÊúçÂä°ÁöÑÈ¢ÜÂÖà„ÄÇ\nÊé•‰∏ãÊù•ÁúãÁúãÈòøÈáå‰∫ëÁöÑÂº∫È°π„ÄÇ\nÈòøÈáå‰∫ëÂú®Êèê‰æõÂü∫Êú¨ÁöÑËÆ°ÁÆó„ÄÅÂ≠òÂÇ®„ÄÅÁΩëÁªúÂ§ñÔºåÈ¢ùÂ§ñÊèê‰æõ‰∫ÜÂæàÂ§öSaaSÊúçÂä°Ôºå‰æãÂ¶ÇÔºåApplication Performance MonitorÔºå Performance Testing Service, Êó•ÂøóÊúçÂä°ÔºåÈìæË∑ØËøΩË∏™ÊúçÂä°ÔºåÊï∞ÊçÆÂ∫ìÁÆ°ÁêÜÊúçÂä°Á≠â„ÄÇËøô‰∫õÊúçÂä°ÊòæÁÑ∂ÂêåÈòøÈáå‰∫ëÊúâÊõ¥Â•ΩÁöÑÈõÜÊàêÔºåÂØπÁî®Êà∑Êù•ËØ¥Êèê‰æõ‰∫ÜÂºÄÁÆ±Âç≥Áî®ÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇËÄåËøô‰πüÊòØ‰∏ÄÊääÂèåÂàÉÂâëÔºåÂà©Áî®Âπ≥Âè∞ÊçÜÁªëÁöÑ‰ºòÂäøÊä¢Âç†Âêà‰ΩúÂºÄÂèëÂïÜÁöÑÂ∏ÇÂú∫ÔºåÈïøÊúüÊù•ËØ¥Âà©Áî®Âπ≥Âè∞ÂûÑÊñ≠‰∏çÂà©‰∫éÂü∫‰∫éÈòøÈáå‰∫ëÁöÑÊäÄÊúØÊúçÂä°Âàõ‰∏ö„ÄÇ\nÊÄª‰πãÔºåÈòøÈáå‰∫ëÂú®‰∫ëËÆ°ÁÆóÊ†∏ÂøÉÊúçÂä°‰∏äÂêåAWSÊØîËøòÊúâÂ∑ÆË∑ùÔºå‰ΩÜ‰ªñÂú®PaaS/SaaSÊúçÂä°‰∏äÂèëÂ±ï‰∏çÈîôÔºåÊõ¥Âä†ÂÆπÊòìÊèê‰æõÂÖ®Â•óÂü∫‰∫éÈòøÈáå‰∫ëÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇÁî±‰∫éÈòøÈáå‰∫ëÂú®ÂõΩÂÜÖÊï∞ÊçÆ‰∏≠ÂøÉÊï∞Èáè‰∏äÁöÑ‰ºòÂäøÂä†‰∏ä‰ªé‰∏áÁΩëÊî∂Ë¥≠ÁöÑBGPËµÑÊ∫êÔºåÂÖ∂ÊúçÂä°Âú®ÂõΩÂÜÖËÆøÈóÆÁΩëÁªúÂª∂Ëøü‰ºöÊõ¥‰Ωé„ÄÇ\nÊúÄÂêéÔºåË∞à‰∏Ä‰∏™ÂæàÊúâÊÑèÊÄùÁöÑËØùÈ¢òÔºåÊòØÂê¶ÈúÄË¶ÅËÄÉËôë‰∫ëÂéÇÂïÜÁöÑÈîÅÂÆö„ÄÇ\nKubernetes‰∫ãÂÆû‰∏äÊàê‰∏∫ÂÆπÂô®ÁºñÊéíÂπ≥Âè∞ÔºåÈ¶ñÂÖàËÄÉËôë‰ΩøÁî®K8SÂèäCNCF landscape‰∏ãÁöÑÈ°πÁõÆ‰Ωú‰∏∫Â∫îÁî®ËøêË°åÁéØÂ¢ÉÔºåÂáèÂ∞ëÂèØËÉΩÁöÑËøÅÁßªÂíåÂ≠¶‰π†ÊàêÊú¨„ÄÇ\nÂØπ‰∏çÂêåÁî®ÈáèÁöÑÂÖ¨Âè∏Êù•ËØ¥ÔºåËÄÉËôë‰∫ëÂéÇÂïÜÈîÅÂÆöÁöÑÁª¥Â∫¶ÂÆåÂÖ®‰∏ç‰∏ÄÊ†∑„ÄÇÂàõ‰∏öÂûãÂÖ¨Âè∏Êàñ‰ªçÂú®Âø´ÈÄüÂèëÂ±ï‰∏öÂä°‰∏≠ÁöÑ‰∏≠Â§ßÂûã‰ºÅ‰∏öÈ¶ñÂÖàÂ∫îËØ•ÈÄâÊã©ÂèØÈù†ÊÄßÈ´òÔºåËß£ÂÜ≥ÊñπÊ°àÂ§öÔºåÊòìÂ≠¶‰π†ÁöÑ‰∫ëÂéÇÂïÜÔºåÂ∞ΩÂèØËÉΩÂà©Áî®‰∫ëÂéÇÂïÜÁöÑÂêÑÁßçÊúçÂä°ÂÅöÂà∞Âø´ÈÄüÈ´òÊïàÂèØÈù†ÁöÑÊé®Ëøõ‰∏öÂä°ÔºåÂ∞ÜÂ∞ΩÈáèÂ§öÁöÑÁ≤æÂäõ„ÄÅ‰∫∫ÂäõÊäïÂÖ•Âà∞‰∏öÂä°Áõ∏ÂÖ≥ÁöÑ‰∫ãÊÉÖ‰∏ä„ÄÇ‰∏öÂä°Á®≥ÂÆöÁöÑÂ§ßÂûãÂÖ¨Âè∏ÔºåÂèØ‰ª•‰ΩøÁî®Â§öÊï∞ÊçÆ‰∏≠ÂøÉÂÆûÁé∞ÂÖ≥ÈîÆ‰∏öÂä°ÁöÑÈ´òÂèØÁî®ÊÄßÔºåË∑®‰∫ëÂÆåÂÖ®‰∏çÂ∫îËØ•‰Ωú‰∏∫È´òÂèØÁî®ÁöÑÂøÖË¶ÅËß£ÂÜ≥ÈÄîÂæÑ„ÄÇÂè¶Â§ñÔºå‰∫ëÂéÇÂïÜÁªùÂØπ‰ºöÊäïÂÖ•È¢ùÂ§ñÁöÑ‰∫∫ÂäõÔºå‰ºòÂÖàÁ∫ßÊîØÊåÅ‰ªñ‰ª¨ÁöÑÂ§ßÂÆ¢Êà∑ÔºåÁîöËá≥‰∏∫ËøôÁ±ªÂÆ¢Êà∑Ë∞ÉÊï¥‰∫ßÂìÅÁ†îÂèë‰ºòÂÖàÁ∫ßÊàñÂçèÂêåÂÆåÊàêÊüê‰∫õÂäüËÉΩÔºåËøôÊ†∑ÁªùÂØπÊòØ‰∏™ÂèåËµ¢ÁöÑÂ±ÄÈù¢ÔºåNetflixÂíåAWSÁöÑ‰∫íÁõ∏ÊàêÂ∞±Â∞±ÊòØ‰∏Ä‰∏™ÂæàÂ•ΩÁöÑ‰æãÂ≠ê„ÄÇÊ≤°ÊúâÁâπÂà´ÂøÖË¶ÅÁöÑÂéüÂõ†Ôºå‰∏çË¶ÅËΩªÊòìÊäïÂÖ•Á≤æÂäõÂ∞Ü‰∏öÂä°‰ªéÊúçÂä°Â∑≤ÁªèÂæàÁ®≥ÂÆöÁöÑ‰∫ëÂéÇÂïÜËøÅÁßªÂà∞Â§ö‰∫ëÂπ≥Âè∞‰∏äÔºåÈÇ£Ê†∑ÂæÄÂæÄÊòØÁôΩÁôΩËÄóË¥πÂäõÊ∞î„ÄÇ\n‰∏ãÈù¢ÊòØslideÁöÑÊúÄÊñ∞ÂÆåÊï¥ÁâàÊú¨Ôºå\n","link":"https://kane.mx/posts/2019/aws-vs-aliyun/","section":"posts","tags":["‰∫ëËÆ°ÁÆó","AWS","ÈòøÈáå‰∫ë"],"title":"ÂÖ¨Êúâ‰∫ëÂØπÊØî"},{"body":"","link":"https://kane.mx/tags/%E9%98%BF%E9%87%8C%E4%BA%91/","section":"tags","tags":null,"title":"ÈòøÈáå‰∫ë"},{"body":"Serverless FrameworkÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÂëΩ‰ª§Ë°åÂ∑•ÂÖ∑„ÄÇ‰ªñÊèê‰æõÂáΩÊï∞ËÑöÊâãÊû∂„ÄÅÊµÅÁ®ãËá™Âä®Âåñ„ÄÅÊúÄ‰Ω≥ÂÆûË∑µÁ≠âÂ∏ÆÂä©ÂºÄÂèë„ÄÅÈÉ®ÁΩ≤Ë∑®‰∫ëÂéÇÂïÜÁöÑÊâòÁÆ°Êó†ÊúçÂä°Âô®ËÆ°ÁÆóÊúçÂä°(ÂÆòÊñπÂ∑≤ÊîØÊåÅaws, Azure, GCP, IBM CloudÁ≠âÂêÑÁßçÂéÇÂïÜÁöÑÊó†ÊúçÂä°Âô®ËÆ°ÁÆó)„ÄÇÂêåÊó∂ÊîØÊåÅ‰ΩøÁî®Êèí‰ª∂Êù•Êâ©Â±ïÂêÑÁßçÂäüËÉΩÔºåÊØîÂ¶ÇÊîØÊåÅÊõ¥Â§ö‰∫ëÂéÇÂïÜÊó†ÊúçÂä°Âô®ËÆ°ÁÆóÊúçÂä°Ôºå‰æãÂ¶ÇÈòøÈáå‰∫ëÁöÑÂáΩÊï∞ËÆ°ÁÆó„ÄÇ\nËøôÈáå‰ΩøÁî®Âü∫‰∫éÂáΩÊï∞ËÆ°ÁÆóÁöÑÈíâÈíâÂõûË∞ÉÂáΩÊï∞Êé•Âè£Á§∫‰æãÊù•ÊºîÁ§∫Â¶Ç‰Ωï‰ΩøÁî®Serverless FrameworkÂ∞Ü‰∏Ä‰∏™Êó†ÊúçÂä°Âô®ÂáΩÊï∞ÈÉ®ÁΩ≤Âà∞AWS Lambda„ÄÇ\nÂÆâË£ÖserverelessÂêéÔºåÂèØ‰ª•ÈÄöËøáserverless createÂëΩ‰ª§ÂàõÂª∫ÂáΩÊï∞ËÑöÊâãÊû∂Â∑•Á®ãÔºåÊàñËÄÖÂú®Â∑≤ÊúâÂ∑•Á®ãÁöÑ‰∏ãÂàõÂª∫serverlessÈÖçÁΩÆÊñá‰ª∂serverless.yml„ÄÇ\nÊé•‰∏ãÊù•ÂèØ‰ª•ÂèÇËÄÉserverless aws referenceÈÖçÁΩÆ‰Ω†ÁöÑaws lambdaÂáΩÊï∞ÂèäÈúÄË¶ÅÁöÑÂêÑÁßçËµÑÊ∫ê„ÄÇÂ¶ÇÊûúÂ∑≤ÁªèÊúâËøá‰ΩøÁî®AWS CloudFormationÊàñËÄÖAWS SAMÁªèÈ™åÁöÑÔºåÂèØ‰ª•ÂæàÂø´ÈÄÇÂ∫îÁºñÂÜôServerlessÈÖçÁΩÆ„ÄÇServerlessÁöÑÈÖçÁΩÆÊú¨Ë¥®‰∏äÊòØÂ∞ÜCloudFormation/SAMÁõ∏ÂÖ≥ÁöÑÊ¶ÇÂøµËøõË°åÊäΩË±°Ôºå‰∏∫ÂêÑ‰∏™‰∫ëÂéÇÂïÜÁöÑÊó†ÊúçÂä°Âô®ËÆ°ÁÆóÊúçÂä°Êèê‰æõÁªü‰∏ÄÁöÑÂ∑•ÂÖ∑„ÄÅÂëΩ‰ª§‰ª•ÂèäÊ¶ÇÂøµÊäΩË±°„ÄÇÂú®ÈÉ®ÁΩ≤aws lambdaÊó∂ÔºåserverlessÈÖçÁΩÆ‰ºöË¢´ËΩ¨Êç¢‰∏∫CloudFormationÈÖçÁΩÆÔºåÈÄöËøáAWS APIËøõË°åÂàõÂª∫ÊàñÂèòÊõ¥„ÄÇ\nÂØπ‰∫éDingtalk Callback on AWS Lambda, serverlessÈÖçÁΩÆÂ£∞ÊòéÂ¶Ç‰∏ã„ÄÇÂÖ∂‰∏≠ÊåáÂÆö‰∫ÜserviceÁöÑÂü∫Êú¨‰ø°ÊÅØÔºåÂÖ®Â±ÄÁöÑÈÖçÁΩÆ(Â¶Çstage„ÄÅregionÁ≠â)„ÄÅ‰∫ëÂéÇÂïÜprovider(ËøôÈáåÊòØaws)„ÄÇÂáΩÊï∞ÁöÑÂü∫Êú¨‰ø°ÊÅØ„ÄÅÊùÉÈôê„ÄÅlayer„ÄÅËß¶ÂèëÂô®ÔºåËá™ÂÆö‰πâlayer‰ª•ÂèäÂÖ∂‰ªñ‰∫ëÂéÇÂïÜËµÑÊ∫êÔºåÊØîÂ¶ÇDingtalk callbackËøôÈáåÁî®Âà∞ÁöÑDynamoDB„ÄÇÂÆåÊï¥ÁöÑserverlessÈÖçÁΩÆÊü•ÁúãËøôÈáå„ÄÇ\n1service: 2 name: dingtalk-callback 3 4frameworkVersion: \u0026#34;\u0026gt;=1.0.0 \u0026lt;2.0.0\u0026#34; 5 6provider: 7 name: aws 8 runtime: java8 9 stage: ${opt:stage, \u0026#39;dev\u0026#39;} # Set the default stage used. Default is dev 10 region: ${opt:region, \u0026#39;ap-southeast-1\u0026#39;} # Overwrite the default region used. Default is ap-southeast-1 11 profile: ${opt:profile, \u0026#39;default\u0026#39;} # The default profile to use with this service 12 versionFunctions: true # Optional function versioning 13 endpointType: regional # Optional endpoint configuration for API Gateway REST API. Default is Edge. 14 15functions: 16 dingtalk-callback: 17 handler: com.github.zxkane.dingtalk.Callback::handleRequest # required, handler set in AWS Lambda 18 name: ${self:provider.stage}-dingtalk-callback # optional, Deployed Lambda name 19 memorySize: 384 # optional, in MB, default is 1024 20 timeout: 15 # optional, in seconds, default is 6 21 environment: # Function level environment variables 22 PARA_DD_TOKEN: DD_TOKEN 23 TABLE_NAME: {Ref: BPMTable} 24 package: 25 artifact: build/libs/dingtalk-callback-1.0.0-SNAPSHOT.jar 26 role: dingtalkCallbackIAMRole 27 layers: # An optional list Lambda Layers to use 28 - {Ref: DependenciesLambdaLayer} 29 events: # The Events that trigger this Function 30 - http: # This creates an API Gateway HTTP endpoint which can be used to trigger this function. Learn more in \u0026#34;events/apigateway\u0026#34; 31 path: dingtalk # Path for this endpoint 32 method: post # HTTP method for this endpoint 33 34layers: 35 dependencies: 36 path: build/deps 37 38resources: # CloudFormation template syntax 39 Resources: 40 dingtalkCallbackIAMRole: 41 Type: AWS::IAM::Role 42 Properties: 43 Policies: 44 - PolicyName: SSMPolicy 45 - PolicyName: DynamoDBPolicy 46 BPMTable: 47 Type: AWS::DynamoDB::Table 48 Properties: 49 TableName: bpm_raw_${self:service.name}_${self:provider.stage} 50 ProvisionedThroughput: 51 ReadCapacityUnits: 1 52 WriteCapacityUnits: 1 ÂØπ‰∫é‰ΩøÁî®Âçï‰∏Ä‰∫ëÂéÇÂïÜÊó†ÊúçÂä°Âô®ËÆ°ÁÆóÂπ∂‰∏îÂ∑≤Áªè‰ΩøÁî®‰∫ÜÁ±ª‰ººsam cliÂÆûÁé∞ÊåÅÁª≠ÈõÜÊàê„ÄÅÊåÅÁª≠ÈÉ®ÁΩ≤ÁöÑÁî®Êà∑ÔºåServerless FrameworkÂπ∂‰∏çËÉΩÂ∏¶Êù•Êõ¥Â§öÁîü‰∫ßÂäõÁöÑÊèêÂçáÔºåÂú®Á®≥ÂÆöÊÄß(Â∞ÅË£Ö‰∫ëÂéÇÂïÜÁöÑÂäüËÉΩÔºåÂ¢ûÂä†Â§çÊùÇÂ∫¶ÂæàÂèØËÉΩÂºïÂÖ•Êñ∞ÁöÑÈóÆÈ¢ò)ÊàñÂäüËÉΩÁöÑÂèäÊó∂ÊÄß‰∏äÂèØËÉΩËøò‰∏çÂ¶Ç‰∫ëÂéÇÂïÜÊèê‰æõÁöÑÂ∑•ÂÖ∑„ÄÇ\nÂØπ‰∫éÊúâÂ§ö‰∫ëÂéÇÂïÜÈÉ®ÁΩ≤Êó†ÊúçÂä°Âô®ÂáΩÊï∞ÈúÄÊ±ÇÁöÑÁî®Êà∑Ôºå‰ΩøÁî®‰∫ÜServerless FrameworkÂπ∂‰∏çËÉΩËΩªÊùæÁöÑÂ∞ÜÊó†ÊúçÂä°Âô®ÂáΩÊï∞ÈÉ®ÁΩ≤Âà∞‰∏çÂêå‰∫ëÂéÇÂïÜÁöÑÊâòÁÆ°ÊúçÂä°‰∏äÔºå‰ªñÂè™ÊòØÂ∏ÆÂä©Êèê‰æõË∑®‰∫ëÂéÇÂïÜÁöÑÁªü‰∏ÄÂ∑•ÂÖ∑ÈìæÂèäÁõ∏‰ººÁöÑÊåÅÁª≠ÈõÜÊàê„ÄÅÈÉ®ÁΩ≤Á≠âÊúÄ‰Ω≥ÂÆûË∑µÊµÅÁ®ã„ÄÇ‰æãÂ¶ÇÂ∞Ü‰∏ÄÂ•óÂáΩÊï∞‰ªéAWSËøÅÁßªÂà∞Azure‰∏äÔºåÈúÄË¶ÅÈáçÊñ∞ÂÆûÁé∞Azure provider‰∏ãÁöÑÈÖçÁΩÆÔºåÂõ†‰∏∫‰∫ëÂéÇÂïÜÁöÑÊâòÁÆ°Êó†ÊúçÂä°Âô®ÊúçÂä°ÂíåÂÖ∂‰ªñ‰∫ëËµÑÊ∫êÈÉΩÂ≠òÂú®ÁùÄÂ§ßÈáèÂ∑ÆÂºÇ„ÄÇÂè¶Â§ñÂáΩÊï∞‰ª£Á†Å‰πüÈúÄË¶ÅÈù¢‰∏¥ÊîπÈÄ†Ôºå‰∏çÂêå‰∫ëÂéÇÂïÜÁöÑËß¶ÂèëÂô®Ê∂àÊÅØ‰∫ã‰ª∂‰πüÈÉΩÊúâ‰∏çÂêåÁöÑÊ†ºÂºèÔºÅËøôÈáåÂèØ‰ª•ËÄÉËôë‰ΩøÁî®Á±ª‰ººSpring Cloud FunctionËøôÊ†∑ÁöÑËß£ÂÜ≥ÊñπÊ°àÊù•ÂÆûÁé∞Ë∑®‰∫ëÂéÇÂïÜÁöÑÂáΩÊï∞ÁºñÂÜô„ÄÇ\nÊÄª‰πãÔºåServerless FrameworkÂØπ‰∫éË∑®‰∫ëÂéÇÂïÜÈÉ®ÁΩ≤Âú∫ÊôØÊúâ‰∏ÄÂÆöÁîü‰∫ßÊïàÁéáÁöÑÊèêÂçáÔºå‰ΩÜ‰ªñÁ¶ªÂÆåÁæéËß£ÂÜ≥Ë∑®‰∫ëÂéÇÂïÜÊó†ÊúçÂä°Âô®ÊâòÁÆ°ÊúçÂä°ÔºàÂêÑÂéÇÂïÜÊúçÂä°Â§©Áîü‰∏çÂÖºÂÆπÔºâËøòÊúâÂæàËøúÁöÑË∑ùÁ¶ªÔºå‰πüËÆ∏Ëøô‰∏™ÊÄùË∑ØÂ∞±ÊòØËµ∞‰∏çÈÄöÁöÑ\u0026#x1f615;„ÄÇ\n","link":"https://kane.mx/posts/2019/serverless-framework/","section":"posts","tags":["‰∫ëËÆ°ÁÆó","FaaS","AWS","AWS Lambda","Serverless Computing"],"title":"Serverless framework 101"},{"body":"Âú®Âü∫‰∫éÂáΩÊï∞ËÆ°ÁÆóÁöÑÈíâÈíâÂõûË∞ÉÂáΩÊï∞Êé•Âè£‰∏≠‰ΩøÁî®ÈíâÈíâÂõûË∞ÉÂáΩÊï∞Ê°à‰æãÂÆûË∑µ‰∫ÜAWS LambdaÊó†ÊúçÂä°ÂáΩÊï∞„ÄÇËØ•Á§∫‰æã‰∏≠ÔºåÊàë‰ª¨Â∞ÜËá™ÂÆö‰πâÁöÑÂáΩÊï∞‰ª£Á†ÅÂèä‰æùËµñÁöÑÁ¨¨‰∏âÊñπÂ∫ìÔºàÊØîÂ¶ÇjsonÂ§ÑÁêÜÂ∫ìjackson, ÈíâÈíâopenapiÂä†ÂØÜÂ∫ì, aws dynamodb clientÁ≠âÔºâÊï¥‰ΩìÊâìÂåÖ‰∏∫‰∏Ä‰∏™ÈÉ®ÁΩ≤ÂåÖÔºå‰∏ä‰º†Âà∞lamdba‰ª£Á†Å‰ªìÂ∫ìÁî®‰∫éÂáΩÊï∞ÊâßË°å„ÄÇ\nÁÑ∂ËÄåÂÆûÈôÖÈ°πÁõÆ‰∏≠ÔºåÂÖ∂ÂÆûÊúâÂ§ßÈáèÁöÑÁõ∏ÂÖ≥ÂáΩÊï∞ÂèØËÉΩ‰ºöÂÖ±‰∫´Ëøô‰∫õÂü∫Á°Ä‰æùËµñÂ∫ì„ÄÅ‰∏âÊñπÂáΩÊï∞Â∫ì(ÊØîÂ¶Çheadless chrome(Puppeteer), pandoc, OCR library -- TesseractÁ≠âÁ≠â)ÊàñËÄÖ‰ΩøÁî®Ëá™ÂÆö‰πâruntime(Â¶ÇÂÆòÊñπÊú™ÊîØÊåÅÁöÑjava11)ÁöÑÈúÄÊ±Ç„ÄÇAWS LambdaÂú®ÂéªÂπ¥Â∫ïÂèëÂ∏É‰∫ÜLambda layersÂäüËÉΩÊù•Êª°Ë∂≥‰∏äËø∞Ëøô‰∫õÂÆûÈôÖÂºÄÂèë‰∏≠ÁöÑÈúÄÊ±Ç„ÄÇ\nÊé•‰∏ãÊù•ÔºåËÆ©Êàë‰ª¨ÁúãÁúãÂ¶Ç‰ΩïÂ∞ÜÂâçÊñá‰∏≠ÁöÑÂáΩÊï∞‰æùËµñÊîæÁΩÆÂà∞‰∏Ä‰∏™ÂçïÁã¨ÁöÑlayer‰∏≠Ôºå‰Ωú‰∏∫‰∏çÂêåÂáΩÊï∞ÁöÑÂÖ±‰∫´‰æùËµñÂ∫ì„ÄÇ\nÂú®Êàë‰ª¨ÁöÑÊûÑÂª∫ÈÖçÁΩÆbuild.gradle‰∏≠ÔºåÂ∞ÜÂáΩÊï∞ÁöÑÂÖ±‰∫´‰æùËµñÊã∑Ë¥ùÂà∞java runtimeÁâπÂÆöÁöÑÁõÆÂΩïÁªìÊûÑjava/lib/‰∏ãÔºå\n153 154 155 156 157 tasks.register\u0026lt;Copy\u0026gt;(\u0026#34;depsLayer\u0026#34;) { into(\u0026#34;$buildDir/deps/java/lib\u0026#34;) from(configurations.compileClasspath.get()) from(configurations.runtimeClasspath.get()) } Êé•‰∏ãÊù•Â∞ÜÂÖ±‰∫´ÁöÑ‰æùËµñÂàõÂª∫‰∏∫‰∏Ä‰∏™lambda layerÔºåÂπ∂‰∏îËÆ©callbackÂáΩÊï∞‰æùËµñËøô‰∏™ÂÖ±‰∫´layerÔºå‰∏çÂÜçÂ∞ÜÊâÄÊúâÁöÑ‰æùËµñÊâìÂåÖ‰∏∫‰∏Ä‰∏™ÂæàÂ§ßÁöÑÈÉ®ÁΩ≤ÂåÖÂáèÂ∞èÊØèÊ¨°ÂèòÊõ¥ÈúÄË¶ÅÂèëÂ∏ÉÁöÑÂåÖÂ§ßÂ∞è„ÄÇ\n31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 DependenciesLayer: Type: AWS::Serverless::LayerVersion Properties: LayerName: DingTalkDependencies Description: DingTalk Dependencies Layer ContentUri: \u0026#39;build/deps\u0026#39; CompatibleRuntimes: - java8 LicenseInfo: \u0026#39;Available under the MIT-0 license.\u0026#39; RetentionPolicy: Retain CallbackFunction: Type: AWS::Serverless::Function # More info about Function Resource: https://github.com/awslabs/serverless-application-model/blob/master/versions/2016-10-31.md#awsserverlessfunction Properties: CodeUri: build/libs/dingtalk-callback-1.0.0-SNAPSHOT.jar Handler: com.github.zxkane.dingtalk.Callback::handleRequest Layers: - !Ref DependenciesLayer Policies: Âú®consoleÊü•ÁúãÈÉ®ÁΩ≤ÂêéÁöÑÂáΩÊï∞ÔºåÂ¶Ç‰∏ãÂõæÔºåÂèØ‰ª•ÁúãÂà∞ÂáΩÊï∞Êñ∞Â¢û‰∫Ü‰∏Ä‰∏™layer„ÄÇ\nÂêåÂÖ∂‰ªñÁöÑËØ≠Ë®Ä„ÄÅÊäÄÊúØ‰∏ÄÊ†∑ÔºåAwesome LayersÈ°πÁõÆÊî∂ÈõÜ‰∫ÜÁõÆÂâç‰∏Ä‰∫õÂ∏∏Áî®‰∏îÁª¥Êä§ËæÉÂ•ΩÁöÑlayerÔºåËá™ÂàõËΩÆÂ≠ê‰πãÂâçÂèØ‰ª•ÂÖàÂèÇËÄÉ‰∏ã\u0026#x1f600;„ÄÇ\n‰ΩøÁî®layerÂêåÊ†∑Êúâ‰ª•‰∏ãÈôêÂà∂Ôºå‰ΩøÁî®ÂâçÈúÄË¶ÅÊ≥®ÊÑèÔºå\n‰æùËµñÁöÑlayerÊï∞‰∏çËÉΩË∂ÖËøá5‰∏™ ÂáΩÊï∞‰ª•Âèä‰æùËµñÁöÑÊâÄÊúâlayersËß£ÂéãÂêé‰∏çÂèØ‰ª•Ë∂ÖËøá250MB ","link":"https://kane.mx/posts/2019/aws-lambda-layers/","section":"posts","tags":["‰∫ëËÆ°ÁÆó","FaaS","AWS","AWS Lambda","Serverless Computing"],"title":"AWS Lambda LayerÂÆûË∑µ"},{"body":"","link":"https://kane.mx/tags/qcon/","section":"tags","tags":null,"title":"QCon"},{"body":"ËøôÂë®ÂèÇÂä†‰∫ÜQCon 2019Âåó‰∫¨Á´ôÔºåËøôÈáåËÆ∞ÂΩï‰∏ãÈÉ®ÂàÜÂç∞Ë±°Ê∑±ÂàªÁöÑ‰∏ªÈ¢ò‰ª•Âèä‰∏™‰∫∫ÊÑüÂèó„ÄÇ\nQConÊòØÁî±InfoQ‰∏ªÂäûÁöÑÁªºÂêàÊÄßÊäÄÊúØÁõõ‰ºöÔºå‰∏ªÈ¢òÊ∂µÁõñ‰∫ÜÂ§ßÂâçÁ´Ø„ÄÅÈ´òÂèØÁî®Êû∂ÊûÑ„ÄÅÂÆπÂô®ÊäÄÊúØ„ÄÅÂ§ßÊï∞ÊçÆ„ÄÅÊú∫Âô®Â≠¶‰π†Á≠âÂêÑÁßçÁÉ≠Èó®ÊäÄÊúØ‰∏ªÈ¢ò„ÄÇÂÖ∂‰∏≠‰πü‰∏ç‰πè‰∏ã‰∏Ä‰ª£ÂàÜÂ∏ÉÂºèÂ∫îÁî®„ÄÅÊ∑∑Ê≤åÂ∑•Á®ãÁ≠âÂâçÊ≤øÊúâÊÑèÊÄùÁöÑ‰∏ªÈ¢òÔºåÂêéÈù¢‰ºöËØ¶ÁªÜ‰ªãÁªçÁõ∏ÂÖ≥ÁöÑ‰∏ªÈ¢òÊºîËÆ≤„ÄÇ\nÂ∑•Á®ãÊïàÁéáÊèêÂçá ËøôÊòØÂú®QConÁ¨¨‰∏ÄÊó•‰∏™‰∫∫ÊÑüÂÖ¥Ë∂£‰∏îÈùûÂ∏∏ÊúâÊÑèÊÄùÁöÑ‰∏Ä‰∏™Á≥ªÂàó‰∏ªÈ¢ò„ÄÇÊó†ËÆ∫ÊòØÂàõ‰∏öÂÖ¨Âè∏„ÄÅÁã¨ËßíÂÖΩ‰ºÅ‰∏öËøòÊòØ‰∫íËÅîÁΩëÂ∑®Â§¥ÈÉΩÂ∏åÊúõ‰∏çÊñ≠ÊèêÂçáÂ∑•Á®ãÊïàÁéáÔºå3‰∏™Áõ∏ÂÖ≥ÁöÑÂàÜ‰∫´ÂàÜÂà´Êù•Ëá™BATÔºåÂèØËßÅ‰∫íËÅîÁΩëÂ∑®Â§¥‰ª¨ÂØπÂõ¢ÈòüÊïàÁéáÊèêÂçáÁöÑÊ∏¥ÊúõÂíåÈáçËßÜ„ÄÇ\n10ÂÄçÈÄüÂéüÂàôÂØπÂ∑•Á®ãÁîü‰∫ßÂäõÂª∫ËÆæÁöÑÊñπÂêëÊÄßÂΩ±Âìç Ëøô‰∏™talkÊù•Ëá™ËÖæËÆØÁöÑÈ´òÁ∫ßÈ°æÈóÆ‰πîÊ¢ÅÔºåËøô‰ΩçËÄÅÂÖÑÂ∑≤ÁªèËøûÁª≠10Âπ¥Âú®QCon‰∏äÂàÜ‰∫´ÊåÅÁª≠ÈõÜÊàê„ÄÅÊåÅÁª≠‰∫§‰ªòÁ≠âÂ∑•Á®ãÊïàÁéáÁõ∏ÂÖ≥ÁöÑ‰∏ªÈ¢ò‰∫ÜÔºÅ‰ªñÁöÑÊºîËÆ≤Âßã‰∫éÂØπÊàêÂäü‰ºÅ‰∏öÁöÑ‰∏Ä‰∏áÊ¨°ÂÆûÈ™åÊ≥ïÂàôÊñπÊ≥ïËÆ∫Ôºå ËÄåÂ§ßÈáèÈ´òÊïàÁöÑÂÆûÈ™åÂü∫‰∫é‰∏Ä‰∏™ÂèåÁéØÊ®°ÂûãÁöÑÂø´ÈÄüÈ™åËØÅÁéØ„ÄÇ ÊúÄÁªàÂ∑•Á®ãÁîü‰∫ßÂäõÊòØÁî±Â∑•‰ΩúÊµÅÁ®ã„ÄÅÊîØÊíëÂ∑•ÂÖ∑ÂíåÂ∑•Á®ãÁ¥†ÂÖª‰∏âÊñπÈù¢‰∏ÄËµ∑ÂÜ≥ÂÆöÁöÑ„ÄÇ ÈùûÂ∏∏ËÆ§ÂèØÂÜ≥ÂÆöÂ∑•Á®ãÊïàÁéáÁöÑËøô‰∏âË¶ÅÁ¥†Ôºå‰∏™‰∫∫ËÆ§‰∏∫Â∑•Á®ãÁ¥†ÂÖªÊòØÂÖ∂‰ªñ‰∏§‰∏™Ë¶ÅÁ¥†ÁöÑÂü∫Áü≥ÔºåÂ•àÈ£ûÊñáÂåñÊâãÂÜå‰∏≠ÂºÄÁØáÂº∫Ë∞ÉÁöÑÂè™ÊãõËÅòÊàêÂπ¥‰∫∫Â∞±ÊòØÂæàÂ•ΩÁöÑËØ†Èáä„ÄÇ\nÁôæÂ∫¶Â∑•Á®ãËÉΩÂäõÊèêÂçá‰πãÈÅì Ëøô‰∏™ÂàÜ‰∫´Êù•Ëá™ÁôæÂ∫¶Á†îÂèëÊïàËÉΩÈÉ®Èó®ÁöÑ‰∫ßÂìÅÁªèÁêÜÔºå‰ªé‰∫∫„ÄÅÊäÄ„ÄÅÊ≥ï‰∏âÊñπÈù¢Âº∫Ë∞ÉÂ∑•Á®ãËÉΩÂäõÊèêÂçáÁöÑÁ≠ñÁï•Ê®°Âûã„ÄÇÂÖ∂ÂÆûËøô‰∏™Ê®°ÂûãÂ∞±ÊòØÂØπÂ∫îÁùÄ‰∏äÈù¢‰πîÊ¢ÅÂàÜ‰∫´ÁöÑÂ∑•Á®ãÁîü‰∫ßÂäõ‰∏âË¶ÅÁ¥†„ÄÇ ÂÖ≥‰∫éÂØπÂ∑•Á®ãÂ∏àÁöÑÂüπÂÖªÂíåÊäÄÊúØËßÑËåÉÔºåÁôæÂ∫¶ÂèëÂ∏É‰∫Ü\u0026quot;ÁôæÂ∫¶Â∑•Á®ãÂ∏àÊâãÂÜå\u0026quot;ÔºåÊçÆËØ¥ÂèØ‰ª•‰ªéÁΩëÁªú‰∏ä‰∏ãËΩΩÂà∞„ÄÇÂ§ßÈáèÂ∑•ÂÖ∑ÁöÑÁªÜËäÇÂàÜ‰∫´Ê∂âÂèäÁöÑÈÉΩÊòØÁôæÂ∫¶ÂÜÖÈÉ®Â∑•ÂÖ∑Ôºå‰∏çËøáÂ∑•ÂÖ∑ÈíàÂØπÁöÑÊÄùË∑ØËøòÊòØÂèØ‰ª•ÂÄüÈâ¥ÁöÑ„ÄÇ\nËèúÈ∏üÈõÜÂõ¢Á†îÂèëÊïàËÉΩÂèòÈù©ÂÆûË∑µ Ëøô‰∏™ÂàÜ‰∫´Êù•Ëá™ÈòøÈáåÁ≥ªÁöÑËèúÈ∏üÈõÜÂõ¢ÔºåÁâπÂà´Âº∫Ë∞ÉÊï∞ÊçÆÂåñÈ©±Âä®ÁöÑÁ†îÂèëÊïàËÉΩÊèêÂçáÔºåÈáåÈù¢ÂæàÊúâÊÑèÊÄùÁöÑ‰∏ÄÁÇπÊòØÂª∫Á´ãÊàêÊú¨Ê®°ÂûãÊù•ËØÑ‰º∞ÊïàËÉΩÁöÑÂ•ΩÂùè„ÄÇ\n‰Ωú‰∏∫ÊïàËÉΩÈÉ®Èó®Ë¥üË¥£‰∫∫ÔºåÊúâÊï∞ÊçÆÁâπÂà´ÊòØÊàêÊú¨Êï∞ÊçÆÔºåËÆ©È´òÂ±ÇÁÆ°ÁêÜËÄÖbuy-in‰Ω†ÁöÑÊÉ≥Ê≥ïÔºåËøôÂ∫îËØ•ÊòØ‰∏™ÈùûÂ∏∏Â•ΩÁöÑËßíÂ∫¶„ÄÇ\nÈ´òÂèØÁî®Êû∂ÊûÑ Â£∞ÊòéÂºèËá™ÊÑàÁ≥ªÁªü‚Äî‚ÄîÈ´òÂèØÁî®ÂàÜÂ∏ÉÂºèÁ≥ªÁªüÁöÑËÆæËÆ°‰πãÈÅì Ëøô‰∏™ÂàÜ‰∫´ÊØîËæÉÁêÜËÆ∫ÂåñÁöÑ‰ªãÁªçÂ£∞ÊòéÂºèÁöÑ„ÄÅÂèØËá™ÊÑàÁöÑÂàÜÂ∏ÉÂºèÁ≥ªÁªüÂéüÁêÜÂíåÂÆûË∑µÔºåÂÖ∂ÂÆû‰∏öÁïåÂ∑≤ÁªèÊúâ‰∏™ÈùûÂ∏∏Â•ΩÁöÑÂèÇËÄÉÂÆûÁé∞ -- Â∞±ÊòØKubernetes \u0026#x1f603;„ÄÇ\nË∂ÖÂ§ßËßÑÊ®°È´òÂèØÁî®ÊÄß‰∫ëÁ´ØÁ≥ªÁªüÊûÑÂª∫‰πãÁ¶Ö ËøôÊòØ‰∏Ä‰∏™ÈùûÂ∏∏ÂÆûÁî®ÁöÑÂ∑•Á®ãÂÆûË∑µÂàÜ‰∫´ÔºåÂàó‰∏æ‰∫ÜÂ§ßÈáèÂ§ßËßÑÊ®°‰∫ëÂéüÁîüÂ∫îÁî®‰∏ÄÂÆö‰ºöÈù¢‰∏¥ÁöÑÊåëÊàòÔºå‰ª•ÂèäÁÆÄÂçïÂèàÂÆûÁî®ÁöÑËß£ÂÜ≥ÊñπÊ≥ï„ÄÇÊØè‰∏Ä‰∏™‰∫ëÂéüÁîüÂ∫îÁî®ÂºÄÂèëËÄÖÈÉΩÂ∫îËØ•ÁúãÁúãËøô‰∏™slideÔºåÂ≠¶‰π†Ââç‰∫∫ÂÆûË∑µÁöÑÁªèÈ™å„ÄÇÂè¶Â§ñ‰∏∫ËÆ≤ÊºîËÄÖËî°Ë∂ÖÂÅö‰∏™Êé®ÂπøÔºåÂØπGoËØ≠Ë®ÄÊúâÂÖ¥Ë∂£ÁöÑÂêåÂ≠¶ÔºåÂèØ‰ª•ËÄÉËôëÂ≠¶‰π†Ëî°Ë∂ÖÁöÑÊûÅÂÆ¢Êó∂Èó¥ËØæÁ®ãGoËØ≠Ë®Ä‰ªéÂÖ•Èó®Âà∞ÂÆûÊàò„ÄÇ\nËøêÁª¥Êû∂ÊûÑ Kubernetes Êó•ÂøóÂπ≥Âè∞Âª∫ËÆæÊúÄ‰Ω≥ÂÆûË∑µ Ëøô‰∏™ÂàÜ‰∫´‰ªãÁªç‰∫ÜKubernetes‰∏äÊó•ÂøóÊñπÊ°àÁöÑËß£ÂÜ≥ÊÄùË∑ØÔºåÂèäÂÆÉÁöÑÂÆûË∑µ -- ÈòøÈáå‰∫ëÁöÑÊó•ÂøóÊúçÂä°„ÄÇÂØπ‰∫éÂæàÂ§öÊúâÂü∫Á°ÄÊúçÂä°Âª∫ËÆæÁöÑÂõ¢ÈòüÂèØ‰ª•‰Ωú‰∏∫ÂæàÂ•ΩÁöÑÂèÇËÄÉÊñπÊ°à„ÄÇÂØπ‰∫éÂ∑≤ÁªèÊâòÁÆ°Âú®ÈòøÈáå‰∫ë‰∏äÁöÑÂ∫îÁî®ÔºåÂª∫ËÆÆÂ∞±‰∏çË¶ÅÈáçÂ§çÂª∫ËÆæ‰ΩéÁ´ØÁöÑËΩÆÂ≠ê‰∫ÜÔºåÈòøÈáå‰∫ëÊó•ÂøóÊúçÂä°Â∫îËØ•ÂÅö‰∏∫Âõ¢ÈòüÁöÑÈ¶ñÈÄâ„ÄÇ‰∏çËÆ∫Âú®ÊÄßËÉΩÂêåÂÖ∂‰ªñ‰∫ëÊâòÁÆ°ÊúçÂä°ÈõÜÊàê‰∏äÔºåÈÉΩËøúËøúÂ•Ω‰∫éËá™Âª∫ÁöÑÊñπÊ°à„ÄÇ\nÂ§ö‰∫ëÁÆ°‰∏ãÁöÑËá™Âä®ÂåñËøêÁª¥Êû∂ÊûÑ Â§ö‰∫ëÊòØÁé∞Âú®‰∏Ä‰∫õÂéÇÂïÜÂäõÊé®ÁöÑËØùÈ¢òÔºå‰∏™‰∫∫ËÆ§‰∏∫ÊòØÂ∏ÇÂú∫ÊéíÂêçÈù†ÂêéÁöÑÊÄªË¶ÅÊâæ‰∫õÊñπÊ≥ïÊù•ÊèêÂçáËá™Â∑±‰∫ßÂìÅÁöÑÁ´û‰∫âÂäõ\u0026#x1f60f;„ÄÇÂàÜ‰∫´ËÄÖ‰ºÅ‰∏öÂÅö‰∫Ü‰∏ÄÂ•óopsÂπ≥Âè∞Êù•ÁÆ°ÁêÜÂ§ö‰∫ëÁöÑËµÑÊ∫êÔºå‰ªñ‰ª¨ÈÄöËøáadapterÊñπÂºèÊù•Â∞Ü‰∏çÂêå‰∫ëÂéÇÂïÜÁöÑÂ∑ÆÂºÇÂíåËµÑÊ∫êËøõË°å‰∫ÜÊäΩË±°„ÄÇËøôÂÖ∂‰∏≠Ê∂âÂèäÂ§ßÈáèÂ§ÑÁêÜ‰∫ßÂìÅÈó¥Â∑ÆÂºÇÊÄßÂíåË¢´Âä®ÈÄÇÈÖçÁöÑÂ∑•‰ΩúÔºå‰∏™‰∫∫‰∏çÂ§™ËÆ§ÂêåËøôÁßçÊñπÂºè„ÄÇÂπ∂‰∏î‰∏¢Êéâ‰∫Üinfra as codeËøôÁ±ªÈáçË¶ÅÁöÑÁâπÊÄßÔºåÂØπ‰∫éÊúâËøôÁßçÈúÄÊ±ÇÁöÑÂ§ßÂûã‰ºÅ‰∏öÊù•ËØ¥‰∏çÊòØ‰∏Ä‰∏™ÂÆåÁæéÁöÑÊñπÊ°à„ÄÇ\nÊ∑∑Ê≤åÂ∑•Á®ã Ê∑∑Ê≤åÂ∑•Á®ãËøô‰∏™ËØùÈ¢òÈùûÂ∏∏ÊúâÊÑèÊÄùÔºåÂêåÊó∂‰πüÊòØËæÉÊñ∞ÁöÑ‰∏ÄÁßçÂÆûË∑µÂ∑•Á®ã„ÄÇ‰ªéÊúÄÊó©ÁöÑÊèêÂá∫„ÄÅÁ≥ªÁªüÂÆûË∑µÂà∞Áé∞Âú®Ëøò‰∏çÂà∞10Âπ¥Êó∂Èó¥„ÄÇÊù•Ëá™ÈòøÈáåÂ∑¥Â∑¥ÁöÑ‰∫ëÂéüÁîüÊû∂ÊûÑ‰∏ãÁöÑÊ∑∑Ê≤åÂ∑•Á®ãÂÆûË∑µÂíåAWSÁöÑAWS ‰∫ë‰∏äÊ∑∑Ê≤åÂ∑•Á®ãÂÆûË∑µ‰πãÂØπÁÖßÂÆûÈ™åËÆæËÆ°ÂíåÂÆûÊñΩ‰∏§‰∏™ÂàÜ‰∫´‰ªãÁªç‰∫Ü‰ªéÊ∑∑Ê≤åÂ∑•Á®ãÁöÑËµ∑Ê∫êÂà∞Â¶Ç‰ΩïÂÖ®Êñπ‰ΩçÁöÑÂÆûË∑µÁî®‰∫éÊèêÂçá‰∫ëÂéüÁîüÂ∫îÁî®ÁöÑ\u0026quot;ÈüßÊÄß\u0026quot;ÔºåÈùûÂ∏∏ÂÄºÂæóÂ≠¶‰π†„ÄÇËî°Ë∂ÖÁöÑË∂ÖÂ§ßËßÑÊ®°È´òÂèØÁî®ÊÄß‰∫ëÁ´ØÁ≥ªÁªüÊûÑÂª∫‰πüÊèêÂà∞‰∫Ü‰ΩøÁî®Ê∑∑Ê≤åÂ∑•Á®ãÊù•ÊèêÂçáÁ≥ªÁªüÁöÑÈ´òÂèØÁî®ÊÄßÔºåÂú®‰∫ëÂéüÁîüÂ∫îÁî®Ë∂äÊù•Ë∂äÊôÆÂèäÁöÑÊÉÖÂÜµ‰∏ãÔºåË¢´Âä®ÁöÑËÆæËÆ°È´òÂèØÁî®Á≥ªÁªüËÇØÂÆö‰∏çÂ¶Ç‰∏ªÂä®(ÁîöËá≥ÊåÅÁª≠ÁöÑËá™Âä®Âåñ)ÂèØÊéßÁöÑÊ≥®ÂÖ•Ê∑∑‰π±Êù•ÈÄêÊ∏êÊèêÂçáÁ≥ªÁªüÁöÑÈ´òÂèØÁî®ÊÄß„ÄÇÁõÆÂâçchaos engineeringÁöÑÂ∑•ÂÖ∑/Âπ≥Âè∞ÊîØÊåÅËøò‰∏çÂ§™ÂÆåÂñÑÔºåËøô‰∏™ÊñπÂêëÁúãËµ∑Êù•ÊòØÊäÄÊúØÂàõ‰∏öÂæàÂ•ΩÁöÑÂàáÂÖ•ÁÇπ\u0026#x1f60f;„ÄÇÊúÄÂêéÂàáËÆ∞‰∏ÄÁÇπÔºåÊ∑∑Ê≤åÂ∑•Á®ãÊúÄÁªà‰∏ÄÂÆöË¶ÅÂú®Áîü‰∫ßÁ≥ªÁªü‰∏äÂÆûÊñΩ„ÄÇ ‰∏ã‰∏Ä‰ª£ÂàÜÂ∏ÉÂºèÂ∫îÁî® Ëøô‰∏™‰∏ªÈ¢òËôΩËØ¥ÂëΩÂêç‰∏∫‰∏ã‰∏Ä‰ª£ÂàÜÂ∏ÉÂºèÂ∫îÁî®Ôºå‰∏ªË¶ÅÂàÜ‰∫´ÁöÑÂ§ßÂ§öÊòØÊúçÂä°Èó¥ÊµÅÈáèÊ≤ªÁêÜÈóÆÈ¢òÔºåÁâπÂà´ÊòØService Mesh‰∏ãÂÆûË∑µÁªèÈ™å„ÄÇÂÖ∂‰∏≠Êù•Ëá™ÈòøÈáåÊùé‰∫ëÁöÑÂàÜÂ∏ÉÂºèÂ∫îÁî®ÁöÑÊú™Êù•‚Äî‚ÄîDistributionlessÁâπÂà´ÂÄºÂæó‰∏ÄÊèê„ÄÇËøô‰∏™ÂàÜ‰∫´Âπ∂Ê≤°ÊúâÂÆûÈôÖÁöÑÊ°à‰æãÊàñÁªèÈ™åÂàÜ‰∫´Ôºå‰ªñÈáçÁÇπÂàÜ‰∫´ÁöÑÊòØÂØπ‰∫éCloud NativeÊú¨Ë¥®ÂíåË∂ãÂäøÁöÑÁúãÊ≥ïÔºåËøô‰∫õËßÇÁÇπÊàë‰∏™‰∫∫ÁâπÂà´ËÆ§Âêå(Â•ΩÂÉèÊâæÂà∞Áü•Èü≥‰ººÁöÑ:grinning:)ÔºÅÂÆåÊï¥ÁöÑslideËøôÈáå‰∏ãËΩΩ„ÄÇ Áî®Êà∑Â¢ûÈïø Êù•Ëá™‰∫ëÊµãÁöÑÈôàÂÜ†ËØöÂú®Êô∫ËÉΩ‰ºòÂåñ \u0026amp; A/B ÊµãËØï - ÂÆûÈ™åÈ©±Âä®Áî®Êà∑Â¢ûÈïøÁöÑÁêÜËÆ∫‰∏éÊäÄÊúØÂÆûË∑µÂàÜ‰∫´‰∫ÜA/BÊµãËØïÂÆûÈ™åÂØπÁî®Êà∑Â¢ûÈïøÁöÑÁêÜËÆ∫ÂèäÂÆûË∑µÔºåÈ°∫‰æø‰πüÊé®Âπø‰∫Ü‰ªñÂÆ∂‰∫ëÊµãÁöÑA/BÊµãËØïSaaSÊúçÂä°„ÄÇÂê¨ÂúàÂÜÖÁöÑÊúãÂèãÂàÜ‰∫´Ôºå‰∫ëÊµãÁöÑA/BÊµãËØïÊúçÂä°Á°ÆÂÆûÊØîËæÉÁÆÄÂçïÂ•ΩÁî®ÔºåÊñπ‰æø‰∫ßÂìÅÂêéÂè∞ÂàõÂª∫ÊµãËØïÂπ∂ÂàÜÊûêÁªìÊûúÔºåÂØπÂ¢ûÈïøÊúâÈúÄÊ±ÇÁöÑÂ∞è‰ºô‰º¥ÂèØ‰ª•ËÄÉËôë‰ΩìÈ™å‰∏ãÔºåÂáèÂ∞ë‰∏çÂøÖË¶ÅÁöÑÈáçÂ§çÂª∫ËÆæËΩÆÂ≠ê„ÄÇ\n","link":"https://kane.mx/posts/2019/2019-qconbeijing-reviews/","section":"posts","tags":["‰ºöËÆÆ","QCon","DevOps","Êû∂ÊûÑ","Ê∑∑Ê≤åÂ∑•Á®ã","Â∑•Á®ãÊïàÁéá"],"title":"QCon2019Âåó‰∫¨Á´ôÂõûÈ°æ"},{"body":"","link":"https://kane.mx/tags/%E4%BC%9A%E8%AE%AE/","section":"tags","tags":null,"title":"‰ºöËÆÆ"},{"body":"","link":"https://kane.mx/tags/%E5%B7%A5%E7%A8%8B%E6%95%88%E7%8E%87/","section":"tags","tags":null,"title":"Â∑•Á®ãÊïàÁéá"},{"body":"","link":"https://kane.mx/tags/%E6%9E%B6%E6%9E%84/","section":"tags","tags":null,"title":"Êû∂ÊûÑ"},{"body":"","link":"https://kane.mx/tags/%E6%B7%B7%E6%B2%8C%E5%B7%A5%E7%A8%8B/","section":"tags","tags":null,"title":"Ê∑∑Ê≤åÂ∑•Á®ã"},{"body":"","link":"https://kane.mx/tags/istio/","section":"tags","tags":null,"title":"Istio"},{"body":"","link":"https://kane.mx/tags/service-mesh/","section":"tags","tags":null,"title":"Service Mesh"},{"body":"","link":"https://kane.mx/tags/spring-cloud/","section":"tags","tags":null,"title":"Spring Cloud"},{"body":"Âü∫‰∫éJavaÁöÑSpring CloudÊòØÁî±JavaÊúÄÂ§ßÂºÄÊ∫êÁîüÊÄÅSpringÁ§æÂå∫Êé®Âá∫ÁöÑOut-of-BoxÂàÜÂ∏ÉÂºèÂæÆÊúçÂä°Ëß£ÂÜ≥ÊñπÊ°àÔºåËá™2016Âπ¥ÂèëÂ∏ÉËµ∑Â∞±Ë¢´‰ºóÂ§öÂºÄÂèëËÄÖÁúãÂ•Ω„ÄÇJava‰Ωú‰∏∫Âπø‰∏∫ÊµÅË°åÁöÑÊúçÂä°Á´ØÁºñÁ®ãËØ≠Ë®ÄÔºåSpring Cloud‰πüÂ∞±Ë∂äÊù•Ë∂äÂ§öÁöÑË¢´Áî®‰∫éÂæÆÊúçÂä°ÂºÄÂèë„ÄÇ\nSpring CloudÈõÜÊàê‰∫ÜNetflix OSSÂºÄÊ∫êÈ°πÁõÆÂÆûÁé∞‰∫ÜÂæàÂ§öÂäüËÉΩ(Êàñ‰Ωú‰∏∫ÂÆûÁé∞‰πã‰∏Ä)ÔºåÂåÖÊã¨ÊúçÂä°Ê≤ªÁêÜ„ÄÅÁΩëÂÖ≥Ë∑ØÁî±„ÄÅÂÆ¢Êà∑Á´ØË¥üËΩΩÂùáË°°„ÄÅÊúçÂä°Èó¥Ë∞ÉÁî®„ÄÅÊñ≠Ë∑ØÂô®Á≠â„ÄÇSpring Cloud NetflixÂ∞ÜÂæàÂ§öÁîü‰∫ßÁ∫ßÂà´ÂæÆÊúçÂä°ËÉΩÂäõÂºÄÁÆ±Âç≥Áî®ÁöÑÂ∏¶Âà∞‰∫ÜSpring CloudÊû∂ÊûÑ‰∏ãÁöÑÂæÆÊúçÂä°‰∏≠ÔºåÂ∏ÆÂä©ÂºÄÂèëËÄÖÂø´ÈÄüÁöÑÊûÑÂª∫Êª°Ë∂≥12Ë¶ÅÁ¥†ÁöÑÂ∫îÁî®„ÄÇ\nÂú®ÂéªÂπ¥Â∫ïÂèëÂ∏ÉÁöÑSpring Cloud GreenwichÁâàÊú¨‰∏≠ÂÆ£Â∏ÉSpring Cloud Netflix‰∏≠ÈáçË¶ÅÁöÑÁªÑ‰ª∂Hystrix„ÄÅRibbon„ÄÅZuul 1Á≠âÁî±‰∫é‰∏äÊ∏∏ÂºÄÊ∫êÈ°πÁõÆËøõÂÖ•Áª¥Êä§Áä∂ÊÄÅÔºåÂØπÂ∫îÁöÑSpring Cloud NetflixÈ°πÁõÆ‰πüËøõÂÖ•Âà∞Áª¥Êä§Áä∂ÊÄÅ„ÄÇËøô‰∫õÈ°πÁõÆÂ∞Ü‰∏çÂÜçÈÄÇÂêàÁî®‰∫éÈïøÊúüÁª¥Êä§ÁöÑ‰∫ßÂìÅ‰∏≠ÔºÅ\nÂêåÊó∂ÈöèÁùÄËøëÂπ¥‰∫ëËÆ°ÁÆóÁöÑÂèëÂ±ïÔºåÁâπÂà´ÊòØKubernetesÊàê‰∏∫ÂÆπÂô®ÁºñÊéíÂπ≥Âè∞ÁöÑ‰∫ãÂÆûÊ†áÂáÜÔºåÂä†‰∏äService Mesh(ÊúçÂä°ÁΩëÊ†º)ÂØπÂæÆÊúçÂä°ÁöÑÊúçÂä°Ê≤ªÁêÜÂíåÊµÅÈáèÊéßÂà∂Ôºå‰∏∫‰∫ëÂéüÁîüÂ∫îÁî®Êèê‰æõ‰∫ÜÊõ¥‰∏∫Áé∞‰ª£„ÄÅÂπ≥Âè∞Êó†ÂÖ≥ÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ\nËÆ©Êàë‰ª¨ÈÄê‰∏ÄÁúãÁúãÂú®KubernetesÂä†‰∏äSerivce Mesh(‰æãÂ¶ÇIstio)Â¶Ç‰ΩïÂÆûÁé∞ÂæÆÊúçÂä°ÁöÑÊúçÂä°ÂèëÁé∞„ÄÅË∑ØÁî±„ÄÅÈìæË∑ØËøΩË∏™„ÄÅÊñ≠Ë∑ØÂô®Á≠âÂäüËÉΩ„ÄÇ\nÈÖçÁΩÆ‰∏≠ÂøÉ Spring Cloud ConfigÈªòËÆ§Êèê‰æõ‰∫ÜÂ§öÁßçÈÖçÁΩÆÁÆ°ÁêÜÂêéÁ´ØÔºå‰æãÂ¶ÇGit„ÄÅVault„ÄÅJDBC BackendÁ≠â„ÄÇÂêåÊó∂‰πüÊúâÂæàÂ§öÂºÄÊ∫êÊñπÊ°àÂèØ‰ª•‰Ωú‰∏∫ÊõøÊç¢ÊñπÊ°àÔºåÊØîÂ¶ÇAlibaba Nacos„ÄÇ\n‰Ωú‰∏∫ÈÉ®ÁΩ≤Âú®Kubernetes‰∏≠ÁöÑÂ∫îÁî®ÔºåÊúÄ‰Ω≥ÂÆûË∑µÊòØÂπ≥Ë°°ConfigmapÂíåSpring Cloud Config„ÄÇÂ∞ÜÊ∂âÂèäÁ®ãÂ∫èÂäüËÉΩÁöÑÈÖçÁΩÆÊîæÁΩÆÂú®ConfigmapÂíåSecretÔºåÈöèÂêåÂæÆÊúçÂä°ÁöÑÂèëÂ∏É‰∏ÄËµ∑ÂÅöÁâàÊú¨ÁÆ°ÁêÜÔºåÂèØ‰ª•ÂÅöÂà∞ÈöèÁùÄÂ∫îÁî®ÂõûÈÄÄÁöÑÊó∂ÂÄôÂêåÊó∂ÂõûÈÄÄÂà∞ÂéÜÂè≤ÂØπÂ∫îÁöÑÈÖçÁΩÆÁâàÊú¨ÔºåËÄå‰∏ç‰ºöÂõ†‰∏∫ÂéÜÂè≤ÁâàÊú¨ÁöÑ‰ª£Á†ÅË¢´ÊúÄÊñ∞ÁâàÊú¨ÁöÑÈÖçÁΩÆÊâÄ‰∏≠Êñ≠„ÄÇSpring Cloud KuberentesÈ°πÁõÆÂæàÂ•ΩÁöÑÊîØÊåÅ‰∫ÜSpring CloudÂ∫îÁî®‰ªéConfigmapÂíåSecret‰∏≠ËØªÂèñÈÖçÁΩÆÈ°π„ÄÇËÄåÊ∂âÂèä‰∏öÂä°ÁöÑÈÖçÁΩÆÈÄâÈ°πÔºåÂ∞ÜÂèØ‰ª•ËÄÉËôëÊîæÂà∞Spring Cloud ConfigÂêéÁ´ØÂÆûÁé∞Áªü‰∏ÄÁÆ°ÁêÜ„ÄÇÂ¶ÇÊûúÂ∫îÁî®ÊòØÈÉ®ÁΩ≤Âú®ÈòøÈáå‰∫ëÔºå‰ΩøÁî®ÈòøÈáå‰∫ëÊâòÁÆ°ÁöÑÈÖçÁΩÆÊúçÂä°ÂíåSpring Cloud Config -- NacosÂ∞ÜÊòØÂæàÂ•ΩÁöÑÈÄâÊã©„ÄÇ\nÊúçÂä°ÂèëÁé∞ Kubernetes ServicesÊèê‰æõ‰∫ÜÈõÜÁæ§ÂÜÖÂéüÁîüÁöÑÊúçÂä°ÂèëÁé∞ËÉΩÂäõÔºåÊòØEurekaÊàñSpring Cloud ZookeeperÁ≠âÊúçÂä°ÂèëÁé∞ÊúçÂä°ÁöÑÂæàÂ•ΩÊõø‰ª£ÂìÅ„ÄÇÂü∫‰∫éK8S ServicesÁöÑÊúçÂä°ÂèëÁé∞ÔºåÂæàÂÆπÊòìÈÄöËøáService MeshËÉΩÂäõÂÆûÁé∞ÈôêÊµÅ„ÄÅA/BÊµãËØï„ÄÅÈáë‰∏ùÈõÄÂèëÂ∏É„ÄÅÊñ≠Ë∑ØÂô®„ÄÅchaosÊ≥®ÂÖ•Á≠âÊúçÂä°Ê≤ªÁêÜËÉΩÂäõ„ÄÇÂêåÊó∂ÂØπÂæÆÊúçÂä°Â∫îÁî®Êù•ËØ¥Ôºå‰∏çÁî®Âú®Â∫îÁî®Á´ØÊ∑ªÂä†ÂØπÂ∫î‰∏âÊñπÂ∫ìÊù•ÂÆûÁé∞ÊúçÂä°Ê≥®ÂÜåÂèäÂèëÁé∞ÔºåÂáèÂ∞ë‰∫ÜÂ∫îÁî®Á´ØÂºÄÂèëÈúÄÊ±Ç„ÄÇ\nÂêÑÁßçÊµÅÈáèÊ≤ªÁêÜÂú∫ÊôØ Â∫îÁî®Ë¢´ÊúçÂä°ÂåñÂêéÔºå‰∏ÄÂÆö‰ºöÈù¢‰∏¥ÊµÅÈáèÊ≤ªÁêÜÁöÑÈóÆÈ¢ò„ÄÇÂØπ‰∫éÂêÑÁßçÊúçÂä°Èó¥Â¶Ç‰ΩïÂÆûÁé∞ÈôêÊµÅ„ÄÅA/BÊµãËØï„ÄÅÈáë‰∏ùÈõÄÂèëÂ∏É„ÄÅÊñ≠Ë∑ØÂô®„ÄÅchaosÊ≥®ÂÖ•ÊµãËØï„ÄÅÈìæÊé•ËøΩË∏™Á≠âÔºåËøôÂÖ∂ÂÆûÊòØ‰∏ÄÁ±ªÈÄöÁî®ÁöÑÈóÆÈ¢ò„ÄÇ\nSpring CloudÊèê‰æõÁöÑÊòØ‰∏ÄÁßçÂÆ¢Êà∑Á´ØËß£ÂÜ≥ÊÄùË∑ØÔºåÈúÄË¶ÅÊØè‰∏™Â∫îÁî®ÂºïÂÖ•ÂØπÂ∫îÂäüËÉΩÁöÑlibrariesÁöÑÊîØÊåÅ„ÄÇÂç≥‰ΩøÈÄöËøáspring boot starterÊèê‰æõ‰∫ÜËøë‰ººÂºÄÁÆ±Âç≥Áî®ÁöÑËÉΩÂäõÔºå‰ΩÜÊòØÊØè‰∏™Â∫îÁî®‰ªçÁÑ∂ÈúÄË¶ÅËá™Ë°åÊ∑ªÂä†ÂØπÂ∫îÁöÑËÉΩÂäõÔºåÁâàÊú¨Êõ¥Êñ∞„ÄÅÂÆâÂÖ®ÊºèÊ¥ûfixÁ≠âÂú∫ÊôØÈÉΩÈúÄË¶ÅÊâãÂä®ÂçáÁ∫ß„ÄÅÊµãËØï„ÄÅÊâìÂåÖ„ÄÅÈÉ®ÁΩ≤„ÄÇÂú®ÂºÇÊûÑÁºñÁ®ãËØ≠Ë®ÄÂÆûÁé∞ÁöÑÂæÆÊúçÂä°Êû∂ÊûÑ‰∏ãÔºåÊú™ÂøÖÊØèÁßçÁºñÁ®ãÊ°ÜÊû∂ÈÉΩËÉΩÊèê‰æõÂæàÂ•ΩÁöÑÂØπÂ∫îËÉΩÂäõÊîØÊåÅ„ÄÇÈô§ÈùûÊúâÁâπÂà´ÁöÑÊúçÂä°Ê≤ªÁêÜÁ≠ñÁï•Ôºå‰∏çÊé®ËçêÂú®ÂæÆÊúçÂä°Ëá™Ë∫´Êù•ÂÆûÁé∞ÊúçÂä°ÊµÅÈáèÁöÑÊéßÂà∂„ÄÇ\nService Mesh(‰æãÂ¶ÇIstioÊàñLinkerd)‰ªéÊï¥‰∏™ÊúçÂä°Ê≤ªÁêÜÂ±ÇÈù¢ÂØπ‰∏äËø∞ÈúÄÊ±ÇÊèê‰æõ‰∫ÜÁªü‰∏ÄÁöÑËß£ÂÜ≥ÊñπÊ°àÔºåËÄå‰∏çÈúÄË¶ÅÂæÆÊúçÂä°ÂÅöËá™Ë∫´ÁöÑÂçáÁ∫ßÊàñÊîπÂä®„ÄÇÂú®Âü∫‰∫éKuberentesÈÉ®ÁΩ≤ËøêË°åÁöÑÂæÆÊúçÂä°Â∫îÁî®ÔºåService MeshÊèê‰æõ‰∫ÜÁªü‰∏ÄÁöÑÊúçÂä°Ê≤ªÁêÜÊñπÊ°àÔºåÂ∞ÜÁî®Êà∑‰ªé‰∏çÂêåÁöÑÂæÆÊúçÂä°‰∏≠Ëá™Ë∫´Áª¥Êä§ÊúçÂä°Ê≤ªÁêÜÂäüËÉΩ‰∏≠Ëß£ÊîæÂá∫Êù•Ôºå‰ªéÂπ≥Âè∞Â±ÇÈù¢Êèê‰æõÊõ¥Âä†Áªü‰∏Ä‰∏ÄËá¥ÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ\nÂú®ÂéªÂπ¥ÁöÑSpringOne Platform 2018‰∏ä‰πüÊúâ‰∏Ä‰∏™Topic A Tale of Two Frameworks: Spring Cloud and Istio Êé¢ËÆ®‰ªÄ‰πàÂú∫ÊôØÂ∫îËØ•‰ΩøÁî®Service MeshÔºå‰ªÄ‰πàÊó∂ÂÄô‰ΩøÁî®Spring CloudÊúçÂä°Ê≤ªÁêÜÁªÑ‰ª∂ÔºåÊúâÂÖ¥Ë∂£ÁöÑÊúãÂèãÂèØ‰ª•Áúã‰∏ÄÁúã„ÄÇ\n","link":"https://kane.mx/posts/effective-cloud-computing/spring-cloud-or-cloud-native/","section":"posts","tags":["‰∫ëËÆ°ÁÆó","kubernetes","spring","spring cloud","service mesh","istio"],"title":"Spring Cloud or Cloud Native"},{"body":"","link":"https://kane.mx/tags/iam/","section":"tags","tags":null,"title":"IAM"},{"body":"","link":"https://kane.mx/tags/oauth2/","section":"tags","tags":null,"title":"Oauth2"},{"body":"Êú¨ÊñáÊòØ‰∏∫Kubernetes‰∏≠‰ªªÊÑèÂ∫îÁî®Ê∑ªÂä†Âü∫‰∫éoauth2ÁöÑËÆ§ËØÅ‰øùÊä§ÁöÑ‰∏ãÁØáÔºåÂ∞ÜÂõæÊñáËØ¶Ëß£Â¶Ç‰Ωï‰ΩøÁî®Âü∫‰∫éÈíâÈíâËÆ§ËØÅÁöÑoauth2 proxy‰∏∫Ëá™Ë∫´Êú¨Ê≤°ÊúâËÆ§ËØÅÊéàÊùÉÂäüËÉΩÁöÑWebÁ´ôÁÇπÂÆûÁé∞ËÆ§ËØÅÂèäÊéàÊùÉ„ÄÇ\nÁ§∫‰æãÊòØ‰ΩøÁî®ÁöÑAWS EKSÊúçÂä°‰Ωú‰∏∫K8SÁéØÂ¢É„ÄÇÈâ¥‰∫éK8SÁöÑÂ∫îÁî®ËøêË°åÊó∂Â±ûÊÄßÔºåËØ•Á§∫‰æã‰πüÂèØ‰ª•ÈÉ®ÁΩ≤Âú®ÂÖ∂‰ªñ‰∫ëÂéÇÂïÜÊâòÁÆ°ÁöÑK8S„ÄÇ\nÁ§∫‰æãÊ®°ÂùóÁÆÄ‰ªã Nginx Ingress Controller‰∏∫K8SÈõÜÁæ§ÂÜÖWebÂ∫îÁî®Êèê‰æõÂèçÂêë‰ª£ÁêÜÔºå‰ª•ÂèäÊîØÊåÅÂ§ñÈÉ®ËÆ§ËØÅ„ÄÇ ÁÆÄÂçïÁöÑWebÁ´ôÁÇπÔºåÂü∫‰∫éNginx dockerÂÆπÂô®„ÄÇËØ•Á´ôÁÇπÈªòËÆ§Ê≤°ÊúâËÆ§ËØÅÂèäÊéàÊùÉÂäüËÉΩÔºå‰ΩøÁî®Â§ñÈÉ®ÈíâÈíâÂ∫îÁî®‰Ωú‰∏∫ËÆ§ËØÅÂèäÊéàÊùÉ„ÄÇ OAuth2 Proxy on DingtalkÊèê‰æõÂü∫‰∫éÈíâÈíâÂ∫îÁî®ÁöÑÊâ´Á†ÅËÆ§ËØÅÂèäÊéàÊùÉÔºåÂè™ÊúâËÆ§ËØÅ‰∏îÊéàÊùÉÁöÑÁî®Êà∑ÊâçÂèØ‰ª•ËÆøÈóÆ‰∏äÈù¢ÁöÑWebÁ´ôÁÇπ„ÄÇ ÈªòËÆ§ËÆæÂÆö WebÁ´ôÁÇπÂüüÂêçweb.kane.mx ËÆ§ËØÅÊúçÂä°ÂüüÂêçoauth.kane.mx ÂáÜÂ§áAWS EKSÁéØÂ¢É ÂàõÂª∫EKSÈõÜÁæ§„ÄÇÁî±‰∫éNginx IngressÊúçÂä°ÊòØLoadBalancerÁ±ªÂûãÔºåEKSÂàõÂª∫NLBÊàñELBÂØπÂ∫îÁöÑtargetsÊó∂ÈúÄË¶ÅtargetsÈÉ®ÁΩ≤Âú®public VPC subnetsÔºåÊâÄ‰ª•‰∏∫‰∫ÜÁÆÄÂåñÈÉ®ÁΩ≤EKSÈõÜÁæ§ÁöÑVPC subnetsÈÉΩÈÄâÊã©public subnet„ÄÇÊñ∞Âª∫ÁöÑEKSÈõÜÁæ§ÂÖÅËÆ∏ÂÖ¨ÂºÄËÆøÈóÆ„ÄÇ Êú¨Âú∞ÂÆâË£ÖÈÖçÁΩÆkubectl, aws-iam-authenticatorÁî®‰∫éËøúÁ®ãÁÆ°ÁêÜÈõÜÁæ§„ÄÇ ‰∏∫ÈõÜÁæ§Ê∑ªÂä†workerËäÇÁÇπ„ÄÇ ÈÖçÁΩÆHelmÈÉ®ÁΩ≤ÁéØÂ¢É„ÄÇ ÈíâÈíâÂ∫îÁî®ÂáÜÂ§á ‰∏∫‰ºÅ‰∏öÊàñÁªÑÁªáÂºÄÈÄöÈíâÈíâÂºÄÂèëÂπ≥Âè∞ ÂàõÂª∫‰∏Ä‰∏™Êñ∞ÁöÑÁßªÂä®Â∫îÁî®„ÄÇÂõûË∞ÉÂüüÂêçÂ°´ÂÜô\u0026lt;http or https\u0026gt;/\u0026lt;ËÆ§ËØÅÊúçÂä°ÂüüÂêç\u0026gt;/oauth2/callback„ÄÇËÆ∞ÂΩï‰∏ãÊù•Â∫îÁî®ÁöÑappIdÂíåappSecret„ÄÇ ÂàõÂª∫‰∏Ä‰∏™‰ºÅ‰∏öÂÜÖÈÉ®Â∑•‰ΩúÂè∞Â∫îÁî®„ÄÇÂú∞ÂùÄÂèØ‰ª•ÈöèÊÑèËÆæÁΩÆ„ÄÇÊúçÂä°Âô®Âá∫Âè£IPËÆæÁΩÆ‰∏∫EKSÈõÜÁæ§‰∏≠Â∑•‰ΩúËäÇÁÇπÁöÑÂÖ¨ÁΩëIPÊàñËÄÖNAT EIPÔºåÂèñÂÜ≥‰∫éÂ∑•‰ΩúËäÇÁÇπÂ¶Ç‰ΩïËÆøÈóÆInternet„ÄÇÂπ∂ËÆ∞ÂΩï‰∏ãÊù•Â∫îÁî®appKeyÂíåappSecret„ÄÇ ÈÉ®ÁΩ≤Á§∫‰æãÂ∫îÁî® ÂÖãÈöÜÁ§∫‰æãÈÉ®ÁΩ≤ËÑöÊú¨„ÄÇ ÊõøÊç¢values.yaml‰∏≠ÁöÑdingtalk_corpid‰∏∫Â∑•‰ΩúÂè∞Â∫îÁî®ÁöÑappKeyÔºå dingtalk_corpsecret‰∏∫Â∑•‰ΩúÂè∞Â∫îÁî®ÁöÑappSecret„ÄÇ Áî±‰∫éÁ§æÂå∫Áª¥Êä§ÁöÑoauth2-proxy chartsÂπ∂‰∏çÊîØÊåÅdingtalkÊâ©Â±ïÁöÑSECRET ENVÔºåÊâÄ‰ª•Â∞ÜÂØÜÈí•ÈÖçÁΩÆÂà∞‰∫Üconfigmap‰∏≠„ÄÇÁî®‰∫éÁîü‰∫ßÁéØÂ¢ÉÁöÑËØùÔºåÂª∫ËÆÆÊåâËøô‰∏™commit‰ΩøÁî®secret‰øùÂ≠òÂ∫îÁî®secret„ÄÇ 62 63 64 65 66 67 68 69 70 71 72 oauth2-proxy: config: clientID: aaa clientSecret: bbb cookieSecret: ccc configFile: |+ email_domains = [ \u0026#34;*\u0026#34; ] cookie_domain = \u0026#34;kane.mx\u0026#34; cookie_secure = false dingtalk_corpid = \u0026#34;\u0026lt;appkey of dingtalk app\u0026gt;\u0026#34; dingtalk_corpsecret = \u0026#34;\u0026lt;appsecret of dingtalk app\u0026gt;\u0026#34; Â¶ÇÊûú‰ªÖÂ∏åÊúõ‰ºÅ‰∏öÈÉ®ÂàÜÈÉ®Èó®ÁöÑÂëòÂ∑•ÂèØ‰ª•Ëé∑ÂæóÊéàÊùÉÔºåÂú®‰∏äÈù¢configFileÈÖçÁΩÆ‰∏ãÊ∑ªÂä†Â¶Ç‰∏ãÈÖçÁΩÆÔºå 1dingtalk_departments = [\u0026#34;xxÂÖ¨Âè∏/‰∫ßÂìÅÊäÄÊúØ‰∏≠ÂøÉ\u0026#34;,\u0026#34;xxÂÖ¨Âè∏/ÈÉ®Èó®2/Â≠êÈÉ®Èó®3\u0026#34;] ÊõøÊç¢ÈÉ®ÁΩ≤Â∫îÁî®ÁöÑÂüüÂêç‰∏∫‰Ω†ÁöÑÂüüÂêç„ÄÇ ÊâßË°å‰ª•‰∏ãÂëΩ‰ª§ÂÆâË£ÖHelmÈÉ®ÁΩ≤‰æùËµñ„ÄÇ 1helm dep up ÊâßË°å‰ª•‰∏ãÂëΩ‰ª§ÈÉ®ÁΩ≤nginx ingress controller, webÂ∫îÁî®‰ª•Âèäoauth2 proxy 1helm upgrade --install -f values.yaml --set oauth2-proxy.config.clientID=\u0026lt;ÁßªÂä®Â∫îÁî®appid\u0026gt;,oauth2-proxy.config.clientSecret=\u0026lt;ÁßªÂä®Â∫îÁî®appsecret\u0026gt; site-with-auth --wait ./ Â¶ÇÊûúÈõÜÁæ§‰∏≠Â∑≤ÁªèÈÉ®ÁΩ≤‰∫ÜNginx Ingress ControllerÔºå‰øÆÊîπvalues.yamlÂ¶Ç‰∏ãÂ∞ÜÂøΩÁï•ÈÉ®ÁΩ≤Nginx ingressÔºå 47 48 49 50 51 52 53 affinity: {} nginx-ingress: enabled: false controller: ingressClass: nginx config: ÈÉ®ÁΩ≤ÊàêÂäüÂêéÔºåËé∑ÂèñELBÂú∞ÂùÄ„ÄÇ 1kubectl get svc -o jsonpath=\u0026#39;{ $.status.loadBalancer.ingress[*].hostname }\u0026#39; \u0026lt;deployment name\u0026gt;-nginx-ingress-controller;echo 2a3afe672259c511e98e2a0a0d88fda3e-xx.elb.ap-southeast-1.amazonaws.com ÈÉ®ÁΩ≤ÊàêÂäüÂêéÈÖçÁΩÆ Â∞ÜÁ´ôÁÇπÂíåoauthÊúçÂä°ÂüüÂêçËß£ÊûêÂà∞‰∏äÈù¢ÈÉ®ÁΩ≤ÂàõÂª∫ÁöÑELB‰∏ä„ÄÇ\nÊµãËØï ËÆøÈóÆWebÁ´ôÁÇπ(Â¶ÇÊú¨Á§∫‰æã‰∏≠ÁöÑhttp://web.kane.mx)ÔºåÊú™ÊéàÊùÉÁöÑÊÉÖÂÜµ‰∏ãÔºåË∞ÉËΩ¨Âà∞ÈíâÈíâÂ∫îÁî®Êâ´Á†ÅÁôªÂΩïÁïåÈù¢„ÄÇ‰ΩøÁî®ÁªÑÁªáÂÜÖÊàêÂëòÁöÑÈíâÈíâÊâ´Á†ÅÊéàÊùÉÂêéÔºåÂ∞ÜË∑≥ËΩ¨ÂõûWebÁ´ôÁÇπÂ∫îÁî®ÔºåÂèØ‰ª•Ê≠£Â∏∏ÊµèËßàËØ•ÂüüÂêç‰∏ãÁöÑÈ°µÈù¢„ÄÇ\n","link":"https://kane.mx/posts/effective-cloud-computing/oauth2-proxy-on-kubernetes/part2/","section":"posts","tags":["‰∫ëËÆ°ÁÆó","IAM","kubernetes","oauth2","ÈíâÈíâ","dingtalk","AWS","AWS EKS"],"title":"‰∏∫Kubernetes‰∏≠‰ªªÊÑèÂ∫îÁî®Ê∑ªÂä†Âü∫‰∫éoauth2ÁöÑËÆ§ËØÅ‰øùÊä§ (‰∏ã)"},{"body":"Áî±‰∫é‰ºÅ‰∏öÂÜÖÈÉ®ÁÆ°ÁêÜÁöÑÈúÄË¶ÅÔºåÁî®Âà∞‰∫ÜÈíâÈíâÁöÑ‰∏öÂä°‰∫ã‰ª∂ÂõûË∞ÉËÉΩÂäõÔºåÊ≠£Â•ΩÂ∞ÜËøô‰∏™ËΩªÈáèÁ∫ßÁöÑÊé•Âè£‰ΩøÁî®Êó†ÊúçÂä°Âô®ÊäÄÊúØÊù•ÂÆûÁé∞ÈÉ®ÁΩ≤Ôºå‰ª•Â∫îÂØπÊµÅÈáèÊó†ËßÑÂæã‰∏ãÁöÑÂä®ÊÄÅÊâ©Â±ï‰º∏Áº©„ÄÅÊåâÈúÄ‰ΩøÁî®„ÄÅÊåâÈáèËÆ°Ë¥πÁ≠âÈúÄÊ±Ç„ÄÇ\nÈòøÈáå‰∫ëÂáΩÊï∞ËÆ°ÁÆóÁâàÊú¨ Áî±‰∫éÂÖ¨Âè∏Á≥ªÁªüÈÉ®ÁΩ≤Âú®ÈòøÈáå‰∫ëÔºåÈ¶ñÂÖàÈÄâÊã©‰ΩøÁî®ÈòøÈáå‰∫ëÂáΩÊï∞ËÆ°ÁÆóÊù•ÂÆûÁé∞ÂèäÈÉ®ÁΩ≤„ÄÇËØ•Êé•Âè£‰ΩøÁî®‰∫ÜJVM‰∏äËØ≠Ë®ÄKotlinÂºÄÂèëÔºåËôΩÁÑ∂ÈòøÈáå‰∫ëÂáΩÊï∞ËÆ°ÁÆóÂÆòÊñπÊîØÊåÅÁöÑÂºÄÂèëËØ≠Ë®ÄÊúâJava‰ΩÜÊ≤°ÊúâKotlin„ÄÇÂÖ∂ÂÆûÊó†ËÆ∫JavaÊàñKotlinÊúÄÁªàÈÉ®ÁΩ≤Êñá‰ª∂ÈÉΩÊòØJava ClassÂ≠óËäÇÁ†ÅÔºåÂä†‰∏äKotlin‰∏éJavaËâØÂ•ΩÁöÑ‰∫íÊìç‰ΩúÊÄßÔºåÂÆûÊµãÂáΩÊï∞ËÆ°ÁÆóÂèØ‰ª•ÂÆåÁæéÊîØÊåÅKotlinÂºÄÂèë(‰∏™‰∫∫ËÆ§‰∏∫‰ªªÊÑèJVM‰∏äÁöÑÂºÄÂèëËØ≠Ë®ÄÈÉΩÊòØÊîØÊåÅÁöÑ)„ÄÇ\nÂêåÊó∂ËØ•ÂáΩÊï∞‰ΩøÁî®‰∫ÜË°®Ê†ºÂ≠òÂÇ®Êù•ÊåÅ‰πÖÂåñÂõûË∞É‰∫ã‰ª∂„ÄÇË°®Ê†ºÂ≠òÂÇ®ÊòØ‰∏™ÊåâÈáèËÆ°Ë¥πÁöÑÂàÜÂ∏ÉÂºèÂ≠òÂÇ®ÔºåÊúâÂÖ¥Ë∂£ÁöÑÂèØ‰ª•Ëá™Ë°åÊü•ÈòÖÊñáÊ°£‰∫ÜËß£Êõ¥Â§ö„ÄÇ\nËØ•ÂáΩÊï∞ÈÄöËøáAPIÁΩëÂÖ≥ÂíåË°®Ê†ºÂ≠òÂÇ®Ëß¶ÂèëÂô®Êù•Ëß¶Âèë„ÄÇËÆøÈóÆÊó•ÂøóÂíåÊâßË°åÊó•ÂøóË¢´Â≠òÂÇ®Âú®Êó•ÂøóÊúçÂä°‰∏≠„ÄÇ\nÂáΩÊï∞ÁöÑÊú¨Âú∞ÊµãËØïÂíåÁ∫ø‰∏äÈÉ®ÁΩ≤Ôºå‰ΩøÁî®‰∫ÜÂáΩÊï∞ËÆ°ÁÆóÊèê‰æõÁöÑÂëΩ‰ª§Ë°åÂ∑•ÂÖ∑Fun„ÄÇÂü∫‰∫éFunÂÆö‰πâÁöÑÈòøÈáå‰∫ëServerlessÊ®°ÂûãÂÆûÁé∞‰∫ÜÂØπÂáΩÊï∞‰ª¨‰ΩøÁî®ËµÑÊ∫êÁöÑÂ£∞ÊòéÂíåÁºñÊéíÔºåÈõÜÊàêGitlab CIÂÆûÁé∞‰∫ÜÂáΩÊï∞ÁöÑCI/CDËá™Âä®ÂåñÂèëÂ∏ÉÊµÅÁ®ã„ÄÇ\n‰∏çÊ∂âÂèäÂÖ¨Âè∏‰∏öÂä°ÁöÑ‰ª£Á†ÅÂ∑≤ÂºÄÊ∫êÂú®GithubÔºåÊúâÂÖ¥Ë∂£ÁöÑÂèØ‰ª•‰Ωú‰∏∫ÂèÇËÄÉ„ÄÇ\nÁõÆÂâçÂáΩÊï∞ËÆ°ÁÆóÂíåË°®Ê†ºÂ≠òÂÇ®ÊúâÂêÑËá™ÁöÑÂÖçË¥πÈÖçÈ¢ùÔºåÂú®‰∏öÂä°Èáè‰∏çÂ§ßÁöÑÊÉÖÂÜµ‰∏ãÔºåËØ•ÊúçÂä°ÂÆåÂÖ®ÂÖçË¥π„ÄÇ\nAWS LambdaÁâàÊú¨ AWS LambdaÊòØÁõÆÂâçÂÖ®ÁêÉ‰ΩøÁî®ÊúÄ‰∏∫ÂπøÊ≥õÁöÑserverlessÊúçÂä°ÔºåÂêåÊó∂‰πüÊòØÂáΩÊï∞ËÆ°ÁÆóÂèëÂ±ïÊñπÂêëÁöÑÂºïÈ¢ÜËÄÖ„ÄÇ\nÁî±‰∫é‰∏Ä‰∫õ‰∏™‰∫∫ÂéüÂõ†ÔºåÁ¨îËÄÖÊúÄËøëÊé•Ëß¶‰∫ÜÈÉ®ÂàÜAWSÊúçÂä°ÔºåÂêåÊó∂Â∞ùËØïÂ∞ÜÈíâÈíâÂõûË∞ÉÂáΩÊï∞ÁßªÊ§çÂà∞‰∫ÜAWS Lambda‰∏ä„ÄÇÈòøÈáå‰∫ë‰∏ä‰ΩøÁî®ÁöÑ‰∫ëÊúçÂä°Êîπ‰∏∫Áî±AWS‰∏äÂØπÂ∫îÊúçÂä°Êù•ÂÆûÁé∞Ôºå‰æãÂ¶ÇÂ≠òÂÇ®‰ΩøÁî®‰∫ÜDynamoDBÔºåÊó•Âøó‰ΩøÁî®CloudWatchÊî∂ÈõÜÂíåÊü•ËØ¢„ÄÇ\nÊú¨Âú∞ÊµãËØïÂíåÈÉ®ÁΩ≤Â∑•ÂÖ∑Ôºå‰ΩøÁî®ÁöÑÊòØSAM CLIÔºåÊåÅÁª≠ÈõÜÊàêÂíåÊåÅÁª≠ÈÉ®ÁΩ≤‰ΩøÁî®ÁöÑÊòØAWS CodeBuildÂíåAWS CodePipeline„ÄÇÊ≠§Â§ñAWSÈÄöËøáAWS CloudFormationÊèê‰æõ‰∏ÄÁßçÈùûÂ∏∏Âº∫Â§ßÁöÑËÉΩÂäõÔºåÂèØ‰ª•Â∞ÜAWS‰∏äÁöÑÂêÑÁßçËµÑÊ∫êÈÄöËøáÈÖçÁΩÆÂ£∞ÊòéÁöÑÊñπÂºèÊù•ÁÆ°ÁêÜ(‰πüÂ∞±ÊòØÁé∞Âú®ÈùûÂ∏∏ÁÉ≠Èó®ÁöÑ‰∏Ä‰∏™Ê¶ÇÂøµ--Infrastructure as Code)„ÄÇAWS CloudFormation‰ºö‰∏∫ÊØèÊ¨°‰∏Ä‰∏™ÊàñÂ§ö‰∏™ËµÑÊ∫êÁöÑÂèòÊõ¥ÁîüÊàêChangeSetÔºåÊèê‰æõÊü•ÁúãÂØπÊØî„ÄÅÁâàÊú¨ÁÆ°ÁêÜ„ÄÅÈÅáÂà∞ÂèòÊõ¥ÈîôËØØÊï¥‰ΩìÂõûÈÄÄÁ≠âËÉΩÂäõ„ÄÇÊâÄ‰ª•ÔºåAWSÁâàÊú¨‰πüÂ∞ÜËØ•È°πÁõÆÁöÑCI/CDÈÉ®ÁΩ≤Áî®Âà∞ÁöÑAWS CodeBuild„ÄÅAWS CodePipeline„ÄÅAmazon DynamoDBÁ≠âËµÑÊ∫êÈÄöËøáCloudFormationÁöÑÈÖçÁΩÆÁÆ°ÁêÜËµ∑Êù•„ÄÇ\nÈÖçÁΩÆ‰ª£Á†ÅÊÆµÂ¶Ç‰∏ãÔºå 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 Description: Create a CodePipeline to include Github source, CodeBuild and Lambda deployment. Parameters: AppBaseName: Type: String Description: App base name Default: dingtalk-callback ArtifactStoreS3Location: Type: String Description: Name of the S3 bucket to store CodePipeline artificat. BranchName: Description: GitHub branch name Type: String Default: master RepositoryName: Description: GitHub repository name Type: String Default: dingtalk-callback-on-aws GitHubOAuthToken: Type: String NoEcho: true Resources: BuildDingtalkProject: Type: AWS::CodeBuild::Project Properties: Name: Fn::Sub: ${AppBaseName}-build-${AWS::StackName} Description: Build, test, package dingtalk callback project ServiceRole: Fn::GetAtt: [ CodeBuildRole, Arn ] Artifacts: Type: S3 Location: Ref: ArtifactStoreS3Location Name: Fn::Sub: ${AppBaseName}-build-${AWS::StackName} NamespaceType: BUILD_ID Path: Fn::Sub: ${AppBaseName}/artifacts Packaging: NONE OverrideArtifactName: true EncryptionDisabled: true Environment: Type: LINUX_CONTAINER ComputeType: BUILD_GENERAL1_SMALL Image: aws/codebuild/java:openjdk-11 PrivilegedMode: false ImagePullCredentialsType: CODEBUILD EnvironmentVariables: - Name: s3_bucket Value: Ref: ArtifactStoreS3Location Source: DingtalkCallbackPipeline: Type: \u0026#39;AWS::CodePipeline::Pipeline\u0026#39; Properties: Name: Fn::Sub: ${AppBaseName}-pipeline-${AWS::StackName} RoleArn: Fn::GetAtt: [ CodePipelineRole, Arn ] Stages: - Name: Source Actions: - Name: SourceAction ActionTypeId: Category: Source Owner: ThirdParty Version: 1 Provider: GitHub OutputArtifacts: - Name: Fn::Sub: ${AppBaseName}-source-changed Configuration: Owner: !Ref GitHubOwner Repo: !Ref RepositoryName Branch: !Ref BranchName OAuthToken: !Ref GitHubOAuthToken PollForSourceChanges: false RunOrder: 1 - Name: Build Actions: - Name: Build_Test_Package InputArtifacts: - Name: Fn::Sub: ${AppBaseName}-source-changed ActionTypeId: Category: Build Owner: AWS Version: 1 Provider: CodeBuild OutputArtifacts: - Name: Fn::Sub: ${AppBaseName}-packaged-yml Configuration: ProjectName: Ref: BuildDingtalkProject RunOrder: 1 AWSÁâàÊú¨ÂÆåÊï¥ÁöÑ‰ª£Á†Å„ÄÅCloudFormationÈÖçÁΩÆ‰ª•ÂèäÈÉ®ÁΩ≤ÊñáÊ°£ÂèØ‰ª•ÈÄöËøáËøôÈáåÊü•Áúã„ÄÇ\n","link":"https://kane.mx/posts/effective-cloud-computing/serverless-dingtalk-callback/","section":"posts","tags":["‰∫ëËÆ°ÁÆó","FaaS","ÈòøÈáå‰∫ë","ÂáΩÊï∞ËÆ°ÁÆó","AWS","AWS Lambda","ÈíâÈíâ","dingtalk","Serverless Computing"],"title":"Âü∫‰∫éÂáΩÊï∞ËÆ°ÁÆóÁöÑÈíâÈíâÂõûË∞ÉÂáΩÊï∞Êé•Âè£"},{"body":"Serverless Computing(Êó†ÊúçÂä°Âô®ËÆ°ÁÆó)ÊòØÁõÆÂâçÊúÄË¢´ÁúãÂ•ΩÁöÑ‰∫ëÁ´ØËÆ°ÁÆóÊâßË°åÊ®°Âûã„ÄÇÂÖ∂ÊúÄÂ§ßÁöÑÂ•ΩÂ§ÑÊòØÊèê‰æõÂàÜÂ∏ÉÂºèÂºπÊÄßÂèØ‰º∏Áº©ÁöÑËÆ°ÁÆóÊâßË°åÁéØÂ¢ÉÔºå‰ªÖ‰∏∫ÂÆûÈôÖ‰ΩøÁî®ËµÑÊ∫ê‰ªòË¥πÔºåÂπ∂‰∏îÂ∞ÜÂ∫îÁî®Áª¥Êä§ËÄÖ‰ªéÂ∏∏ËßÑÁöÑËøêÁª¥‰∫ãÂä°‰∏≠Ëß£ÊîæÂá∫Êù•ÔºåÊõ¥Âà©‰∫é‰∏ìÊ≥®Âà∞ÂÖ∑‰ΩìÁöÑ‰∏öÂä°‰∏ä„ÄÇ\nÂú®‰∏ªÊµÅÁöÑÂ∫îÁî®ÈÉ®ÁΩ≤ÊñπÂºè‰∏ãÔºåÊó†ËÆ∫ÊòØ‰ΩøÁî®‰∫ë‰∏ªÊú∫ËøòÊòØKubernetes‰Ωú‰∏∫ËøêË°åÁéØÂ¢ÉÔºåÈÉΩ‰ºöÊúâÂ§ßÈáèËøêÁª¥Â±ÇÈù¢ÁöÑ‰∫ãÂä°ÈúÄË¶ÅËÄÉËôëÂíåÂ§ÑÁêÜÔºåÂπ∂‰∏îÂ∫îÁî®Á®ãÂ∫èÈúÄË¶ÅÊåâÁÖßÂàÜÂ∏ÉÂºèÁ®ãÂ∫èÁöÑËÆæËÆ°ÂáÜÂàôÊù•Â∫îÂØπÂ∫îÁî®ÁöÑÊ∞¥Âπ≥‰º∏Áº©„ÄÇÂêåÊó∂ÈöèÁùÄ‰∫ëËÆ°ÁÆóÊúçÂä°ÁöÑÂèëÂ±ïÂíåÂÆåÂñÑÔºå‰∫ëËÆ°ÁÆóÂéÇÂïÜÊèê‰æõ‰∫ÜË∂äÊù•Ë∂äÂ§öÁöÑÂü∫Á°ÄÊúçÂä°Ôºå‰æãÂ¶ÇAPIÁΩëÂÖ≥„ÄÅÂØπË±°Â≠òÂÇ®„ÄÅÊ∂àÊÅØÈòüÂàó„ÄÅÊó•Âøó„ÄÅÁõëÊéßÁ≠âÊúçÂä°ÔºåÂáΩÊï∞ËÆ°ÁÆóÂèØ‰ª•ÂÆåÁæéÁöÑÂêåÂÖ∂‰ªñ‰∫ëÊúçÂä°ÈõÜÊàêÔºåÂ∏ÆÂä©Áî®Êà∑Âø´ÈÄüÂÆûÁé∞Âá∫Áîü‰∫ßÁ∫ßÂà´ÁöÑÂºπÊÄßÂèØ‰º∏Áº©ÁöÑÂ∫îÁî®„ÄÇ\nÈÇ£ÂáΩÊï∞ËÆ°ÁÆóÊòØ‰ªÄ‰πàÂë¢ÔºüËÆ©Êàë‰ª¨‰∏ÄËµ∑Êù•ÁúãÁúãÈòøÈáå‰∫ëÂØπ‰∫éÂáΩÊï∞ËÆ°ÁÆóÁöÑÂÆö‰πâ„ÄÇ\nÈòøÈáå‰∫ëÂáΩÊï∞ËÆ°ÁÆóÊòØ‰∫ã‰ª∂È©±Âä®ÁöÑÂÖ®ÊâòÁÆ°ËÆ°ÁÆóÊúçÂä°„ÄÇÈÄöËøáÂáΩÊï∞ËÆ°ÁÆóÔºåÊÇ®Êó†ÈúÄÁÆ°ÁêÜÊúçÂä°Âô®Á≠âÂü∫Á°ÄËÆæÊñΩÔºåÂè™ÈúÄÁºñÂÜô‰ª£Á†ÅÂπ∂‰∏ä‰º†„ÄÇÂáΩÊï∞ËÆ°ÁÆó‰ºö‰∏∫ÊÇ®ÂáÜÂ§áÂ•ΩËÆ°ÁÆóËµÑÊ∫êÔºå‰ª•ÂºπÊÄß„ÄÅÂèØÈù†ÁöÑÊñπÂºèËøêË°åÊÇ®ÁöÑ‰ª£Á†ÅÔºåÂπ∂Êèê‰æõÊó•ÂøóÊü•ËØ¢„ÄÅÊÄßËÉΩÁõëÊéß„ÄÅÊä•Ë≠¶Á≠âÂäüËÉΩ„ÄÇÂÄüÂä©‰∫éÂáΩÊï∞ËÆ°ÁÆóÔºåÊÇ®ÂèØ‰ª•Âø´ÈÄüÊûÑÂª∫‰ªª‰ΩïÁ±ªÂûãÁöÑÂ∫îÁî®ÂíåÊúçÂä°ÔºåÊó†ÈúÄÁÆ°ÁêÜÂíåËøêÁª¥„ÄÇËÄå‰∏îÔºåÊÇ®Âè™ÈúÄË¶Å‰∏∫‰ª£Á†ÅÂÆûÈôÖËøêË°åÊâÄÊ∂àËÄóÁöÑËµÑÊ∫ê‰ªòË¥πÔºå‰ª£Á†ÅÊú™ËøêË°åÂàô‰∏ç‰∫ßÁîüË¥πÁî®„ÄÇ\nÂü∫‰∫éÂáΩÊï∞ËÆ°ÁÆóÁöÑÁâπÁÇπÔºåÂèØ‰ª•ÂæàÂ•ΩÊª°Ë∂≥‰ª•‰∏ãÈúÄÊ±ÇÔºå\n‰∏öÂä°ÊµÅÈáè‰∏çÁ°ÆÂÆöÊàñÊúâÊòéÁªÜÁöÑÂë®ÊúüÊÄß ÊûÑÂª∫ÂàÜÂ∏ÉÂºèÁ≥ªÁªüÁªèÈ™å‰∏çË∂≥ Êó†ÈúÄËøêÁª¥ ÊåâÈúÄËÆ°ÁÆó ËÆ°Ë¥πÁÅµÊ¥ª Áî±‰∫éÂáΩÊï∞ËÆ°ÁÆóÁöÑÊâ©Â±ïËÉΩÂäõÔºåÂØπËøêÁª¥ÁöÑË¶ÅÊ±ÇÊûÅÂ∞ëÔºåÊåâÈáèËÆ°Ë¥πÁ≠âÁâπÊÄßÁî®‰∫éÈúÄË¶ÅÂø´ÈÄüÈ™åËØÅÁöÑÊó©ÊúüÈ°πÁõÆ‰πüÊòØÈùûÂ∏∏Â•ΩÁöÑÂú∫ÊôØ„ÄÇ\n‰∏ãÈù¢Ëøô‰∏™slideÊòØËøëÊúüÈíàÂØπÈòøÈáå‰∫ëÂáΩÊï∞ËÆ°ÁÆóÂÅöÁöÑÂàÜ‰∫´„ÄÇ\n","link":"https://kane.mx/posts/effective-cloud-computing/serverless-computing-101/","section":"posts","tags":["‰∫ëËÆ°ÁÆó","FaaS","ÈòøÈáå‰∫ë","ÂáΩÊï∞ËÆ°ÁÆó","Serverless Computing"],"title":"Êó†ÊúçÂä°Âô®ËÆ°ÁÆó101"},{"body":"‰ºÅ‰∏öÈöèÁùÄ‰∏öÂä°ÁöÑÂèëÂ±ïÔºåÂøÖÁÑ∂‰ºöÈÉ®ÁΩ≤ÂêÑÁßçÂêÑÊ†∑ÁöÑITÁ≥ªÁªü„ÄÇÂá∫‰∫éÂÆâÂÖ®ÊÄßÁöÑËÄÉËôëÔºå‰∏Ä‰∫õÁ≥ªÁªü‰ªÖÂèØ‰ºÅ‰∏öÂÜÖÈÉ®‰ΩøÁî®ÔºåÁîöËá≥‰ªÖÂºÄÊîæÁªô‰ºÅ‰∏öÈÉ®ÂàÜÈÉ®Èó®ÂëòÂ∑•‰ΩøÁî®„ÄÇ\nËøô‰∫õITÁ≥ªÁªüÂ§ßËá¥ÂèØÂàÜ‰∏∫‰∏§Á±ªÔºå\nÁ≥ªÁªüÊú¨Ë∫´‰∏çÊîØÊåÅ‰ªª‰ΩïËÆ§ËØÅÊú∫Âà∂Ôºå‰æãÂ¶ÇËµÑËÆØÊàñÊñáÊ°£Á±ªÁ≥ªÁªü„ÄÇÈúÄË¶ÅÂ¢ûÂä†ËÆ§ËØÅ‰øùÊä§ÔºåËÉΩÂ§üÈôêÂà∂Èùû‰ºÅ‰∏öÂëòÂ∑•ËÆøÈóÆÂç≥ÂèØ„ÄÇÁ≥ªÁªüËøêÁª¥ÈÄöÂ∏∏ÁöÑÂÅöÊ≥ïÊòØÔºå‰∏∫Á´ôÁÇπËÆæÁΩÆHTTP BasicËÆ§ËØÅ‰øùÊä§„ÄÇÁî±‰∫éHTTP BasicËÆ§ËØÅÊòØÈÄöËøáÈ¢ÑËÆæÁöÑÁî®Êà∑„ÄÅÂØÜÁ†ÅËÆ§ËØÅÔºåËÆ§ËØÅ‰ø°ÊÅØÊØîËæÉÂÆπÊòìÊ≥ÑÈú≤„ÄÇÂç≥‰ΩøÂÆöÊúüÊõ¥Êç¢ÂØÜÁ†ÅÔºå‰ΩÜÈúÄË¶ÅÈ¢ùÂ§ñÁöÑÊú∫Âà∂ÈÄöÁü•Áî®Êà∑ÂØÜÁ†ÅÁöÑÂèòÊõ¥ÔºåÁî®Êà∑‰ΩìÈ™å‰πü‰∏çÂ•Ω„ÄÇ Á≥ªÁªüËá™Ë∫´ÊîØÊåÅËÆ§ËØÅÔºåÁîöËá≥ÊîØÊåÅÂ§öÁßçËÆ§ËØÅÊú∫Âà∂„ÄÇÊØîÂ¶ÇÊúÄÂ∏∏Áî®ÁöÑÂºÄÊ∫êCI/CDÂ∑•ÂÖ∑ÔºåJenkinsÂÜÖÁΩÆÊîØÊåÅÊú¨Âú∞Êï∞ÊçÆÂ∫ìËÆ§ËØÅ„ÄÅÈÄöËøáÊèí‰ª∂ÊîØÊåÅÂ§öÁßçÁ¨¨‰∏âÊñπÁ≥ªÁªüÈõÜÊàêËÆ§ËØÅ„ÄÇÂ¶ÇÊûúÂ§ßÈáèÁöÑITÁ≥ªÁªüÈÉΩÊúâ‰∏ÄÂ•óÁã¨Á´ãÁöÑÁî®Êà∑ÁÆ°ÁêÜÔºåÈöèÁùÄ‰ºÅ‰∏öÁöÑÂëòÂ∑•ÁöÑÂèòÊõ¥ÔºåÁî®Êà∑ÁöÑÂ¢ûÂà†Á≠âÊìç‰ΩúÂØπÁ≥ªÁªüÁÆ°ÁêÜÂëòÊù•ËØ¥ÊòØ‰∏çÂ∞èÁöÑÂ∑•‰ΩúÈáè„ÄÇÂêåÊó∂Ôºå‰πüÂæàÂÆπÊòìÁî±‰∫é‰∫∫‰∏∫ÁñèÂøΩÔºåÈÄ†ÊàêËµÑ‰∫ß„ÄÅÊï∞ÊçÆÁöÑÂÆâÂÖ®ÈöêÊÇ£„ÄÇ ÂÅáËÆæ‰ºÅ‰∏öËá™Ë∫´Â∑≤ÁªèÊúâ‰∫Ü‰∏ÄÂ•óOAÁ≥ªÁªüÂåÖÂê´ÂëòÂ∑•„ÄÅÁªÑÁªáÁªìÊûÑÁÆ°ÁêÜÔºå‰æãÂ¶ÇÔºåÂõΩÂÜÖÁõÆÂâçÊúÄ‰∏∫ÊôÆÂèäÊµÅË°åÁöÑÈíâÈíâÊàñ‰ºÅ‰∏öÂæÆ‰ø°„ÄÇÊàë‰ª¨ÂÆåÂÖ®ÂèØ‰ª•Êèê‰æõ‰∏ÄÂ•óÂü∫‰∫éoauth 2.0ÂçèËÆÆÁöÑËÆ§ËØÅÊñπÂºèÔºåËÆ©‰ª•‰∏ä‰∏§Á±ªITÁ≥ªÁªü‰ΩøÁî®‰ºÅ‰∏öÂ∑≤ÊúâÁöÑOAÁ≥ªÁªü(ÈíâÈíâÊàñ‰ºÅ‰∏öÂæÆ‰ø°)Êù•ÂÆûÁé∞ÁôªÂΩïËÆ§ËØÅ„ÄÇÂÅöÂà∞Ëøô‰∏ÄÁÇπÂêéÔºå‰ºÅ‰∏öÊó†ËÆ∫ÊúâÂ§öÂ∞ëITÁ≥ªÁªüÈÉΩ‰∏çÂÜçÈúÄË¶ÅÈ¢ùÂ§ñÁÆ°ÁêÜÁî®Êà∑ÁöÑÊàêÊú¨ÔºåÂπ∂‰∏î‰πüÈÅøÂÖç‰∫ÜÊï∞ÊçÆÂÆâÂÖ®ÈöêÊÇ£„ÄÇ\nÈíâÈíâÈÄöËøáÈíâÈíâÂºÄÊîæÂπ≥Âè∞Êèê‰æõÁöÑAPIÂºÄÊîæ‰∫ÜËÆ∏Â§öÈíâÈíâÂÜÖÈÉ®ÁöÑËÉΩÂäõÔºå‰æãÂ¶ÇÔºåË∫´‰ªΩÈ™åËØÅ„ÄÅÈÄöËÆØÂΩïÁÆ°ÁêÜÁ≠âÁ≠â„ÄÇÁÑ∂ËÄåÈíâÈíâÁöÑ‰∏âÊñπÁΩëÁ´ôÁôªÂΩïÊé•Âè£Âπ∂‰∏çÊòØÊ†áÂáÜÁöÑoauth 2.0ÂçèËÆÆÂÆûÁé∞ÔºåÊàë‰ª¨ÈúÄË¶ÅÈÄöËøá‰∏Ä‰∏™oauth2 proxy‰ª£ÁêÜÂ∑•ÂÖ∑ÂÆûÁé∞Â∞ÜÈíâÈíâÁöÑ‰∏âÊñπÁΩëÁ´ôÁôªÂΩïÂÖºÂÆπoauth2ÂçèËÆÆ„ÄÇÂêåÁêÜÔºå‰ΩøÁî®Ëøô‰∏™oauth2‰ª£ÁêÜÂ∑•ÂÖ∑ÔºåÂèØ‰ª•‰ΩøÁî®Google„ÄÅFacebookÁ≠â‰∏âÊñπÁΩëÁ´ô‰Ωú‰∏∫Áªü‰∏ÄËÆ§ËØÅÊñπÂºè„ÄÇ\nÊúâ‰∫ÜÂü∫‰∫éÈíâÈíâÁöÑoauth2‰ª£ÁêÜ‰Ωú‰∏∫‰ºÅ‰∏öÁªü‰∏ÄÁôªÂΩïÊñπÂºèÔºåÂØπ‰∫é‰∏äÈù¢‰∏§Â§ßÁ±ªÁ≥ªÁªüÁöÑËÆ§ËØÅÈúÄÊ±ÇËß£ÂÜ≥ÊñπÊ°àÂàÜÂà´Â¶Ç‰∏ãÔºå\nÈÉ®ÁΩ≤Âú®Kubernetes‰∏≠Êó†ÂÜÖÁΩÆËÆ§ËØÅÊú∫Âà∂ÁöÑWebÂ∫îÁî®ÔºåÈÄöËøánginx-ingressÁöÑÂ§ñÈÉ®OAUTHËÆ§ËØÅÂÆûÁé∞Âü∫‰∫éoauth2ÁöÑÂÆâÂÖ®ËÆ§ËØÅ„ÄÇ JenkinsÂèØ‰ª•ÈÄöËøáÂèçÂêë‰ª£ÁêÜÊèí‰ª∂ÂÆûÁé∞‰ΩøÁî®oauth2ËÆ§ËØÅÁôªÂΩï„ÄÇ Âú®‰∏ãÁØá‰∏≠ÔºåÊàë‰ª¨Â∞ÜÂõæÊñáËØ¶Ëß£Â¶Ç‰Ωï‰∏ÄÊ≠•Ê≠•ÂÆûÁé∞‰∏∫‰∏Ä‰∏™Êó†ËÆ§ËØÅÁöÑ‰ºÅ‰∏öÊñáÊ°£WebÂ∫îÁî®Ê∑ªÂä†Âü∫‰∫éÈíâÈíâÁöÑÁªü‰∏ÄËÆ§ËØÅ„ÄÇ\n","link":"https://kane.mx/posts/effective-cloud-computing/oauth2-proxy-on-kubernetes/part1/","section":"posts","tags":["‰∫ëËÆ°ÁÆó","IAM","kubernetes","oauth2","ÈíâÈíâ","dingtalk"],"title":"‰∏∫Kubernetes‰∏≠‰ªªÊÑèÂ∫îÁî®Ê∑ªÂä†Âü∫‰∫éoauth2ÁöÑËÆ§ËØÅ‰øùÊä§ (‰∏ä)"},{"body":"‰ºÅ‰∏ö‰ΩøÁî®ÂÖ¨Êúâ‰∫ëÊúçÂä°ÁöÑÁ¨¨‰∏Ä‰ª∂‰∫ãÊÉÖÂ∞±ÊòØÂàõÂª∫‰∫ëÂ∏êÂè∑ÔºåÊúâ‰∫ÜÂ∏êÂè∑‰πãÂêéÂ¶Ç‰ΩïËÆ©‰ºÅ‰∏öÂëòÂ∑•ÂÆâÂÖ®ÂêàËßÑÁöÑ‰ΩøÁî®‰∫ëÂ∏êÂè∑‰∏ãÁöÑÂêÑÁßçËµÑÊ∫êÊòØÂºÄÂêØ‰∫ë‰πãÊóÖÂêéÁöÑÁ¨¨‰∏Ä‰∏™ËÄÉÈ™å„ÄÇ\n‰∫ëËÆ°ÁÆóÂéÇÂïÜÈíàÂØπ‰ºÅ‰∏ö‰∏ä‰∫ëÂêéÈù¢‰∏¥ÁöÑÁ¨¨‰∏Ä‰∏™ÈúÄÊ±ÇÂ∑≤ÁªèÊé®Âá∫‰∫ÜÂÆåÂñÑÁöÑËß£ÂÜ≥ÊñπÊ°à--Identity and Access Management„ÄÇIAMÂèØ‰ª•Â∏ÆÂä©‰∫ëÂ∏êÂè∑ÂÆâÂÖ®Âú∞ÊéßÂà∂ÂØπ‰∫ëËÆ°ÁÆóÊúçÂä°ËµÑÊ∫êÁöÑËÆøÈóÆ„ÄÇ‰ºÅ‰∏öÂèØ‰ª•‰ΩøÁî®IAMÊéßÂà∂ÂØπÂì™‰∏™Áî®Êà∑ËøõË°åË∫´‰ªΩÈ™åËØÅ (ÁôªÂΩï) ÂíåÊéàÊùÉ (ÂÖ∑ÊúâÊùÉÈôê) ‰ª•‰ΩøÁî®ËµÑÊ∫ê„ÄÇ\n‰∫ëÂéÇÂïÜÊòØÂê¶Êèê‰æõÂÆåÂñÑÁöÑIAMÊúçÂä°ÂèØ‰ª•‰Ωú‰∏∫Êï¥‰Ωì‰∫ßÂìÅËß£ÂÜ≥ÊñπÊ°àÊòØÂê¶ÊàêÁÜüÁöÑ‰∏Ä‰∏™Ë°°ÈáèÊåáÊ†áÔºåÊØîÂ¶ÇAWSÁöÑIAMÂíåÈòøÈáå‰∫ëÁöÑËÆøÈóÆÊéßÂà∂ÈÉΩÊòØËæÉ‰∏∫ÊàêÁÜüÂÆåÂñÑÁöÑ‰∫ßÂìÅ„ÄÇÂõΩÂÜÖÊüê‰∏™‰ª•AIËÉΩÂäõ‰∏∫ÂçñÁÇπÁöÑ‰∫ëÂéÇÂïÜÔºåÂú®IAM‰∫ßÂìÅÊñπÈù¢Âá†‰πé‰∏∫Èõ∂ÔºåÂæàÈöæÁõ∏‰ø°ÂØπÂÆâÂÖ®ÂêàËßÑÊúâÈúÄÊ±ÇÁöÑ‰ºÅ‰∏ö‰ºöÂÆåÊï¥‰ΩøÁî®‰ªñÁöÑ‰∫ë‰∫ßÂìÅ‰Ωú‰∏∫Ëß£ÂÜ≥ÊñπÊ°à„ÄÇ\nIAMÈÄöÂ∏∏Êèê‰æõ‰ª•‰∏ãÂäüËÉΩ:\nÂØπ‰∫ëË¥¶Êà∑ÁöÑÂÖ±‰∫´ËÆøÈóÆÊùÉÈôê ÂÖÅËÆ∏Âú®‰∏Ä‰∏™‰∫ëË¥¶Êà∑‰∏ãÂàõÂª∫Âπ∂ÁÆ°ÁêÜÂ§ö‰∏™Áî®Êà∑Ë∫´‰ªΩÔºåÂπ∂ÂÖÅËÆ∏ÁªôÂçï‰∏™Ë∫´‰ªΩÊàñ‰∏ÄÁªÑË∫´‰ªΩÔºàÊó¢ÂèØ‰ª•ÊòØÂΩìÂâç‰∫ëÂ∏êÂè∑‰∏ã‰πüÂèØ‰ª•ÊòØÂÖ∂‰ªñ‰∫ëÂ∏êÂè∑‰∏ãÔºâÂàÜÈÖç‰∏çÂêåÁöÑÊùÉÈôêÁ≠ñÁï•Ôºå‰ªéËÄåÂÆûÁé∞‰∏çÂêåÁî®Êà∑Êã•Êúâ‰∏çÂêåÁöÑ‰∫ëËµÑÊ∫êËÆøÈóÆÊùÉÈôêÔºåËÄå‰∏çÂøÖÂÖ±‰∫´‰∫ëÂ∏êÂè∑Ê†πÁî®Êà∑ÁöÑÂØÜÁ†ÅÊàñËÆøÈóÆÂØÜÈí•„ÄÇ\nÁ≤æÁªÜÊùÉÈôê ÂèØ‰ª•ÈíàÂØπ‰∏çÂêåËµÑÊ∫êÂêë‰∏çÂêå‰∫∫ÂëòÊéà‰∫à‰∏çÂêåÊùÉÈôê„ÄÇÂèØ‰ª•Ë¶ÅÊ±ÇÁî®Êà∑ÂøÖÈ°ª‰ΩøÁî®ÂÆâÂÖ®‰ø°ÈÅìÔºàÂ¶Ç SSLÔºâ„ÄÅÂú®ÊåáÂÆöÊó∂Èó¥ËåÉÂõ¥„ÄÅÊàñÂú®ÊåáÂÆöÊ∫ê IP Êù°‰ª∂‰∏ãÊâçËÉΩÊìç‰ΩúÊåáÂÆöÁöÑ‰∫ëËµÑÊ∫ê„ÄÇ\nÂ§öÈáçÈ™åËØÅ (MFA) ÂèØ‰ª•Âêë‰∫ëË¥¶Êà∑ÂíåÂêÑ‰∏™Áî®Êà∑Ê∑ªÂä†ÂèåÈáçË∫´‰ªΩÈ™åËØÅ‰ª•ÂÆûÁé∞Êõ¥È´òÂÆâÂÖ®ÊÄß„ÄÇÂÄüÂä©MFAÔºåÁî®Êà∑‰∏ç‰ªÖÂøÖÈ°ªÊèê‰æõ‰ΩøÁî®Ë¥¶Êà∑ÊâÄÈúÄÁöÑÂØÜÁ†ÅÊàñËÆøÈóÆÂØÜÈí•ÔºåËøòÂøÖÈ°ªÊèê‰æõÊù•Ëá™ÁªèËøáÁâπÊÆäÈÖçÁΩÆÁöÑËÆæÂ§áÁöÑ‰ª£Á†Å„ÄÇ\nËÅîÂêàË∫´‰ªΩ ÂèØ‰ª•ÂÖÅËÆ∏Â∑≤Âú®ÂÖ∂‰ªñ‰ΩçÁΩÆÔºà‰æãÂ¶ÇÔºåÂú®‰ºÅ‰∏öÁΩëÁªú‰∏≠ÊàñÈÄöËøá Internet Ë∫´‰ªΩÊèê‰æõÂïÜÔºâËé∑ÂæóÂØÜÁ†ÅÁöÑÁî®Êà∑Ëé∑ÂèñÂØπ‰∫ëË¥¶Êà∑ÁöÑÁî®Êà∑ËÆøÈóÆÊùÉÈôê„ÄÇ\nÂêéÈù¢‰ºöÊúâ‰∏ìÈó®ÁöÑÊñáÁ´†Êù•ËÆ≤Â¶Ç‰ΩïÂÆûË∑µËÅîÂêàË∫´‰ªΩ„ÄÇ\nÁªü‰∏ÄË¥¶Âçï ‰∫ëË¥¶Êà∑Êé•Êî∂ÂåÖÊã¨ÊâÄÊúâÁî®Êà∑ÁöÑËµÑÊ∫êÊìç‰ΩúÊâÄÂèëÁîüË¥πÁî®ÁöÑÁªü‰∏ÄË¥¶Âçï„ÄÇ\nÂ∞ΩÁÆ°IAMÊèê‰æõ‰∫Ü‰∏äÈù¢ÁßçÁßçÂäüËÉΩÔºå‰∫ëÂ∏êÂè∑ÁöÑÁÆ°ÁêÜËÄÖ‰ªçÂèØÈÄöËøá‰∏Ä‰∫õÊúÄ‰Ω≥ÂÆûË∑µÊù•Êõ¥Â•ΩÁöÑ‰ΩøÁî®IAM‰∫ßÂìÅÊù•ÊèêÂçáÂÆâÂÖ®Á∫ßÂà´ÂíåÂáèÂ∞ëËøêÁª¥ÊàêÊú¨„ÄÇ\nIAMÊúÄ‰Ω≥ÂÆûË∑µ Â∞ΩÈáè‰∏çË¶Å‰ΩøÁî®‰∫ëÂ∏êÂè∑ÁöÑÊ†πÁî®Êà∑Ôºå‰∏çË¶Å‰∏∫Ê†πÁî®Êà∑ÂàõÂª∫AK„ÄÇ‰∫ëÂ∏êÂè∑ÁÆ°ÁêÜÂëò‰πü‰ΩøÁî®ÂêÑËá™Áã¨Á´ãÁöÑÂ≠êË¥¶Âè∑„ÄÇ ‰∏∫‰ºÅ‰∏ö‰∏≠ÊØè‰∏Ä‰∏™ÈúÄË¶Å‰ΩøÁî®‰∫ëÊúçÂä°ÁöÑÂëòÂ∑•ÂçïÁã¨ÂàõÂª∫Â≠êË¥¶Êà∑Ôºå‰∏îÈªòËÆ§‰∏çÂÖÅËÆ∏ÂàõÂª∫AK„ÄÇ‰æø‰∫éÂëòÂ∑•Á¶ªËÅåÁöÑÊó∂ÂÄôÔºåÈÄöËøáÂà†Èô§Â∏êÂè∑Êù•ÂÆåÂÖ®Ê∏ÖÁêÜÁî®Êà∑Âú®‰∫ëËÆ°ÁÆóÂπ≥Âè∞ÁöÑÂêÑÁßçÊùÉÈôê„ÄÇ ÂØÜÁ†ÅÂÆâÂÖ®ÂÆûË∑µÔºå ÈôêÂà∂ÂØÜÁ†ÅÂº∫Â∫¶‰∏çÂ∞ë‰∫é8‰ΩçÔºåÂøÖÈ°ªÁî±Â§ßÂ∞èÂÜôÂ≠óÊØç„ÄÅÊï∞Â≠óÂíåÁ¨¶Âè∑‰∏≠ÁöÑ‰∏âÁßçÁªÑÊàê„ÄÇ Âº∫Âà∂ÂØÜÁ†ÅËøáÊúüÊó∂Èó¥‰∏çË∂ÖËøá90Â§©Ôºå‰∏îËøáÊúüÂêé‰∏çÂèØÁôªÂΩï„ÄÇ Êñ∞ÂØÜÁ†ÅËá≥Â∞ëÁ¶ÅÊ≠¢‰ΩøÁî®Ââç3Ê¨°ÂØÜÁ†Å„ÄÇ ËÆæÁΩÆÂØÜÁ†ÅÈáçËØïÁ∫¶ÊùüÔºå‰æãÂ¶ÇÔºå‰∏ÄÂ∞èÊó∂ÂÜÖ‰ΩøÁî®ÈîôËØØÂØÜÁ†ÅÊúÄÂ§ßÂ∞ùËØï9Ê¨°ÁôªÂΩï„ÄÇ Âº∫Âà∂ÊâÄÊúâÁî®Êà∑ÂêØÁî®‰∏§Ê≠•ËÆ§ËØÅ„ÄÇ ÂØπËÆøÈóÆÁΩëÁªúÊúâÈôêÂà∂ÁöÑ‰ºÅ‰∏öÔºåÂèØ‰ª•ÂºÄÂêØÁôªÂΩïIPÈôêÂà∂„ÄÇ [Êé®ËçêÂÅöÊ≥ï]Â∑≤ÊúâSSOÂçïÁÇπÁôªÂΩïÁ≥ªÁªüÁöÑ‰ºÅ‰∏öÔºåÂèØ‰ª•ÈÄöËøáSAML 2.0Ê†áÂáÜÂÆûÁé∞‰ªé‰ºÅ‰∏öÊú¨Âú∞Ë¥¶Âè∑Á≥ªÁªüÁôªÂΩïÂà∞ÈòøÈáå‰∫ëÔºå‰ªéËÄåÊª°Ë∂≥‰ºÅ‰∏öÁöÑÁªü‰∏ÄÁî®Êà∑ÁôªÂΩïËÆ§ËØÅË¶ÅÊ±Ç„ÄÇ ÁªÜÁ≤íÂ∫¶ÁöÑÊùÉÈôêÁÆ°ÁêÜÔºå ‰∏∫ÂêÑÁßç‰∫ëËµÑÊ∫êÂàõÂª∫ÊúÄÁªÜÁ≤íÂ∫¶ÁöÑÊùÉÈôêÁ≠ñÁï•„ÄÇ‰æãÂ¶ÇÔºåÂàÜÂà´‰∏∫RDSÂÆû‰æãrds-instance-1ÂàõÂª∫Âè™ËØªÊùÉÈôêÁ≠ñÁï•rds-instance-1-readonly-accessÔºåRDSÂÆû‰æãrds-instance-2ÂàõÂª∫Âè™ËØªÊùÉÈôêÁ≠ñÁï•rds-instance-2-readonly-access„ÄÇ Ê†πÊçÆËÅåËÉΩ„ÄÅÈÉ®Èó®Á≠âÁª¥Â∫¶‰∏∫‰∫ëÂ∏êÂè∑Â≠êÁî®Êà∑ÂàõÂª∫Áî®Êà∑ÁªÑ„ÄÇ‰æãÂ¶ÇÔºåÊåâÈ°πÁõÆÂàõÂª∫Áî®Êà∑ÁªÑÔºågroup-project-aÔºågroup-project-b„ÄÇÂ¶ÇÊûúproject-aÁî®Êà∑ÈúÄË¶ÅËÆøÈóÆrds-instance-1ÁöÑ‰ø°ÊÅØÔºåÂ∞ÜËá™ÂÆö‰πâÊùÉÈôêrds-instance-1-readonly-accessÊéàÊùÉÁªôgroup-project-a„ÄÇÂÜçÂ∞ÜÁõ∏ÂÖ≥Áî®Êà∑Âä†Âà∞Áî®Êà∑ÁªÑgroup-project-a‰∏≠ÔºåËøôÊ†∑Ëøô‰∫õÁî®Êà∑Â∞±ÂÖ∑ÊúâÂè™ËØªËÆøÈóÆRDSÂÆû‰æãrds-instance-1ÁöÑÊùÉÈôê„ÄÇËÄå‰∏çÊòØÂ∞ÜÊâÄÊúâRDSÁöÑËØªÂÜôÊùÉÈôêÈÉΩÊéà‰∫àËøô‰∫õÁî®Êà∑ÔºåÊúÄÂ§ßÈôêÂ∫¶ÁöÑ‰øùËØÅÁî®Êà∑‰∏çËé∑ÂèñË∂ÖËøáÂÆûÈôÖÈúÄË¶ÅÁöÑÊùÉÈôê„ÄÇ Âú®ÂÆûÈôÖÂú∫ÊôØ‰∏≠ÔºåÈÄöÂ∏∏‰ºöÈÄöËøá‰∫ëËÆ°ÁÆóÊúçÂä°ÁöÑAPIÊù•ÂÆåÊàêÊüê‰∫õÂë®ÊúüÊÄß‰ªªÂä°ÔºåÊØîÂ¶ÇÊØèÊó•RDS‰∏≠ÁöÑÊÖ¢Êü•ËØ¢ÁªüËÆ°„ÄÅ‰∫ëÂ∏êÂè∑ÊØèÊó•Ëä±Ë¥πÁªüËÆ°Á≠â„ÄÇËøô‰∫õ‰ªªÂä°ÈÉΩÈúÄË¶Å‰∏Ä‰∏™‰∫ëÂ∏êÂè∑ÁöÑAKÊù•ÂÆåÊàêAPIÁöÑË∫´‰ªΩËÆ§ËØÅ„ÄÇÊúÄ‰Ω≥ÁöÑÂÅöÊ≥ïÊòØÔºå‰∏∫ÊØèÁ±ªÁõ∏ÂÖ≥ÁöÑ‰ªªÂä°ÂàõÂª∫‰∏Ä‰∏™ÂäüËÉΩÊÄßÂ≠êË¥¶Âè∑ÔºåÁ¶ÅÁî®‰ªñ‰ª¨ÁöÑwebÁôªÂΩïÔºå‰∏îÈÅµÂæ™ÁâπÊÆäÁöÑÂëΩÂêçËßÑËåÉ(functional-ÂºÄÂ§¥)ÔºåÊØîÂ¶Çfunctional-rds-stats„ÄÅfunctional-cost-stats„ÄÇÂàõÂª∫ÊúÄÂ∞èÁöÑÊùÉÈôêÁ≠ñÁï•ÔºåÁÑ∂ÂêéÂàÜÈÖçÁªôËøô‰∫õÂäüËÉΩÊÄßÁî®Êà∑„ÄÇ‰æãÂ¶ÇÔºåfunctional-rds-stats‰ªÖË¢´Êéà‰∫àRDSÂè™ËØªÊùÉÈôêÔºåfunctional-cost-stats‰ªÖË¢´Êéà‰∫àË¥πÁî®ÁöÑÂè™ËØªÊùÉÈôê„ÄÇ‰∏∫Ëøô‰∫õÂ≠êË¥¶Âè∑ÂàõÂª∫AKÔºåÊØèÁ±ª‰ªªÂä°‰ΩøÁî®‰∏çÂêåÁöÑAKÊù•ÂÆåÊàêAPIËÆ§ËØÅÔºåËÄå‰∏çÊòØÈÉΩ‰ΩøÁî®Âêå‰∏Ä‰∏™AK„ÄÇËøôÊ†∑ÁöÑÂ•ΩÂ§ÑÊòØÔºå‰∏çÂêåÁ±ªÂûã‰ªªÂä°ÁöÑAKÂÖ∑Êúâ‰∏çÂêåÁöÑÊùÉÈôêÔºåÊúÄÂ§ßÈôêÂ∫¶ÁöÑ‰øùÊä§‰∫Ü‰∫ëÂ∏êÂè∑ÁöÑÂÆâÂÖ®ÔºåÂπ∂‰∏îËøô‰∫õAK‰∏çË∑üÂÆûÈôÖÁöÑÂëòÂ∑•Â≠êË¥¶Âè∑ÂÖ≥ËÅîÔºå‰∏ç‰ºöÂõ†‰∏∫ÂëòÂ∑•Â∏êÂè∑ÁöÑÂèòÊõ¥ËÄåÂèóÂΩ±Âìç„ÄÇÂ¶ÇÊúâÊõ¥È´òÁöÑÂÆâÂÖ®ÂêàËßÑÁöÑË¶ÅÊ±Ç‰∏ãÔºåÂèØ‰ª•ÂÆöÊúü‰ΩúÂ∫üÂ∑≤ÊúâAKÔºåÂàõÂª∫Êñ∞AKÊõøÊç¢„ÄÇËá≥‰∫éAKÊÄéÊ†∑ÂÆâÂÖ®ÁÆ°ÁêÜÔºå‰πãÂêé‰ºöÊúâ‰∏ìÈó®ÁöÑÊñáÁ´†Êù•ËØ¶Ëß£„ÄÇ ","link":"https://kane.mx/posts/effective-cloud-computing/iam-best-practice/","section":"posts","tags":["‰∫ëËÆ°ÁÆó","ÈòøÈáå‰∫ë","AWS","IAM"],"title":"IAMÊúÄ‰Ω≥ÂÆûË∑µ"},{"body":"ËøôÊòØ‚ÄúÂ¶Ç‰ΩïÈ´òÊïà‰ΩøÁî®‰∫ëÊúçÂä°‚ÄùÁ≥ªÂàóÊñáÁ´†ÁöÑÈ¶ñÁØáÂàÜ‰∫´„ÄÇÂèØËÉΩÊúâÊúãÂèãÂ•ΩÂ•á‰∏∫‰ªÄ‰πà‰∏çÊòØ‰ªé‰∫ëËÆ°ÁÆóÊúÄÂü∫Á°ÄÁöÑÊúçÂä°--ËÆ°ÁÆóËµÑÊ∫êECS/EC2ËÆ≤Ëµ∑Âë¢ÔºüÂú®Cloud NativeÂ∑≤ÁªèË¢´Ë∂äÊù•Ë∂äÊé•ÂèóÁöÑ‰ªäÂ§©ÔºåÂü∫‰∫éKubernetesÈÉ®ÁΩ≤„ÄÅÁºñÊéíÂ∫îÁî®ÁöÑÊñπÂºèÂ∑≤ÁªèÊòØ‰∏öÁïåÁöÑ‰∫ãÂÆûÊ†áÂáÜ„ÄÇÊó†ËÆ∫ÊòØ‰∫íËÅîÁΩëÂ∑®Â§¥Ôºå‰º†Áªü500Âº∫‰ºÅ‰∏öÔºåËøòÊòØÂàõ‰∏öÂõ¢ÈòüÈÉΩÂú®‰ΩøÁî®ÊàñËßÑÂàí‰ΩøÁî®Kubernetes‰Ωú‰∏∫Â∫îÁî®Á®ãÂ∫èÁöÑËá™Âä®ÂåñÈÉ®ÁΩ≤„ÄÅÂèØÊâ©Â±ïÁÆ°ÁêÜÂπ≥Âè∞„ÄÇÂú®‰∫ëËÆ°ÁÆóÂπ≥Âè∞ÔºåËôöÊãüÊú∫Ë∂äÊù•Ë∂ä‰∏çÈúÄË¶ÅÂçïÁã¨ÁöÑÁÆ°ÁêÜÔºåÂú®ÁªùÂ§ßÂ§öÊï∞ÁöÑ‰∏öÂä°Âú∫ÊôØ‰∏ãÔºåÂÆÉ‰ª¨Âè™ÊòØ‰Ωú‰∏∫ÂÆπÂô®ÈõÜÁæ§ÊâÄÁÆ°ÁêÜÁöÑËÆ°ÁÆóËµÑÊ∫ê„ÄÇÁîöËá≥ËôöÊãüÊú∫ÁöÑÂàõÂª∫Âà∞ÈîÄÊØÅÊï¥‰∏™ÁîüÂëΩÂë®ÊúüÁÆ°ÁêÜÈÉΩÂèØ‰ª•Áî±KubernetesÊ†πÊçÆÈõÜÁæ§ÁöÑË¥üËΩΩÊù•Ëá™Âä®ÂÆåÊàê„ÄÇ\nÊâÄÊúâ‰∏ªÊµÅÁöÑ‰∫ëËÆ°ÁÆóÂéÇÂïÜÈÉΩÂú®Ëß£ÂÜ≥ÊñπÊ°à‰∏≠ÂäõÊé®ÊâòÁÆ°ÁöÑKubernetesÔºåAWSÁöÑEKSÔºåAzure‰∏äÁöÑAKSÔºåÂΩìÁÑ∂Â∞ë‰∏ç‰∫ÜGoogleÂÆ∂GCP‰∏äÁöÑKubernetes Engine„ÄÇÂõΩÂÜÖÈòøÈáå‰∫ëÔºåËÖæËÆØ‰∫ëÁ≠âÊØè‰∏Ä‰∏™ÂÖ¨Êúâ‰∫ëÁé©ÂÆ∂‰πüÈÉΩÂü∫‰∫éÂºÄÊ∫êKubernetesÊé®Âá∫‰∫ÜÊâòÁÆ°ÊúçÂä°„ÄÇÂ¶ÇÊûú‰∏ÄÂÆ∂‰∫ëËÆ°ÁÆóÂéÇÂïÜÂú®Êèê‰æõÊâòÁÆ°KubernetesËøô‰∏ÄÊúçÂä°‰∏äÊ≤°Ë∑ü‰∏ä‰∏öÁïåÁöÑÊ≠•‰ºêÔºåÂ∞ÜÊù•ÊûÅÂ§ßÂèØËÉΩË¢´Ê∑òÊ±∞Âá∫Ëøô‰∏™Â∏ÇÂú∫„ÄÇ\nÊâòÁÆ°ÁöÑKubernetesÁ±ªÂûã ‰ª•ÂõΩÂÜÖÁöÑÈòøÈáå‰∫ë‰∏∫‰æãÔºåÁõÆÂâçÊèê‰æõ‰∫Ü‰∏§Â§ßÁ±ª‰∏âÁßç‰∏çÂêåÁöÑKubernetesÊâòÁÆ°ÊúçÂä°„ÄÇ\nÁªèÂÖ∏Dedicated KubernetesÊ®°Âºè„ÄÇËøôÁßçÊ®°Âºè‰∏ãÁî®Êà∑ÂèØ‰ª•ÈÄâÊã©ÂÆø‰∏ªÊú∫ÂÆû‰æãËßÑÊ†ºÂíåÊìç‰ΩúÁ≥ªÁªüÔºåÊåáÂÆöKubernetesÁâàÊú¨„ÄÅËá™ÂÆö‰πâKubernetesÁâπÊÄßÂºÄÂÖ≥ËÆæÁΩÆÁ≠â„ÄÇÁî®Êà∑ÈúÄË¶ÅÊâãÂä®Áª¥Êä§ÈõÜÁæ§Ôºå‰æãÂ¶ÇÂçáÁ∫ßKubernetesÁâàÊú¨ÔºåÂÜÖÁΩÆÁªÑ‰ª∂ÁâàÊú¨Á≠â„ÄÇÂèØ‰ª•ÊâãÂä®ÊàñËá™Âä®‰º∏Áº©ÈõÜÁæ§ËäÇÁÇπÊï∞ÁõÆ„ÄÇÁõÆÂâçËØ•Ê®°Âºè‰∏ãÊúâ‰∏§ÁßçÁ±ªÂûãÔºåÁ¨¨‰∏ÄÁßçÈõÜÁæ§‰∏ªËäÇÁÇπÈúÄË¶Å‰ΩøÁî®Áî®Êà∑ÁöÑECSÔºåÁî®Êà∑ÂèØËøúÁ®ãÁôªÂΩïÊàñÁÆ°ÁêÜËøô‰∫õECS„ÄÇÂè¶‰∏ÄÁßçÊòØÔºå‰∏ªËäÇÁÇπ‰πüÁî±‰∫ëÂéÇÂïÜÊâòÁÆ°ÔºåÁî®Êà∑Âè™ËÉΩÈÄöËøáAPI ServerÁÆ°ÁêÜKubernetes„ÄÇÂú®Ë¥πÁî®ÊñπÈù¢ÔºåÊó†ËÆ∫ÊòØÂê¶ÊâòÁÆ°ÈõÜÁæ§‰∏ªËäÇÁÇπÔºåÈõÜÁæ§ÊúçÂä°ÂÖçË¥πÔºåÊåâ‰ΩøÁî®ÁöÑECSÂÆû‰æãÂèäËÆ°Ë¥πÊñπÂºèÊî∂Ë¥π„ÄÇ Serverless Ê®°Âºè(ÁõÆÂâçÂÖ¨Êµã‰∏≠ÔºåÊöÇÊó∂ÂÖçË¥π)„ÄÇÊó†ÈúÄÂàõÂª∫Â∫ïÂ±ÇËôöÊãüÂåñËµÑÊ∫êÔºåÂèØ‰ª•Âà©Áî® Kubernetes ÂëΩ‰ª§ÊåáÊòéÂ∫îÁî®ÂÆπÂô®ÈïúÂÉè„ÄÅCPUÂíåÂÜÖÂ≠òË¶ÅÊ±Ç‰ª•ÂèäÂØπÂ§ñÊúçÂä°ÊñπÂºèÔºåÁõ¥Êé•ÂêØÂä®Â∫îÁî®Á®ãÂ∫è„ÄÇÊåâÂÆπÂô®‰ΩøÁî®ÁöÑCPUÂíåÂÜÖÂ≠òËµÑÊ∫êÈáèËÆ°Ë¥π„ÄÇËøôÁßçÊ®°Âºè‰∏ãÂ∫îËØ•ÊòØÂú®‰∏Ä‰∏™ÈõÜÁæ§ÂÜÖÂÆûÁé∞Â§öÁßüÊà∑ÔºåÁõÆÂâçÊúâ‰∫õfeatures‰∏çË¢´ÊîØÊåÅ„ÄÇ‰æãÂ¶ÇÔºåÈÉ®ÁΩ≤‰∏çÊîØÊåÅDaemonSetÔºåIngress‰∏çÊîØÊåÅNodePortÁ±ªÂûãÔºåÂ≠òÂÇ®‰∏çÊîØÊåÅPVÂíåPVCÁ≠â„ÄÇ Áî®Êà∑ÂèØ‰ª•Ê†πÊçÆËá™Â∑±ÁöÑ‰∏öÂä°Á±ªÂûãÊù•ÈÄâÊã©ÈÄÇÂêàÁöÑÊâòÁÆ°KubernetesÈõÜÁæ§„ÄÇÂ¶ÇÊûúÈÉ®ÁΩ≤ÁöÑÂ∫îÁî®ÊòØÊó†Áä∂ÊÄÅÁöÑWebÊúçÂä°ÔºåÂèØ‰ª•ÈÄâÊã©Serverless KubernetesÈõÜÁæ§ÔºåËøõ‰∏ÄÊ≠•ÂáèÂ∞ëËøêÁª¥Â∑•‰ΩúÈáè„ÄÇ\nÂ¶ÇÊûúÁî®Êà∑ÈÉ®ÁΩ≤ÁöÑÂ∫îÁî®ÊúâÁä∂ÊÄÅÔºåÈúÄË¶ÅÊåÇËΩΩÂ§ñÈÉ®Â≠òÂÇ®Ôºå‰æãÂ¶ÇMongDBÈõÜÁæ§ÔºåMQÈõÜÁæ§ÔºåÂèØ‰ª•ÈÄâÊã©ÁªèÂÖ∏Dedicated KubernetesÊ®°Âºè„ÄÇÂ¶ÇÊûúÁî®Êà∑ÈúÄË¶ÅÈÄöËøáKubernetesÁªÑ‰ª∂Êâ©Â±ïÊàñËá™ÂÆö‰πâÂÆûÁé∞Êüê‰∫õÂäüËÉΩÔºåËøô‰∫õÈúÄÊ±Ç‰∫ëÂéÇÂïÜÁöÑÊ†áÂáÜÁâàÂπ∂Ê≤°ÊúâÊèê‰æõÔºåËøôÊó∂ÂèØ‰ª•ÈÄâÊã©ÁªèÂÖ∏Dedicated KubernetesÊ®°ÂºèÔºåÂà©Áî®KubernetesÈ´òÂ∫¶ÁÅµÊ¥ªÁöÑÊâ©Â±ïÊú∫Âà∂Êù•Êª°Ë∂≥Ëá™ÂÆö‰πâÈúÄÊ±Ç„ÄÇ\nÊâòÁÆ°KuberentesÁöÑ‰ºòÂäø ÂõΩÂÜÖÁöÑÈòøÈáå‰∫ëÊúâÁØáÊäÄÊúØÊñáÊ°£ÂØπÊØîÈòøÈáå‰∫ëKubernetes vs. Ëá™Âª∫KubernetesÔºåÊñáÁ´†ÁúãËµ∑Êù•ËôΩÁÑ∂ÊúâÂéÇÂïÜËá™ÂçñËá™Â§∏ÁöÑÂ´åÁñë„ÄÇ‰Ωú‰∏∫ÈòøÈáå‰∫ëK8SÁöÑÂÆ¢Êà∑ÔºåÂú®‰ΩøÁî®ÊâòÁÆ°K8SËøë‰∏ÄÂπ¥Êù•ÔºåÊ∑±ÂàáÁöÑ‰Ωì‰ºöÂà∞‰∫ëÂéÇÂïÜÊâòÁÆ°K8SÂ∏¶Êù•ÁöÑÁßçÁßçÂ•ΩÂ§ÑÔºåÊñáÊ°£‰∏≠ÊèêÂà∞ÁöÑÁßçÁßç‰ºòÂäøÁ°ÆÂÆûÊòØË®Ä‰πãÂáøÂáø„ÄÇ\nÊé•‰∏ãÊù•ÂÖ∑‰ΩìÁúãÁúã‰∫ëÂéÇÂïÜÊâòÁÆ°K8SÂà∞Â∫ïÊúâÂì™‰∫õ‰ºòÂäø„ÄÇ\n‰æøÊç∑ ÈÄöËøáWebÁïåÈù¢/API‰∏ÄÈîÆÂàõÂª∫KubernetesÈõÜÁæ§ÔºåÈõÜÁæ§ÂçáÁ∫ß„ÄÇ WebÁïåÈù¢/APIÂÆûÁé∞ÈõÜÁæ§ÁöÑÊâ©ÂÆπÊàñÁº©ÂÆπ„ÄÇ ÈõÜÁæ§ÁöÑÂÆâË£ÖÔºåË°•‰∏Å‰ª•ÂèäÂ∏∏ËßÑÁâàÊú¨ÂçáÁ∫ßÂú®ËøêÁª¥Â∑•‰Ωú‰∏≠Â±û‰∫é‰ΩìÂäõÊ¥ª„ÄÇÂú®ËßÑÊ®°‰∏çÂ§ßÁöÑÊó∂ÂÄôÔºå‰ΩøÁî®‰∫∫Â∑•ÂÆûÁé∞ÈúÄË¶ÅËä±Ë¥π‰∏çÂ∞ëÊó∂Èó¥ÂáÜÂ§áÁéØÂ¢ÉÊµãËØïÈ™åËØÅÔºå‰∏îÊòìÈîô„ÄÇÂ¶ÇÊûúÈõÜÁæ§‰ΩìÈáè‰∏çÂ§üÂ§ßÁöÑËØùÔºåÂºÄÂèëËá™Âä®ÂåñËøêÁª¥ËÑöÊú¨ÂèàÊµ™Ë¥π‰∫∫ÂäõÊàêÊú¨„ÄÇ‰∫ëËÆ°ÁÆóÂéÇÂïÜÁöÑÊâòÁÆ°K8SÈõÜÁæ§Â∞ÜÊèê‰æõ‰∏ì‰∏ö„ÄÅÁ®≥ÂÆöÁöÑÊäÄÊúØËøêÁª¥ÊúçÂä°ÔºåÂíåÂá†‰πé‰∏∫Èõ∂ÁöÑ‰∫∫ÂäõÊàêÊú¨„ÄÇ\n‰ªéÊïàÁéáÂíå‰∫∫ÂäõÊàêÊú¨‰∏äÁúãÔºåÊâòÁÆ°K8SÈõÜÁæ§ÂÆåËÉúËá™Âª∫KubernetesÈõÜÁæ§„ÄÇ\nÂäüËÉΩÊõ¥Âº∫Â§ß Kubernetes‰Ωú‰∏∫‰∏Ä‰∏™ÂÆπÂô®ÁºñÊéíÁ≥ªÁªüÔºåÂºÄÊ∫êÁâàÊú¨‰∏≠ËÆ∏Â§öÁªÑ‰ª∂Ê≤°ÊúâÈªòËÆ§ÂÆûÁé∞ÊàñÂÆûÁé∞ÊúâÈôêÔºåÈúÄË¶ÅË∑üËøêË°åÁéØÂ¢É(Â¶ÇÊâòÁÆ°K8SÁöÑ‰∫ëÂπ≥Âè∞)ÈõÜÊàê„ÄÇ‰æãÂ¶ÇÔºåÂ≠òÂÇ®ÔºåLoad BalancerÔºåÁΩëÁªúÁ≠âÊ†∏ÂøÉÁªÑ‰ª∂„ÄÇÂÆòÊñπÊñáÊ°£Internal load balancerÂ∞±Êèê‰æõ‰∫ÜÂú®‰∏çÂêåÁöÑ‰∫ëÂéÇÂïÜÁéØÂ¢É‰∏≠ÁöÑ‰ΩøÁî®Á§∫‰æã„ÄÇÈÉ®ÁΩ≤‰∏Ä‰∏™Âº∫Â§ß‰∏îÂÆåÊï¥ÁöÑK8SÈõÜÁæ§ÈúÄË¶ÅÂêåËÆ∏Â§ö‰∫ëËÆ°ÁÆóÁöÑÂü∫Á°ÄÁªÑ‰ª∂ÈõÜÊàê(‰∏îÂè™ËÉΩÈÄöËøáAPIÂÆåÊàê)ÔºåËøôÂæÄÂæÄÊòØ‰∫ëËÆ°ÁÆóÂéÇÂïÜÁöÑÂº∫È°π„ÄÇ\n‰∫ëÂéÇÂïÜÊâòÁÆ°ÁöÑK8SÂèØ‰ª•Âú®‰ª•‰∏ãÊñπÈù¢Êèê‰æõÂº∫Â§ßÁöÑ‰∫ëËÆ°ÁÆóÂπ≥Âè∞ÊîØÊåÅÔºå\nÁΩëÁªú È´òÊÄßËÉΩ VPC ÁΩëÁªúÊèí‰ª∂„ÄÇ ÊîØÊåÅ network policy ÂíåÊµÅÊéß„ÄÇ Ë¥üËΩΩÂùáË°° ÊîØÊåÅÂàõÂª∫ÂÖ¨ÁΩëÊàñÂÜÖÁΩëË¥üËΩΩÂùáË°°ÂÆû‰æãÔºåÊàñËÄÖÂ§çÁî®Â∑≤ÊúâÂÆû‰æã„ÄÇÊîØÊåÅÊåáÂÆöÂ∏¶ÂÆΩÂ§ßÂ∞è„ÄÅËÆ°Ë¥πÊñπÂºè„ÄÅ4Â±ÇÊàñ7Â±ÇÂçèËÆÆ‰ª£ÁêÜÁ≠â‰∫ëÂéÇÂïÜË¥üËΩΩÂùáË°°ÂäüËÉΩ„ÄÇÂØπÂ∫îÁî®ËøêÁª¥Êù•ËØ¥ÂèØ‰ª•ÊääË¥üËΩΩÂùáË°°ÁöÑÈÖçÁΩÆÈÄöËøá‰ª£Á†ÅÂÆûÁé∞ÔºåÂπ∂‰∏îÊîØÊåÅÁâàÊú¨ÊéßÂà∂„ÄÇÂØπÊØî‰º†ÁªüÁöÑ‰∫ëÁ´ØÈÉ®ÁΩ≤Ôºå‰πüÂèØ‰ª•Â∞ÜÂ∫îÁî®ÈÉ®ÁΩ≤ÂíåÂ∫îÁî®ËøêÁª¥ÈõÜÊàêÂú®‰∏ÄËµ∑Áªü‰∏ÄÁÆ°ÁêÜÔºåÈÅøÂÖçÂ∫îÁî®ÂèëÂ∏ÉÂíåËøêÁª¥ÈÖçÁΩÆÁöÑÂâ≤Ë£ÇÔºåÂáèÂ∞ë‰∫∫‰∏∫ËøêÁª¥Â§±ËØØ„ÄÇ\nÈòøÈáå‰∫ëÊâòÁÆ°K8SÁöÑË¥üËΩΩÂùáË°°ËØ¶ÁªÜÈÖçÁΩÆÂèØ‰ª•ÂèÇËÄÉËøô‰∏™ÊñáÊ°£ÔºåAWS‰∏äËßÅÊ≠§ÊñáÊ°£„ÄÇ\nÂ≠òÂÇ® ÈõÜÊàê‰∫Ü‰∫ëÂéÇÂïÜÁöÑ‰∫ëÁõò„ÄÅÊñá‰ª∂Â≠òÂÇ®NAS„ÄÅÂùóÂ≠òÂÇ®Á≠âÂ≠òÂÇ®ÊñπÊ°àÔºåÂü∫‰∫éÊ†áÂáÜÁöÑFlexVolumeÈ©±Âä®ÔºåÊèê‰æõ‰∫ÜÊúÄ‰Ω≥ÁöÑÊó†ÁºùÈõÜÊàê„ÄÇ\nÂ¶ÇÊûúÊòØÂú®‰∫ëÂéÇÂïÜÁöÑËôöÊãüÊú∫‰∏äËá™Âª∫KubernetesÈõÜÁæ§ÔºåÈªòËÆ§Êó†Ê≥ï‰ΩøÁî®‰∫ë‰∏äÁöÑÂ≠òÂÇ®ËµÑÊ∫ê„ÄÇÂ¶ÇÊûúÈúÄË¶ÅÂà©Áî®‰∫ëÂéÇÂïÜÊèê‰æõÁöÑÂ≠òÂÇ®ÊñπÊ°àÔºå‰æãÂ¶ÇÂØπË±°Â≠òÂÇ®ÔºåÂ∞±ÈúÄË¶ÅËá™Ë°åÂºÄÂèëÂü∫‰∫éFlexVolumeÁöÑÈ©±Âä®„ÄÇÂú®ÂéÇÂïÜÊâòÁÆ°K8SÂ∑≤ÁªèÂÆåÁæéËß£ÂÜ≥‰∫ÜÂ≠òÂÇ®ÈõÜÊàêÁöÑÈóÆÈ¢òÔºå‰ΩïÂøÖËá™Â∑±ÂèàÂéªË¥πÊó∂Ë¥πÂäõÁöÑÂÆöÂà∂ÂºÄÂèëÂë¢Ôºü\nÂèØ‰ª•ÁúãÂà∞Ôºå‰∫ëÂéÇÂïÜÊâòÁÆ°ÁöÑK8SÈõÜÁæ§Âú®ÁΩëÁªú„ÄÅË¥üËΩΩÂùáË°°ÂíåÂ≠òÂÇ®‰∏äÊúâËÆ∏Â§öÂ§©ÁÑ∂ÁöÑ‰ºòÂäø„ÄÇÂú®ÂÖ∂‰ªñÂá†‰∏™Áª¥Â∫¶ÔºåÊâòÁÆ°ÁöÑK8SÈõÜÁæ§ÂêåÊ†∑‰πü‰ºò‰∫éËá™Âª∫ÁöÑK8SÔºå\nËøêÁª¥ ÈõÜÊàêÂéÇÂïÜÁöÑÊó•ÂøóÊúçÂä°ÔºåÁõëÊéßÊúçÂä°„ÄÇ K8SÈõÜÁæ§cluster autoscalerËá™Âä®Âà©Áî®‰∫ëÂéÇÂïÜÁöÑÂºπÊÄß‰º∏Áº©Êâ©Áº©ÂÆπÈõÜÁæ§ËäÇÁÇπ„ÄÇ ÈïúÂÉè‰ªìÂ∫ì È´òÂèØÁî®ÔºåÊîØÊåÅÂ§ßÂπ∂Âèë„ÄÇ ÊîØÊåÅÈïúÂÉèÂä†ÈÄü„ÄÇ ÊîØÊåÅ p2p ÂàÜÂèë„ÄÇ ÂèØÈõÜÊàê‰∫ëÂπ≥Âè∞ÁöÑÁî®Êà∑ÊùÉÈôê„ÄÇ ÈÉ®ÂàÜÂéÇÂïÜÁõÆÂâçÂÖçË¥π‰∏î‰∏çÈôêÂÆπÈáè„ÄÇ È´òÂèØÁî® Êèê‰æõÂ§öÂèØÁî®Âå∫ÊîØÊåÅ„ÄÇ ÊîØÊåÅÂ§á‰ªΩÂíåÂÆπÁÅæ„ÄÇ ÊäÄÊúØÊîØÊåÅ ‰∏ìÈó®ÁöÑÊäÄÊúØÂõ¢Èòü‰øùÈöúÂÆπÂô®ÁöÑÁ®≥ÂÆöÊÄß„ÄÇ ÊØè‰∏™ Linux ÁâàÊú¨ÔºåÊØè‰∏™ Kubernetes ÁâàÊú¨ÈÉΩ‰ºöÂú®ÁªèËøá‰∏•Ê†ºÊµãËØï‰πãÂêé‰πãÂêéÊâç‰ºöÊèê‰æõÁªôÁî®Êà∑„ÄÇ Êèê‰æõ Kubernetes ÂçáÁ∫ßËÉΩÂäõÔºåÊñ∞ÁâàÊú¨‰∏ÄÈîÆÂçáÁ∫ß„ÄÇ ‰∏∫ÂºÄÊ∫êËΩØ‰ª∂Êèê‰æõÂÖúÂ∫ïÔºåÊó†ËÆ∫ÊòØK8S„ÄÅDockerÁîöËá≥LinuxËá™Ë∫´ÁöÑÈóÆÈ¢òÊèê‰æõÊîØÊåÅ„ÄÇ ‰∏ì‰∏öÁöÑÊäÄÊúØÂõ¢ÈòüÊòØÊèê‰æõÁ®≥ÂÆöK8SÊúçÂä°ÂøÖ‰∏çÂèØÂ∞ëÁöÑ„ÄÇ‰ΩÜÁªùÂ§ßÂ§öÊï∞‰ºÅ‰∏öÊòØÊó†Ê≥ïÂÅöÂà∞Êúâ‰∏ì‰∏öÁöÑÊäÄÊúØÂõ¢ÈòüÊù•Áª¥Êä§K8S„ÄÅÊèê‰æõK8SÊàñÂÆπÂô®ÊäÄÊúØËá™Ë∫´ÁöÑÂêÑÁßçÊúÄ‰Ω≥ÂÆûË∑µ„ÄÅÂèëÁé∞‰ª•Âèä‰øÆÂ§çÂºÄÊ∫êËΩØ‰ª∂Bug„ÄÇ\nÂú®Á¨îËÄÖÁöÑ‰ΩøÁî®ÊâòÁÆ°K8SÁöÑÊó∂ÂÄôÂ∞±ÈÅáÂà∞ËøôÊ†∑ÁöÑÁä∂ÂÜµ„ÄÇÂÖ∂‰∏≠‰∏Ä‰∏™ÈõÜÁæ§ÂçáÁ∫ßÂà∞Êñ∞ÁâàÊú¨KubernetesÂêéÔºåÂÜÖÁΩÆDNSÁªÑ‰ª∂‰ªéKubeDNSË¢´ÊõøÊç¢‰∏∫ÂÖ®Êñ∞ÁöÑCoreDNSÔºåËÄåÂΩìÊó∂ÁöÑCoreDNSÁâàÊú¨Âú®Service ExternalNameÊîØÊåÅ‰∏äÊúâBugÔºåÂØºËá¥Â∑≤ÊúâÁöÑËøôÁßçServiceÊó†Ê≥ïÊèê‰æõÊúçÂä°„ÄÇÂú®Âêå‰∫ëÂéÇÂïÜÁöÑÊäÄÊúØÂõ¢ÈòüÊ≤üÈÄöÂêéÔºåÂÖàÁî®workaroundÂ∞ÜÈóÆÈ¢òÂø´ÈÄüÁªïËøáÔºå‰∏çÂΩ±Âìç‰∏öÂä°ÁöÑ‰ΩøÁî®„ÄÇÂêåÊó∂Ôºå‰∫ëÂéÇÂïÜÁöÑÊäÄÊúØ‰∫∫ÂëòÔºà‰πüÊòØK8SÁ§æÂå∫committerÔºâÁªßÁª≠Ë∞ÉÁ†îÔºåÂèëÁé∞ËØ•ÈóÆÈ¢òÊòØCoreDNSÁöÑBug„ÄÇÂú®‰∏∫ÂºÄÊ∫êCoreDNSÈ°πÁõÆÂàõÂª∫IssueÂêéÔºåÂêåÊó∂Êèê‰æõPatchÔºåÂèàÂú®CoreDNS committerÂª∫ËÆÆ‰∏ãÂÆåÂñÑ‰∫ÜÊµãËØïÁî®‰æãÔºåÊé®Âä®‰∫ÜËØ•ÈóÆÈ¢òÂø´ÈÄüÂú®CoreDNS‰∏≠Ë¢´‰øÆÂ§ç„ÄÇCoreDNSÂåÖÂê´FixÁöÑÁâàÊú¨ÂèëÂ∏ÉÂêéÔºå‰∫ëÂéÇÂïÜÊäÄÊúØÊîØÊåÅÂõ¢ÈòüÂ∞ÜÊõ¥ÂÆåÁæéÁöÑËß£ÂÜ≥ÊñπÊ°àÊèê‰æõÁªô‰∫ÜÊàë‰ª¨„ÄÇ‰Ωú‰∏∫K8SÊúçÂä°ÁöÑÁî®Êà∑ÔºåËøôÁßç‰ΩìÈ™åÊòØÊûÅÂ•ΩÁöÑ„ÄÇÂΩìÊó∂Êàë‰ª¨ÁöÑÊäÄÊúØÂõ¢ÈòüÊó¢Ê≤°ÊúâÁ≤æÂäõ‰πüÊ≤°ÊúâËÉΩÂäõÂø´ÈÄüÂèëÁé∞Âπ∂‰øÆÂ§çÂºÄÊ∫êËΩØ‰ª∂‰∏≠ÁöÑËøôÁ±ªÈóÆÈ¢òÔºåËÄå‰∫ëÂéÇÂïÜÁöÑÊúçÂä°Èó¥Êé•Â∏ÆÊàë‰ª¨ÂÆûÁé∞‰∫ÜËøôÁßçËÉΩÂäõ„ÄÇ\nËøôÂÖ∂ÂÆûÊòØ‰∏ÄÁßçÈùûÂ∏∏Â•ΩÁöÑÂÖ±Ëµ¢ÂïÜ‰∏öÊ®°ÂºèÔºå‰∫ëÂéÇÂïÜÊúâËÉΩÂäõ‰∏îÊúâÂä®ÂäõÊäïÂÖ•È°∂Â∞ñÊäÄÊúØÂõ¢ÈòüÂ∞ÜÂºÄÊ∫êÊäÄÊúØÂïÜ‰∏öÂåñÔºå‰∫ëÂéÇÂïÜÁöÑÁî®Êà∑ÂàôÁî®ÊúÄÂ∞èÁöÑ‰ª£‰ª∑Ëé∑Âæó‰∫ÜÊúÄ‰ºòÁöÑÂü∫Á°ÄÊúçÂä°Êù•‰∏∫Ê†∏ÂøÉ‰∏öÂä°ËµãËÉΩ„ÄÇ\n","link":"https://kane.mx/posts/effective-cloud-computing/using-kubernetes-on-cloud/","section":"posts","tags":["‰∫ëËÆ°ÁÆó","ÈòøÈáå‰∫ë","AWS","kubernetes"],"title":"‰∏çË¶ÅËá™Âª∫Kubernetes"},{"body":"ËøôÊòØ‚ÄúÂ¶Ç‰ΩïÈ´òÊïà‰ΩøÁî®‰∫ëÊúçÂä°‚ÄùÁ≥ªÂàóÊñáÁ´†ÁöÑÂºïÂ≠ê„ÄÇËØ•Á≥ªÂàóÂ∞ÜËÆ≤Ëø∞Â¶Ç‰ΩïÂà©Áî®ÂêÑÁßçÂÖ¨Êúâ‰∫ëÊúçÂä°Êù•ÂÆâÂÖ®ÂêàËßÑ„ÄÅÈ´òË¥®Èáè„ÄÅÂø´ÈÄü„ÄÅ‰ΩéÊàêÊú¨ÁöÑÊâìÈÄ†‰∫ßÂìÅ/Á≥ªÁªüÔºåÂ∏ÆÂä©‰ºÅ‰∏öÔºàÁâπÂà´ÊòØ‰∏≠Â∞èÂæÆÂàõ‰∏öÂõ¢ÈòüÔºâÂú®‰∫∫Â∞ëÔºåÈí±Áº∫ÁöÑÊÉÖÂÜµ‰∏ãÂÅöÂà∞ÊúÄÈ´òÊïàÁéá„ÄÇ\n‰∏™‰∫∫‰ΩøÁî®ÂÖ¨Êúâ‰∫ëÊúçÂä°ÁöÑÁªèÂéÜ Âàù‰ºö ÊúÄÊó©ÊòØ2012Âπ¥Âú®parttimeÈ°πÁõÆ‰∏≠ÂºÄÂßãÊé•Ëß¶‰ΩøÁî®‰∫ëËÆ°ÁÆóÊúçÂä°ÔºåÂΩìÊó∂ÁöÑÂàùÂàõÂõ¢Èòü‰πüÊòØÂ∏åÊúõÁî®ÊúÄ‰ΩéÁöÑÊàêÊú¨Êù•È™åËØÅideaÔºåÊâÄÊúâ‰ΩøÁî®‰∫Ü‰∫ëÊúçÂä°Êù•ÂÅöPOC„ÄÇÁõÆÂâçÂõΩÂÜÖÂ∏ÇÂú∫ÊúÄÈ¢ÜÂÖàÁöÑ‰∫ëËÆ°ÁÆóÂéÇÂïÜÈòøÈáå‰∫ëÈÇ£Êó∂‰πüÊâçÊèê‰æõÂÖ¨Êúâ‰∫ëÊúçÂä°‰∏çÂà∞1Âπ¥„ÄÇÁî±‰∫é‰∫ë‰∫ßÂìÅ‰∏çÂ§üÊàêÁÜüÔºåÂä†‰∏äÂõ¢ÈòüÊäÄËÉΩÁªèÈ™å‰∏çË∂≥ÔºåËá™Âä©‰∫íÂä©ÁöÑÊ∏†ÈÅì‰∏çÁïÖÔºåÂØºËá¥ÊúÄÂàùÁöÑ‰∫ëËÆ°ÁÆó‰ΩøÁî®‰ΩìÈ™åÂπ∂‰∏çÂ•ΩÔºåÂõ¢ÈòüÊ≤°ÊúâÈÄâÊã©ÂÆåÂÖ®‰ΩøÁî®‰∫ëÊúçÂä°ÊûÑÂª∫‰∫ßÂìÅ„ÄÇ\nIaaS or PaaS ‰∫ëËÆ°ÁÆóÂÖ¥Ëµ∑ÁöÑÊó©ÊúüÔºå‰∫ëÂéÇÂïÜÂ§ßËá¥ÂàÜ‰∏∫‰∏§Á±ªÔºåÊèê‰æõÂü∫‰∫éIaaSÊàñPaaSÁöÑ‰∫ëÊúçÂä°„ÄÇ2013Âπ¥Ëµ∑‰πüÊúâÂ∞ùËØï‰∏çÂêåÁ±ªÂûãÁöÑÂéÇÂïÜÂπ≥Âè∞ÔºåËôΩÁÑ∂‰πüËæÉÂ•ΩÁöÑÂÆåÊàê‰∏Ä‰∫õ‰ΩìÈáè‰∏çÂ§ßÁöÑÈ°πÁõÆÔºå‰ΩÜË¶ÅÂú®‰ªñ‰ª¨‰∏äÈù¢ÊûÑÂª∫Â§ßËßÑÊ®°Áî®Êà∑‰∫ßÂìÅÊàñ‰ºÅ‰∏öÁ∫ßÂ∫îÁî®ÔºåÂú®‰∫ë‰∫ßÂìÅÂÆåÂñÑÂ∫¶‰∏äÊàñÊîØÊåÅÂºÄÂèëÂõ¢ÈòüÂçè‰Ωú‰∏äÈÉΩÊúâ‰∏çÂ∞ëÊ¨†Áº∫ÔºåËøòÊúâÂ§ßÈáèÁöÑÂü∫Á°ÄÂ∑•‰ΩúÊàñÈôêÂà∂ÁïôÁªô‰∫ÜÂºÄÂèëÂõ¢ÈòüËá™Ë∫´Ëß£ÂÜ≥„ÄÇ\nAll-in Cloud 2015Âπ¥ÊàëÂºÄÂßã‰∏Ä‰∏™ÂæÆÁîµÂΩ±È°πÁõÆÂàõ‰∏öÔºåÂõ¢ÈòüÊòØ‰∏çÂà∞10‰∫∫ÁöÑÂæÆÂûãÂõ¢Èòü„ÄÇ‰ªéÊïàÁéáÂíåÊàêÊú¨ËÄÉËôëÔºåÊàë‰ª¨Â∞ÜÊâÄÊúâÁöÑÊúçÂä°ÈÉΩÊîæÂà∞‰∫ÜÈòøÈáå‰∫ë‰∏ä„ÄÇÊàë‰ª¨‰ΩøÁî®‰∫ÜÂ§öÁßç‰∫ë‰∫ßÂìÅÔºå‰æãÂ¶ÇÔºå‰∫ë‰∏ªÊú∫ÔºàÂ§öÁßçOSÔºâÔºåÂØπË±°Â≠òÂÇ®ÔºåÂõæÁâáÂ§ÑÁêÜÔºåCDNÔºåSLBÔºå‰∫∫ËÑ∏ËØÜÂà´Á≠â‰∫ëÊúçÂä°ÔºåÁªìÂêàDevopsÈõÜÊàêÂºÄÂèëÔºåÊµãËØïÔºåÈÉ®ÁΩ≤pipelineÊù•Âä†ÈÄü‰∫ßÂìÅÁöÑËø≠‰ª£ÂíåÊõ¥Êñ∞„ÄÇÊØèÂêçÂ∑•Á®ãÂ∏àÊâøÊãÖ‰∏ÄÁßç‰ª•‰∏äËßíËâ≤ÔºåÂâçÁ´ØÔºåÂêéÁ´ØÔºåËøêÁª¥ÔºåÊï∞ÊçÆÔºåËßÜÈ¢ëÊ∏≤ÊüìÁ≠â„ÄÇÂêàÁêÜ‰ΩøÁî®‰∫ëÂéÇÂïÜÁöÑÂêÑÁßç‰∫ßÂìÅÂ∏ÆÊàë‰ª¨Âú®Ë¥®ÈáèÔºåÊïàÁéáÔºåÊàêÊú¨‰∏äËé∑ÂæóÂ∑®Â§ßÁöÑÊî∂Áõä„ÄÇ\n2017Âπ¥ÊàëÂä†ÂÖ•‰∫Ü‰∏ÄÂÆ∂‰ºÅ‰∏öË¥¢Á®éÊúçÂä°ÁöÑÂàùÂàõÂÖ¨Âè∏Ë¥üË¥£ÊäÄÊúØÂõ¢Èòü„ÄÇÂÖ¨Âè∏Âú®2018Âπ¥Ëé∑Âæó‰∫ÜBËΩÆÊäïËµÑÔºåÁ†îÂèë‰∫ßÂìÅËøêËê•Âõ¢ÈòüËøëÁôæ‰∫∫ÔºåÂ±û‰∫é‰∏≠Á≠âËßÑÊ®°„ÄÇÈöèÁùÄÂêÑÁßçÂºÄÊ∫êÊäÄÊúØÁöÑÂ∑®Â§ßËøõÊ≠•ÂíåÂΩ±ÂìçÈÄêÊ≠•Êâ©Â§ßÔºåÂæÆÊúçÂä°Êû∂ÊûÑÁöÑÊµÅË°åÔºåÂü∫‰∫éKubernetesÁöÑCloud Native ComputingÂÖ¥Ëµ∑„ÄÇÊàë‰ª¨Âà©Áî®‰∫ëÂéÇÂïÜÁöÑÂÆπÂô®ÊúçÂä°ÔºåDBaaSÔºåBig DataÔºåAIÊäÄÊúØÁ≠âÁî®ÊúÄÈ´òÊïàÁöÑÊñπÂºèÂ∞ÜÊï∞‰∏™Âçï‰ΩìÂ∫îÁî®Âπ≥ÊªëÂçáÁ∫ßÂà∞È´òÂèØÁî®ÂºπÊÄßÁöÑÂàÜÂ∏ÉÂºèÊû∂ÊûÑÔºåÊõ¥Â•ΩÁöÑÊª°Ë∂≥Â§çÊùÇ‰∏öÂä°ÁöÑÂ§öÂèòÈúÄÊ±ÇÔºåÂÖ¨Âè∏ÊúçÂä°‰πüÂú®ÂÖ®ÂõΩ300Â§ö‰∏™ÂüéÂ∏ÇËêΩÂú∞ÔºåÊúçÂä°‰∫ÜÊï∞ÂçÅ‰∏á‰∏≠Â∞èÂæÆ‰ºÅ‰∏öÂÆ¢Êà∑„ÄÇÂêåÊó∂Âà©Áî®‰∫ëÂéÇÂïÜÁöÑVPCÔºåËÆøÈóÆÊéßÂà∂ÔºåWAFÁ≠â‰∫ßÂìÅËøõË°åÊùÉÈôêÊéßÂà∂ÂíåÂÆâÂÖ®‰øùÊä§ÔºåÊúâÊïàÈò≤ËåÉ‰∫ÜÂõ†‰∏∫Âõ¢ÈòüÊâ©Â§ßÁÆ°ÁêÜÈöæÂ∫¶Â¢ûÂä†ËÄåÂá∫Áé∞ÂÆâÂÖ®ÈóÆÈ¢ò„ÄÇ\nÁºòËµ∑ ‰Ωú‰∏∫‰∏ÄÂêç‰∫ëËÆ°ÁÆóÊúçÂä°6Âπ¥ÁöÑÁî®Êà∑ÔºåËßÅËØÅ‰∫ÜÂºÄÊ∫êÊäÄÊúØÁöÑÂø´ÈÄüÂèëÂ±ïÂíåÂΩ±ÂìçÂäõÊÄ•ÂâßÊâ©Â§ßÔºåÊÑüÂèóÂà∞Êï¥‰∏™‰∫ëËÆ°ÁÆóË°å‰∏öÂíåÂéÇÂïÜÁöÑÈïøË∂≥ËøõÊ≠•„ÄÇËßÅËØÅ‰∫ÜÂõΩÂÜÖÂ§¥ÈÉ®‰∫ëÂéÇÂïÜ‰ªéÊúÄÂàùÁöÑ‰ΩøÁî®ÈöæÂ∫¶È¢áÂ§ßÔºåÁé∞Âú®ÊàêÈïø‰∏∫‰∏á‰ºóÂàõ‰∏öÁöÑÈ¶ñÈÄâÊúçÂä°ÂïÜ„ÄÇ\nËøáÂéªÁöÑ‰∏ÄÂπ¥ÂèÇÂä†‰∫ÜÊï∞Âú∫ÊäÄÊúØ‰ºöËÆÆÔºåÂÖ∂‰∏≠‰∏ªÈ¢òÂ§ßÂ§öÂÅèÂêë‰∫éÁî±Áü•ÂêçÁöÑ‰∫íËÅîÁΩëÊàñË°å‰∏öÂÖ¨Âè∏ÂàÜ‰∫´Âú®Êµ∑ÈáèÊï∞ÊçÆ‰∏ãÁöÑÊäÄÊúØÂ∫îÁî®„ÄÇËøô‰∫õÊäÄÊúØÂπøÊ≥õÊ∂âÂèäÂºÄÂèëËØ≠Ë®Ä„ÄÅÂ∫îÁî®Êû∂ÊûÑ„ÄÅÊÄßËÉΩ„ÄÅÂ§ßÊï∞ÊçÆ„ÄÅÊú∫Âô®Â≠¶‰π†Âíå‰∫∫Â∑•Êô∫ËÉΩÁ≠âÈ¢ÜÂüüÔºåÊó†ËÆ∫Ëøô‰∫õÂÖ¨Âè∏ÊòØÂê¶ÈááÁî®ÂºÄÊ∫ê‰∫ßÂìÅÔºåÂú®Âõ¢ÈòüÂçïÂÖµÊäÄÊúØËÉΩÂäõÔºå‰∏ì‰∏öÁöÑÂàÜÂ∑•ÔºåÂØπÂºÄÊ∫êÈ°πÁõÆÁöÑÁ†îÂèëÊäïÂÖ•ÂäõÈáèÔºåËøô‰∫õÁªèÈ™åÂíåÊñπÊ≥ïÂπ∂‰∏çÊòØ‰∏≠Â∞è‰ºÅ‰∏öÂèØ‰ª•ËΩªÊòìÂÄüÈâ¥ÁöÑ„ÄÇËÄå‰∫ëËÆ°ÁÆóÂéÇÂïÜÂ∞ÜËøô‰∫õÈ¢ÜÂüüÊúÄÂü∫Á°ÄÈÄöÁî®ÁöÑËÉΩÂäõ‰ª•‰∫ßÂìÅÁöÑÊñπÂºèËæìÂá∫ÁªôÁî®Êà∑Ôºå‰ª•ÊåâÁî®ÈáèÁöÑÊñπÂºèËÆ°Ë¥πÔºå‰ΩøÁî®Êõ¥ÁÆÄÂçïÔºåÊúâ‰∏ì‰∏öÂõ¢ÈòüÁª¥Êä§ÂíåÊîØÊåÅ„ÄÇ‰∏≠Â∞èÂõ¢ÈòüÂ∞±Â∫îËØ•Â∞ÜËøô‰∫õ‰∫ãÊÉÖ‚ÄúÂ§ñÂåÖ‚ÄùÁªô‰∫ëÂéÇÂïÜÔºåÈõÜ‰∏≠Á≤æÂäõÂà∞‰∏öÂä°‰∏äÔºåÂ∞ÜÊúÄÂ§ßÁöÑÁ†îÂèëËµÑÊ∫êÁî®Âà∞ÊúÄÊ†∏ÂøÉÊúÄÂÖ≥ÈîÆÁöÑÂú∞Êñπ„ÄÇ\nÊàëÂêåÂõ¢ÈòüÂêå‰∫ãÊ≤üÈÄö‰∏≠ÔºåÂíåÂÖ¨Âè∏Á†îÂèëÂÄôÈÄâ‰∫∫Èù¢ËØï‰∫§ÊµÅ‰∏≠ÔºåÂèëÁé∞ËÆ∏Â§ö‰ªé‰∏öËÄÖÂØπ‰∫ëËÆ°ÁÆóÊúçÂä°‰∫ÜËß£Ëøò‰∏çÂ§üÊ∑±ÂÖ•„ÄÇËÆ∏Â§ö‰∫∫ÁêÜËß£‰∏≠ÁöÑ‰∫ëËÆ°ÁÆóÊúçÂä°Âè™Êúâ‰∫ëÊúçÂä°Âô®„ÄÅ‰∫ëÊï∞ÊçÆÂ∫ìÁ≠âÂ∞ëÊï∞‰∫ßÂìÅÔºåÈúÄË¶ÅËá™Â∑±ÂÆâË£ÖÁª¥Êä§Â∫îÁî®ÊúçÂä°Âô®„ÄÅË¥üËΩΩÂùáË°°„ÄÅÊî∂ÈõÜÊó•ÂøóÁ≠âÁ≠âÁúãËµ∑Êù•ÊØè‰∏™Â∫îÁî®ÈÉΩÁªï‰∏çÂºÄÁöÑ‰∫ãÊÉÖ„ÄÇ‰ªñ‰ª¨ÁöÑËÆ§Áü•ËøòÂÅúÁïôÂú®ÊéíÊü•Â∫îÁî®ÂºÇÂ∏∏ËøòÈúÄË¶ÅËøúÁ®ãÁôªÂΩïÊúçÂä°Âô®ÁúãÊó•ÂøóÔºåÂÅö‰∏çÂà∞ÂêàÁêÜÁöÑÊ†πÊçÆÂú∫ÊôØÈ´òÊïàÁªÑÂêà‰ΩøÁî®‰∫ëÊúçÂä°ÔºåÂ∞Ü‰∫ëÊúçÂä°ÂΩìÂÅöÊ∞¥Áîµ‰∏ÄÊ†∑Ôºå‰Ωú‰∏∫ÊúÄÂü∫Á°ÄÁöÑËÉΩÂäõÂä†ÈÄü‰∏öÂä°ÁöÑÂèëÂ±ï„ÄÇ‰∏öÂä°‰∏äÊòØÈááÁî®ÂêçÊ∞îÂ§ß‰∏îÊàêÁÜüÁöÑ‰∫ßÂìÅÔºåÂ∞ùËØïÊñ∞È≤úÁúãËµ∑Êù•ÈÖ∑‰ΩÜ‰∏çÈÇ£‰πàÂÆåÂñÑÁöÑ‰∫ßÂìÅÔºåËøòÊòØ‰∫åÊ¨°ÂºÄÂèëÊàñËá™Á†îÂºÄÂèëÔºüË¶ÅÂÅöÂá∫ÊúÄ‰ºòÁöÑÈÄâÊã©ÈúÄË¶ÅÂ∑•Á®ãÂ∏àËÉΩÂ§ü‰ªéÊúâÈ´òÂ∫¶ÁöÑÂÖ®Â±ÄËßíÂ∫¶Êù•ËÄÉÈáèÔºåÁîöËá≥Âú®Áü≠Êó∂Èó¥ÂÜÖËÉΩÁî®POCÈ°πÁõÆÈ™åËØÅÂ§ö‰∏™ÂèØÈÄâÁöÑÊñπÊ°àÔºåÂü∫‰∫éÊï∞ÊçÆÂÅöÂá∫ÊúÄÁªàÁöÑÈÄâÊã©„ÄÇ\nËøôÂ∞±ÊòØËøô‰∏™Á≥ªÂàóÁöÑÁºòËµ∑Ôºå‰πãÂêéÊàëÂ∞ÜÈôÜÁª≠ÂàÜ‰∫´‰ΩøÁî®ÈÇ£‰∫õÈ´òÊïàÁöÑ‰∫ëÊúçÂä°‰∫ßÂìÅÁöÑÂú∫ÊôØ„ÄÅÂøÉÂæó„ÄÅ‰Ωì‰ºöÁ≠âÁ≠â„ÄÇ\nÂ∞ÅÈù¢ÂõæÁâáCloud ComputingÂºïÁî®Ëá™The Blue Diamond Gallery under CC BY-SA 3.0\n","link":"https://kane.mx/posts/effective-cloud-computing/preface/","section":"posts","tags":["‰∫ëËÆ°ÁÆó","ÈòøÈáå‰∫ë"],"title":"ÁúüÁöÑ‰ºöÁî®‰∫ëÊúçÂä°ÂêóÔºü"},{"body":"‰∏äÂë®ÂèÇÂä†‰∫ÜArchSummit(ÂÖ®ÁêÉÊû∂ÊûÑÂ∏àÂ≥∞‰ºö)ÔºåÂú®ËøôÈáåËÆ∞ÂΩï‰∏ãÈÉ®ÂàÜÂèÇÂä†ÁöÑ‰∏ªÈ¢ò‰ª•Âèä‰∏™‰∫∫ÊÑüÂèó„ÄÇ\n‰ºöËÆÆÂõûÈ°æ ‰ªäÂπ¥ÂèÇÂä†‰∫ÜÂá†Ê¨°ÊäÄÊúØ‰ºöËÆÆÔºåÂæÆÊúçÂä°„ÄÅÂÆπÂô®ÊäÄÊúØ„ÄÅÂå∫ÂùóÈìæ„ÄÅÂ§ßÊï∞ÊçÆ„ÄÅÊú∫Âô®Â≠¶‰π†‰ª•Âèä‰∫∫Â∑•Êô∫ËÉΩÈÉΩÊòØÂΩì‰∏ãÊúÄÁÉ≠Èó®ÁöÑ‰∏ªÈ¢ò„ÄÇÂêåÊ†∑ËøôÊ¨°ArchSummitÁªùÂ§ßÈÉ®ÂàÜtopicsÈÉΩË∑üËøô‰∫õ‰∏ªÈ¢òÁõ∏ÂÖ≥„ÄÇ\nËøôÊ¨°‰ºöËÆÆ‰∏ªË¶ÅÂèÇÂä†‰∫Ü‰∏§‰∏™‰∏ìÂú∫‰∏ªÈ¢òÔºåKubernetesÁöÑÂ∫îÁî®ÂíåÂø´ÊâãÁßëÊäÄÊäÄÊúØ‰∏ìÈ¢ò„ÄÇ\nÂü∫‰∫é Kubernetes ÁöÑ DevOpsÊòØÊù•Ëá™ÂæÆËΩØAzureÁöÑÂÆπÂô®Â∑•Á®ãÂ∏àÂàÜ‰∫´Â¶Ç‰ΩïÂü∫‰∫é Kubernetes ÁöÑ CI/CD ËêΩÂú∞ÂÆûË∑µ„ÄÇËØ•ÂàÜ‰∫´‰∏≠ÊèêÂà∞‰∫ÜCI/CDÂêÑ‰∏™Ê≠•È™§‰∏≠ÈÉΩÊúâ‰ºóÂ§öÁöÑÂ∑•ÂÖ∑ÊîØÊåÅÔºåÂ¶Ç‰ΩïÈÄâÊã©ÂêàÈÄÇKubernetesÁöÑÂ∑•ÂÖ∑Â∞ÜÊåÅÁª≠ÈõÜÊàêÂíåÈÉ®ÁΩ≤‰∏≤ËÅîÂú®‰∏ÄËµ∑ÊòØDevops‰∏≠ÁöÑ‰∏ªË¶ÅÊåëÊàò„ÄÇÂàÜ‰∫´ËÄÖ‰πüÂÆâÂà©‰∫ÜAKSÊèê‰æõDevopsÂÆåÊï¥ÁöÑÂ∑•ÂÖ∑ÈìæÔºå‰ª•ÂèäÂ∞ÜÂºÄÊ∫êÂ∑•ÂÖ∑ÂêåAKS‰∏≠ÁöÑÊúçÂä°ÈõÜÊàêÂÆûÁé∞CI/CDÁöÑÊúÄ‰Ω≥ÂÆûË∑µ„ÄÇ\nÊàë‰ª¨ÂôºÈáåÂï™Âõ¢ÈòüÂú®CI/CD„ÄÅDevopsËøôÂùóÂÅöÂæóËøò‰∏çÈîô„ÄÇCI/CD pipelinesÊåÅÁª≠Â∞ÜÂ∫îÁî®ÈÉ®ÁΩ≤Âú®ËøêË°åÁöÑKubernetesÈõÜÁæ§ÔºåËøáÁ®ã‰∏≠‰ΩøÁî®ÁöÑÂ∑•ÂÖ∑ÈìæÂü∫Êú¨‰πüÊòØÁ§æÂå∫ÊàñCNCFÊé®ËçêÁöÑ‰∏ªÊµÅÂ∑•ÂÖ∑„ÄÇ‰∏ã‰∏ÄÊ≠•ÂèØ‰ª•ËÄÉËôëÂêå‰∫ëÂéÇÂïÜÁöÑDevopsÂ∑•ÂÖ∑ÈìæÈõÜÊàêÔºåËøõ‰∏ÄÊ≠•ÂáèÂ∞ëÁª¥Êä§ÊàêÊú¨„ÄÇ\nÂü∫‰∫éIstio on Kubernetes‰∫ëÂéüÁîüÂ∫îÁî®ÁöÑÊúÄ‰Ω≥ÂÆûË∑µÊù•Ëá™ÈòøÈáå‰∫ëÂÆπÂô®Â∑•Á®ãÂ∏àÁöÑÂàÜ‰∫´„ÄÇ‰ªñÊ¶ÇË¶ÅÁöÑÂàÜ‰∫´‰∫ÜIstioÊäÄÊúØÂíåÂÆûÁé∞ÂéüÁêÜ„ÄÇÂΩìÁÑ∂‰πüÂ§ßÂäõ‰ªãÁªç‰∫ÜÈòøÈáå‰∫ëÂÆπÂô®ÊúçÂä°ÂØπIstioÁöÑÂéüÁîüÊîØÊåÅÔºå‰ª•ÂèäÈòøÈáå‰∫ëÂØπÂÆ¢Êà∑‰ΩøÁî®IstioÁöÑÊîØÊåÅÔºåÂç≥‰ΩøÂÆ¢Êà∑ÈóÆÈ¢òÈùûÂ∏∏ÁöÑÂàùÁ∫ß‰ªñ‰ª¨ÁöÑÊäÄÊúØÊîØÊåÅ‰πüÂæàÂà∞‰Ωç„ÄÇ\nIstioÂèØ‰ª•ËØ¥ÊòØCNCFÂú®Kubernetes‰∏ä‰∫ãÂÆûÁöÑÊúçÂä°Ê≤ªÁêÜÂÆûÁé∞„ÄÇÂôºÈáåÂï™ÊäÄÊúØÂõ¢Èòü‰πü‰∏ÄÁõ¥Âú®ÂÖ≥Ê≥®Ëøô‰∏ÄÂùóÔºåÊ≠£Âú®Â∞ùËØïÂºïÂÖ•IstioÊèêÂçáÊúçÂä°ÁöÑSLA„ÄÇ\nÂø´ÊâãÊäÄÊúØÂõ¢ÈòüÁöÑ4‰∏™ÂàÜ‰∫´ÈÉΩÊòØÂõ¥ÁªïËß£ÂÜ≥ÊòéÁ°ÆÁöÑ‰∏öÂä°ÈóÆÈ¢òËÄåÂÅöÂæóÊäÄÊúØÂ∑•‰ΩúÔºåÈùûÂ∏∏ÂÖ∑ÊúâÂÆûÊàòÊÄß„ÄÇÂÖ∂‰∏≠Âø´Êâã‰∏á‰∫øÁ∫ßÂÆûÊó∂ OLAP Âπ≥Âè∞ÁöÑÂª∫ËÆæ‰∏éÂÆûË∑µ‰ªãÁªç‰∫ÜÂø´ÊâãÂÆûÊó∂OLAPÂπ≥Âè∞‰ªé0Âà∞1ÁöÑÊê≠Âª∫ËøáÁ®ã„ÄÇËØ•Âπ≥Âè∞‰ªé‰ªäÂπ¥4ÊúàÂºÄÂßãÊê≠Âª∫ÔºåÊà™Ê≠¢Âà∞11ÊúàÔºåÊØèÊó•ÂèØ‰ª•ÂÆûÊó∂ËÆ°ÁÆóÂ§ÑÁêÜË∂ÖËøá‰∏á‰∫øÁöÑÊï∞ÊçÆ„ÄÇËÄåÊï¥‰∏™Âπ≥Âè∞ÁöÑÊê≠Âª∫Áî±‰∏§ÂêçÂ§ßÊï∞ÊçÆÂ∑•Á®ãÂ∏àÂ§ñÂä†‰∏ÄÂêçÂâçÁ´ØÂ∑•Á®ãÂ∏àË¥üË¥£portalÁ≠âUIÔºå‰∫∫Êïà‰∫ßÂá∫ËÆ©‰∫∫ÈùûÂ∏∏‰Ω©Êúç„ÄÇÁªìÂêàÊúãÂèãÈó¥‰º†Ë®ÄÂø´ÊâãÁªôÊäÄÊúØ‰∫∫ÂëòÁöÑofferÔºåÂø´ÊâãÂ∫îËØ•ÊòØ‰∏ÄÂÆ∂Âú®ÂÆûË∑µÁ±ª‰ººNetflixÁÆ°ÁêÜÊñáÂåñÁöÑÂÖ¨Âè∏„ÄÇ\nÊúÄÂêéÁªôÂ§ßÂÆ∂Êé®Ëçê‰∏Ä‰∏™ÂõΩ‰∫ßÁöÑÂàÜÂ∏ÉÂºèNew SQLÊï∞ÊçÆÂ∫ìTiDBÁõ∏ÂÖ≥ÁöÑ‰∏ªÈ¢ò„ÄÇTiDBÊòØÂõΩÂÜÖÊäÄÊúØÂõ¢ÈòüÂºÄÊ∫êÁöÑ‰∏Ä‰∏™ÂàÜÂ∏ÉÂºèÊï∞ÊçÆÂ∫ìÔºåÂ∑≤Ë¢´CNCF‰Ωú‰∏∫DatabaseÂÆûÁé∞Êé®ËçêÊñπÊ°à‰πã‰∏Ä„ÄÇ‰ªñ‰ª¨ÁöÑCTOÂàÜ‰∫´‰∫ÜTiDB on Kubernetes ÊúÄ‰Ω≥ÂÆûË∑µÔºå‰ª•Âèä‰ªñ‰ª¨ÂÆ¢Êà∑Âåó‰∫¨Èì∂Ë°åÂú®‰∏§Âú∞Â§öÊ¥ªÁöÑÊ†∏ÂøÉÁ≥ªÁªü‰∏≠ÈááÁî®ÁöÑÊï∞ÊçÆÂ∫ìÂ∞±ÊòØTiDB„ÄÇ\n‰∏™‰∫∫ÊÑüÂèó ‰ºöËÆÆÁöÑÂàÜ‰∫´ËÄÖÂ§ßÂ§öÊù•Ëá™ÂõΩÂÜÖ‰∏ÄÁ∫øÁöÑ‰∫íËÅîÁΩëÂÖ¨Âè∏Ôºå‰ªñ‰ª¨ÊôÆÈÅçÂÖ∑Â§áÊµÅÈáèÂ§ß„ÄÅÊï∞ÊçÆÂ§ö„ÄÅÊäÄÊúØÂõ¢ÈòüËÉΩÂäõÊõ¥Âº∫Á≠âÁâπË¥®„ÄÇÂπ∂‰∏îÂæàÂ∞ë‰ΩøÁî®ÂÖ¨Êúâ‰∫ëÊúçÂä°Ôºå‰ΩøÁî®ÂºÄÊ∫ê‰∫ßÂìÅÂ§öÊï∞‰πü‰ºöÁª¥Êä§ÁßÅÊúâÁâàÊú¨„ÄÇ‰ªñ‰ª¨ÁöÑ‰∏öÂä°Ëß£ÂÜ≥ÊñπÊ°àÂØπ‰∏≠Â∞èÂûãÊäÄÊúØÂõ¢ÈòüÊù•ËØ¥ÂèØÂ§çÂà∂ÊÄß‰∏çÂº∫ÔºåÁÖßÊê¨ÂÆûÊñΩÁöÑÈöæÂ∫¶È´òÔºåÊõ¥Â§öÁöÑÊòØÂú®Êâ©Â±ïÊÄùË∑Ø‰∫ÜËß£‰∏öÁïåÊäÄÊúØÂä®ÊÄÅ„ÄÇ‰∏≠Â∞èÂûãÊäÄÊúØÂõ¢ÈòüÊúÄÁ¥ßËø´ÁöÑ‰∫ãÊÉÖÊòØÊª°Ë∂≥‰∏öÂä°Âø´ÈÄüÂèëÂ±ïÂíåÈúÄÊ±ÇÂ§öÂèòÔºåÊõ¥ÂêàÁêÜÁöÑËß£Ê≥ïÊòØÈÄâÁî®‰∫ëÂéÇÂïÜÁöÑÊúçÂä°ÊàñÁ¨¨‰∏âÊñπÊúçÂä°Âø´ÈÄüÈ´òÊïàÁöÑÊª°Ë∂≥‰∏öÂä°ÈúÄÊ±Ç„ÄÇÊûÅÂÆ¢ÈÇ¶Êóó‰∏ãÁöÑ‰ºöËÆÆÂ§ßÂ§öÁº∫Â∞ëËøôÁ±ªÁöÑÂàÜ‰∫´ÔºåÁõ∏ÊØî‰πã‰∏ãAWSÁöÑreInventÂ§ß‰ºöÂú®ËøôÊñπÈù¢ÂÅöÂæóÊõ¥Â•Ω„ÄÇ\n","link":"https://kane.mx/posts/2018/2018-12-13-bj-archsummit-review/","section":"posts","tags":["‰ºöËÆÆ","Êû∂ÊûÑ","ArchSummit"],"title":"2018Âåó‰∫¨ArchSummitÂõûÈ°æ"},{"body":"","link":"https://kane.mx/tags/archsummit/","section":"tags","tags":null,"title":"ArchSummit"},{"body":"","link":"https://kane.mx/tags/jenkins/","section":"tags","tags":null,"title":"Jenkins"},{"body":"","link":"https://kane.mx/tags/trouble-shooting/","section":"tags","tags":null,"title":"Trouble-Shooting"},{"body":"VÁßòÂºÄÂèëÂõ¢Èòü‰∏ÄÁõ¥‰ΩøÁî®ÁùÄJenkins CIÊù•ÊåÅÁª≠ÈõÜÊàêVÁßòÊúçÂä°ÁöÑÊñ∞ÂäüËÉΩÂíåÂêÑÁßçÊîπËøõ„ÄÇËøëÊó•ÔºåJenkins CIÂú®ÈáçÂêØ‰πãÂêéÔºåÂæàÂ§öÂ∑≤Êúâ‰ªªÂä°ÁöÑÈÖçÁΩÆÊó†Ê≥ïË¢´Jenkins CIÂÆåÊï¥ÁöÑÂä†ËΩΩÔºåÂØºËá¥ÂæàÂ§öÂäüËÉΩÊó†Ê≥ï‰ΩøÁî®„ÄÇÂØºËá¥Êàë‰ª¨Êï¥‰∏™ÁΩëÁ´ôÁöÑÂêÑÁßçÊúçÂä°Êó†Ê≥ïË¢´ÂçáÁ∫ßÊõ¥Êñ∞‰∫Ü:-(\nJenkins CIÂú®ÁÆ°ÁêÜÊéßÂà∂Âè∞ÂàóÂá∫Â¶Ç‰∏ãÁöÑÈîôËØØ‰ø°ÊÅØÔºåÁ§∫ÊÑèÁé∞Êúâ‰ªªÂä°ÁöÑÈÉ®ÂàÜÈÖçÁΩÆÁî±‰∫éÈîôËØØÊó†Ê≥ïÂä†ËΩΩ„ÄÇ\nCannotResolveClassException: hudson.plugins.git.GitSCM, CannotResolveClassException: com.cloudbees.jenkins.plugins.BitBucketTrigger, CannotResolveClassException: hudson.plugins.emailext.ExtendedEmailPublisher, CannotResolveClassException: hudson.plugins.parameterizedtrigger.BuildTrigger ÈÄöËøá‰∏äÈù¢ÁöÑÈîôËØØ‰ø°ÊÅØÔºåÊàë‰ª¨ÂàùÊ≠•ËÆ§‰∏∫ÈîôËØØÊòØÁî±‰∫éÊèí‰ª∂Êó†Ê≥ïË¢´Jenkins CIÂä†ËΩΩ„ÄÇ‰ΩÜÊòØÈÄöËøáJenkins CIÁöÑÊèí‰ª∂ÁÆ°ÁêÜÂàóË°®ÔºåÊàë‰ª¨ÂèëÁé∞GitÊèí‰ª∂Â∑≤ÁªèË¢´ËÆ§‰∏∫ÊòØÂÆâË£ÖÁöÑ‰∫Ü„ÄÇÂêåÊó∂Êàë‰ª¨‰πüÂèØ‰ª•Âú®Jenkins CIÂÆâË£ÖÁõÆÂΩï‰∏≠ÊâæÂà∞Êèí‰ª∂ÂØπÂ∫îÁöÑÊñá‰ª∂git.jarÔºåÂπ∂‰∏îÊàêÂäüÈ™åËØÅ‰∫ÜÁ±ªhudson.plugins.git.GitSCM‰πüÊòØÂ≠òÂú®Âú®jarÊñá‰ª∂ÈáåÈù¢ÁöÑ„ÄÇÈáçÊñ∞ÂÆâË£ÖGit clientÊèí‰ª∂‰πü‰∏çËÉΩËß£ÂÜ≥Ëøô‰∏™ÈîôËØØÔºÅ\nÁªèËøáËøõ‰∏ÄÊ≠•ÁöÑÂàÜÊûêÔºåÈÄöËøáJenkins CIÁöÑÁ≥ªÁªüÊó•ÂøóÔºåÊàë‰ª¨ÂèëÁé∞GitÊèí‰ª∂ËôΩÁÑ∂ÊòØÊàêÂäüÂÆâË£Ö‰∫ÜÔºå‰ΩÜÊòØÂÆÉÊâÄ‰æùËµñÁöÑÊüê‰∫õÊèí‰ª∂Ê≤°ÊúâË¢´ÂÆâË£ÖÔºÅËøôÂØºËá¥Jenkins CIÊó†Ê≥ïÊ≠£Á°ÆÂä†ËΩΩGitÊèí‰ª∂„ÄÇÈÄöËøáÊó•ÂøóÁöÑÊèêÁ§∫ÔºåÂ∞ÜÁº∫Â§±ÁöÑÊèí‰ª∂‰∏Ä‰∏ÄÂÆâË£Ö‰∏äÔºåÈáçÂêØJenkins CIÂêéÔºåÊèí‰ª∂Âä†ËΩΩÊ≠£Â∏∏Ôºå‰ªªÂä°ÊâßË°å‰πüÊÅ¢Â§çÊ≠£Â∏∏„ÄÇ\nËøô‰∏™ÈîôËØØÂá∫Áé∞ÁöÑËøòÊòØÁõ∏ÂΩìÂ•áÊÄ™„ÄÇÂõ†‰∏∫Jenkins CI‰ºöÂú®ÂÆâË£ÖÊèí‰ª∂ÁöÑÊó∂ÂÄôÂ∞Ü‰æùËµñÁöÑÊèí‰ª∂‰∏ÄÂπ∂ÂÆâË£Ö‰∏ä„ÄÇÊ≠§Â§ñËØ•Jenkins CIÂ∑≤ÁªèËøêË°åÂæà‰πÖ‰∫ÜÔºåËøô‰∫õÊèí‰ª∂‰πüÊòØ‰∏ÄÁõ¥ÂÆâË£ÖÁùÄÁöÑ„ÄÇ‰∏çËøáÁé∞Âú®ÂõûÊÉ≥Ëµ∑‰πãÂâçÂçáÁ∫ßJenkins CIÊèí‰ª∂ÁöÑÊó∂ÂÄôÔºåÈÉ®ÂàÜÊèí‰ª∂Áî±‰∫éÁΩëÁªúÂéüÂõ†ÂçáÁ∫ßÂ§±Ë¥•‰∫ÜÔºå‰ΩÜÊòØÊ≤°ÊúâÈáçÊñ∞Êõ¥Êñ∞„ÄÇËøôÂØºËá¥Ëøô‰∫õÊèí‰ª∂Â§ÑÂú®‰∫Ü‰∏Ä‰∏™‰∏çÊ≠£Á°ÆÁöÑÁä∂ÊÄÅ„ÄÇÂú®ÈáçÂêØJenkins CIÂêéÔºåËøô‰∫õÊèí‰ª∂Ë¢´Ê†áËÆ∞‰∏∫Êú™ÂÆâË£ÖÔºåÂØºËá¥‰æùËµñÂÆÉ‰ª¨ÁöÑÊèí‰ª∂Êó†Ê≥ïÂä†ËΩΩ„ÄÇ\n","link":"https://kane.mx/posts/2016/how-to-fix-jenkins-fail-to-load-job-config/","section":"posts","tags":["Jenkins","trouble-shooting"],"title":"Â¶Ç‰Ωï‰øÆÂ§çJenkins CIÊó†Ê≥ïËØªÂèñÂ≠òÂú®ÁöÑ‰ªªÂä°ÈÖçÁΩÆ"},{"body":"","link":"https://kane.mx/tags/mongodb/","section":"tags","tags":null,"title":"MongoDB"},{"body":"MongoDBÊòØÁõÆÂâçÊúÄ‰∏∫ÊµÅË°åÁöÑNoSQLÊï∞ÊçÆÂ∫ì‰πã‰∏Ä„ÄÇVÁßòÁöÑÂêéÂè∞Êï∞ÊçÆÂ∞±ÊòØ‰øùÂ≠òÂú®MongoDB‰∏≠ÁöÑÂì¶;)\nÂ∞ΩÁÆ°MongoDBÁöÑÊÄßËÉΩ‰∏∫‰∏öÁïåÁß∞ÈÅìÔºå‰ΩÜ‰ªª‰ΩïÊï∞ÊçÆÂ∫ìÁ≥ªÁªü‰ΩøÁî®‰∏≠ÈÉΩÂ≠òÂú®ÁùÄÊÖ¢Êü•ËØ¢ÁöÑÈóÆÈ¢ò„ÄÇÊÖ¢Êü•ËØ¢ÁöÑÊÄßËÉΩÈóÆÈ¢òÔºåÂèØËÉΩÊòØÁî±‰∫é‰ΩøÁî®ÈùûÊúÄ‰ºòÁöÑÊü•ËØ¢ËØ≠Âè•Ôºå‰∏çÊ≠£Á°ÆÁöÑÁ¥¢ÂºïÊàñÂÖ∂‰ªñÈÖçÁΩÆÂéüÂõ†ÂØºËá¥ÁöÑ„ÄÇ‰ΩÜÂºÄÂèë‰∫∫ÂëòÊàñÊï∞ÊçÆÂ∫ìÁª¥Êä§‰∫∫ÂëòÈ¶ñÂÖàË¶ÅÊâæÂá∫Ëøô‰∫õ‰ΩéÊïàÁöÑÊü•ËØ¢ÔºåÊâçËÉΩÂÅöÂá∫ÂØπÂ∫îÁöÑÊü•ËØ¢‰ºòÂåñ„ÄÇ\nÂú®MongoDB‰∏≠ÂÆûÁé∞ÊÖ¢Êü•ËØ¢ÁöÑprofileÊòØÈùûÂ∏∏ÂÆπÊòìÔºåÂõ†‰∏∫MongoDBÂÜÖÁΩÆ‰∫ÜprofileÂºÄÂÖ≥Êù•ËÆ∞ÂΩïÊâßË°åÊó∂Èó¥Ëß¶Âèë‰∫ÜprofileÊù°‰ª∂ÁöÑÊü•ËØ¢„ÄÇ\nÂèÇÁÖßdb.setProfileLevel()ÁöÑÊñáÊ°£ÔºåÈÄöËøá‰ª•‰∏ãÂëΩ‰ª§Â∞±ÂèØ‰ª•ËÆ∞ÂΩïÊâßË°åÊó∂ÈïøË∂ÖËøá300msÁöÑÊü•ËØ¢„ÄÇ\n1db.setProfilingLevel(1, 300) ÂΩìÊÖ¢Êü•ËØ¢Ë¢´ÈáçÁé∞ÂêéÔºåÂèØ‰ª•ÈÄöËøáÊü•Êâæsystem.profile collectionÊù•Êü•ÁúãÊâßË°åÊó∂ÈïøË∂ÖËøá300msÁöÑÊü•ËØ¢„ÄÇ\nË¢´profilerËÆ∞ÂΩï‰∏ãÊù•ÊÖ¢Êü•ËØ¢recordÁúãËµ∑Êù•Â¶Ç‰∏ãÔºå\n1{ 2 \u0026#34;op\u0026#34; : \u0026#34;query\u0026#34;, 3 \u0026#34;ns\u0026#34; : \u0026#34;myCollection\u0026#34;, 4 \u0026#34;query\u0026#34; : { 5 \u0026#34;builds\u0026#34; : { 6 \u0026#34;$elemMatch\u0026#34; : { 7 \u0026#34;builtTime\u0026#34; : null, 8 \u0026#34;$and\u0026#34; : [ 9 { 10 \u0026#34;createdTime\u0026#34; : { 11 \u0026#34;$lt\u0026#34; : ISODate(\u0026#34;2016-09-20T20:07:00.796Z\u0026#34;) 12 } 13 } 14 ] 15 } 16 } 17 }, 18 \u0026#34;ntoreturn\u0026#34; : 0, 19 \u0026#34;ntoskip\u0026#34; : 0, 20 \u0026#34;nscanned\u0026#34; : 0, 21 \u0026#34;nscannedObjects\u0026#34; : 18231, 22 \u0026#34;keyUpdates\u0026#34; : 0, 23 \u0026#34;writeConflicts\u0026#34; : 0, 24 \u0026#34;numYield\u0026#34; : 577, 25 \u0026#34;locks\u0026#34; : { 26 \u0026#34;Global\u0026#34; : { 27 \u0026#34;acquireCount\u0026#34; : { 28 \u0026#34;r\u0026#34; : NumberLong(1156) 29 } 30 }, 31 \u0026#34;Database\u0026#34; : { 32 \u0026#34;acquireCount\u0026#34; : { 33 \u0026#34;r\u0026#34; : NumberLong(578) 34 } 35 }, 36 \u0026#34;Collection\u0026#34; : { 37 \u0026#34;acquireCount\u0026#34; : { 38 \u0026#34;r\u0026#34; : NumberLong(578) 39 } 40 } 41 }, 42 \u0026#34;nreturned\u0026#34; : 2, 43 \u0026#34;responseLength\u0026#34; : 98076, 44 \u0026#34;millis\u0026#34; : 11161, 45 \u0026#34;execStats\u0026#34; : { 46 \u0026#34;stage\u0026#34; : \u0026#34;COLLSCAN\u0026#34;, 47 \u0026#34;filter\u0026#34; : { 48 \u0026#34;builds\u0026#34; : { 49 \u0026#34;$elemMatch\u0026#34; : { 50 \u0026#34;$and\u0026#34; : [ 51 { 52 \u0026#34;$and\u0026#34; : [ 53 { 54 \u0026#34;createdTime\u0026#34; : { 55 \u0026#34;$lt\u0026#34; : ISODate(\u0026#34;2016-09-20T20:07:00.796Z\u0026#34;) 56 } 57 } 58 ] 59 }, 60 { 61 \u0026#34;builtTime\u0026#34; : { 62 \u0026#34;$eq\u0026#34; : null 63 } 64 } 65 ] 66 } 67 } 68 }, 69 \u0026#34;nReturned\u0026#34; : 2, 70 \u0026#34;executionTimeMillisEstimate\u0026#34; : 11080, 71 \u0026#34;works\u0026#34; : 18233, 72 \u0026#34;advanced\u0026#34; : 2, 73 \u0026#34;needTime\u0026#34; : 18230, 74 \u0026#34;needFetch\u0026#34; : 0, 75 \u0026#34;saveState\u0026#34; : 577, 76 \u0026#34;restoreState\u0026#34; : 577, 77 \u0026#34;isEOF\u0026#34; : 1, 78 \u0026#34;invalidates\u0026#34; : 0, 79 \u0026#34;direction\u0026#34; : \u0026#34;forward\u0026#34;, 80 \u0026#34;docsExamined\u0026#34; : 18231 81 }, 82 \u0026#34;ts\u0026#34; : ISODate(\u0026#34;2016-09-20T23:07:14.313Z\u0026#34;), 83 \u0026#34;client\u0026#34; : \u0026#34;10.171.127.66\u0026#34;, 84 \u0026#34;allUsers\u0026#34; : [ 85 { 86 \u0026#34;user\u0026#34; : \u0026#34;dbuser\u0026#34;, 87 \u0026#34;db\u0026#34; : \u0026#34;mydb\u0026#34; 88 } 89 ], 90 \u0026#34;user\u0026#34; : \u0026#34;dbuser@mydb\u0026#34; 91} ‰∏äÈù¢ÁöÑÊï∞ÊçÆÂÖ∑‰ΩìËß£ËØªÂ¶Ç‰∏ãÔºå\nop: 'query'Ë°®Á§∫ÊâßË°åÁöÑÊòØÊü•ËØ¢Ôºå nsÊòØÊåáÊü•ËØ¢ÁöÑcollectionÔºå queryÊòØÂÖ∑‰ΩìÁöÑÊü•ËØ¢ËØ≠Âè•Ôºå Ê†∏ÂøÉÈÉ®ÂàÜÊòØexecStatsÔºåÁªôÂá∫‰∫ÜÁöÑÊü•ËØ¢ËØ≠Âè•ÂÖ∑‰ΩìÊâßË°åÁªüËÆ°ÔºåË∑ü**.explain('execStats')**ÁöÑÂÜÖÂÆπÊòØ‰∏ÄËá¥ÁöÑ„ÄÇ‰∏äÈù¢ÁöÑÁªüËÆ°ÊòØËØ¥ÔºåËøô‰∏™queryÊâßË°å‰∫ÜÊï¥‰∏™collectionÁöÑÊâ´Êèè(ÊÄªËÆ°Êâ´Êèè‰∫Ü18231‰∏™ÊñáÊ°£)ÔºåÊúÄÁªàËøîÂõû‰∫Ü2Êù°ÊñáÊ°£ÔºåËä±Ë¥π‰∫Ü11080msÔºå‰πüÂ∞±ÊòØ11sËøòÂ§öÁöÑÊó∂Èó¥ÔºÅËøôË°®ÊòéË¢´ËÆ∞ÂΩï‰∏ãÁöÑÊÖ¢Êü•ËØ¢Ë∑ücollectionÁöÑÁ¥¢ÂºïËÆæÁΩÆÊúâÈóÆÈ¢òÔºåËØ•Êü•ËØ¢Ê≤°ÊúâÁî®‰∏äÁ¥¢Âºï„ÄÇËß£ÂÜ≥ÊñπÊ°àÂæàÁÆÄÂçïÔºåÊîπÂñÑÊü•ËØ¢ËØ≠Âè•‰ΩøÁî®Â≠òÂú®ÁöÑÁ¥¢ÂºïÊàñËÄÖËÆæÁΩÆÂêàÁêÜÁöÑÁ¥¢Âºï„ÄÇ tsÊòØÊü•ËØ¢ÂºÄÂßãËØ∑Ê±ÇÁöÑÊó∂Èó¥Ôºå allUsersÂíåuserÈÉΩÊòØMongoDB clientËøûÊé•ÊâÄ‰ΩøÁî®ÁöÑÁî®Êà∑„ÄÇ ","link":"https://kane.mx/posts/2016/how-to-find-slow-queries-in-mongodb/","section":"posts","tags":["MongoDB","performance-tuning"],"title":"MongoDB‰∏≠Â¶Ç‰ΩïÊâæÂá∫ÊÖ¢Êü•ËØ¢"},{"body":"Swarm modeÂú®Docker v1.12‰∏≠Ê≠£ÂºèÂèëÂ∏ÉÔºåSwarm modeÂ∏¶Êù•‰∫ÜËØ∏Â¶ÇDockerÈõÜÁæ§ÔºåÂÆπÂô®ÁºñÊéíÔºåÂ§ö‰∏ªÊú∫ÁΩëÁªúÁ≠âÊøÄÂä®‰∫∫ÂøÉÁöÑÁâπÊÄß„ÄÇVÁßòÂõ¢Èòü‰πüÂ∞ùËØïÁùÄÂ∞ÜÂêÑÁßçÂêéÂè∞ÊúçÂä°ÈÉ®ÁΩ≤Âà∞Docker Swarm ClusterËé∑ÂèñÊõ¥Â•ΩÁöÑÂºπÊÄßËÆ°ÁÆóËÉΩÂäõ„ÄÇ\nDocker v1.12‰∏≠Ê≠£ÂºèÂèëÂ∏ÉÁöÑDocker SwarmÂú®Êàë‰ª¨ÂÆûÁî®‰∏≠ÂèëÁé∞‰ªçÊúâ‰∏çÂ∞ë‰∏çË∂≥‰πãÂ§ÑÔºåËÆ©Êàë‰ª¨‰∏Ä‰∏ÄÂàÜ‰∫´ÁªôÂ§ßÂÆ∂„ÄÇ\nÊó†Ê≥ïÂ∞ÜÊúçÂä°ÁöÑpublishedÁ´ØÂè£Âè™ÁªëÂÆöÂà∞ÁâπÁÇπÁöÑÁΩëÂç°‰∏ä„ÄÇÊØîÂ¶ÇÊàë‰ª¨ÁöÑ‰∫ë‰∏ªÊú∫ÔºàÂêåÊó∂‰πüÊòØSwarm manager/nodeÔºâÊúâeth0Âíåeth1‰∏§ÂùóÁΩëÂç°ÔºåÂàÜÂà´ËøûÊé•ÂÜÖÁΩëÂíåÂ§ñÁΩë„ÄÇÊàë‰ª¨ËÆ°ÂàíÂú®Docker Swarm‰∏≠ËøêË°å‰∏Ä‰∏™nginxÊúçÂä°ÔºåÈÄöËøá80/443Á´ØÂè£Êèê‰æõHTTP/HTTPSÊúçÂä°„ÄÇÂΩìÊàë‰ª¨Â∏åÊúõÂ∞Ünginx‰∏≠ÁöÑWebÊúçÂä°Êö¥Èú≤Âú®‰∫ë‰∏ªÊú∫‰∏äÊó∂ÔºåÊàë‰ª¨ÈÄöËøá‰ª•‰∏ãÂëΩ‰ª§ÂàõÂª∫nginxÊúçÂä°„ÄÇÁÑ∂ËÄåÊàë‰ª¨Êó†Ê≥ïÈÄâÊã©Â∞ÜpublishedÁöÑ80Á´ØÂè£ÁªëÂÆöÂú®Âì™‰∏™interface‰∏ä„ÄÇDocker Swarm‰ºöËá™Âä®Â∞ÜÊúçÂä°ÁõëÂê¨Âà∞Swarm nodeÁöÑÊâÄÊúâ80Á´ØÂè£‰∏ä„ÄÇÂ¶ÇÊûúÊàë‰ª¨Âè™ÊÉ≥Â∞ÜËøô‰∏™ÊúçÂä°Êö¥Èú≤Âú®ÂÜÖÁΩëinterfaceÊöÇÊó∂Êó†Ê≥ïÂÆûÁé∞„ÄÇ 1docker service create --name vme-nginx --network vme-network --replicas 1 \\ 2 --publish 80:80 --publish 443:443 \\ 3 nginx:1.11 Êó†Ê≥ï‰∏∫Docker SwarmÂÜÖËøêË°åÁöÑÊúçÂä°ËÆæÁΩÆ‰∏ªÊú∫Âêç„ÄÇÈÄöËøádocker runÂëΩ‰ª§ÊâßË°åÁöÑÂÆπÂô®ÂèØ‰ª•ËÆæÁΩÆhostname„ÄÇÊØîÂ¶ÇÔºå 1docker run --hostname vme-nginx nginx:1.11 ‰ΩÜÊòØdocker service createÂëΩ‰ª§Áº∫Â∞ëÁ≠â‰ª∑ÁöÑÂèÇÊï∞‰∏∫ÂÆπÂô®ÊåáÂÆöhostname„ÄÇ‰∏Ä‰∫õ‰æùËµñ‰∫éhostnameÁöÑÊúçÂä°Â∞ÜÊó†Ê≥ïÈÉ®ÁΩ≤Âú®Docker Swarm‰∏≠ÔºåÊØîÂ¶Çclustered rabbitmq„ÄÇ Docker composeËøò‰∏çËÉΩ‰∏éDocker SwarmÂÆåÁæéÈõÜÊàê„ÄÇÁõÆÂâçÊúâ‰∏Ä‰∏™experimentalÁöÑDocker Stacks and Distributed Application BundlesÂú®Â∞ùËØïÂÅöÊõ¥Â•ΩÁöÑÊï¥Âêà„ÄÇ docker service updateÊúâÊó∂‰∏çËÉΩÊõ¥Êñ∞Ê≠£Âú®ËøêË°å‰∏≠ÁöÑcontainer„ÄÇÊõ¥Â§öËÆ®ËÆ∫ËßÅËøô‰∏™issue„ÄÇ ","link":"https://kane.mx/posts/2016/the-limitations-docker-swarm-mode-v1.12/","section":"posts","tags":["docker","docker-swarm"],"title":"Docker Swarm mode(v1.12.x)ÁöÑ‰∏Ä‰∫õ‰ΩøÁî®ÈôêÂà∂"},{"body":"","link":"https://kane.mx/tags/docker-swarm/","section":"tags","tags":null,"title":"Docker-Swarm"},{"body":"","link":"https://kane.mx/tags/ubuntu-1404/","section":"tags","tags":null,"title":"Ubuntu-1404"},{"body":"VÁßòÂõ¢Èòü‰∏ÄÁõ¥Ëá¥Âäõ‰∫éÁî®ÊäÄÊúØÊîπÂñÑ‰∫ßÂìÅ„ÄÇVÁßòÂêéÂè∞ÁöÑÂêÑÁßçÊúçÂä°‰∏ÄÁõ¥ÊòØÈÄöËøáÂÆåÂñÑÁöÑDevopsÊµÅÁ®ãËá™Âä®ÈÉ®ÁΩ≤Âà∞DockerÂÆπÂô®ÈõÜÁæ§„ÄÇÈöèÁùÄSwarm modeÂú®Docker v1.12‰∏≠Ê≠£ÂºèÂèëÂ∏ÉÔºåSwarm modeÂ∏¶Êù•‰∫ÜËØ∏Â¶ÇDockerÈõÜÁæ§ÔºåÂ§ö‰∏ªÊú∫ÁΩëÁªúÁ≠âÊøÄÂä®‰∫∫ÂøÉÁöÑÁâπÊÄß„ÄÇÊàë‰ª¨‰πüÂú®Â∞ùËØïÂ∞ÜVÁßòÊúçÂä°ÈÉ®ÁΩ≤Âà∞Docker Swarm ClusterËé∑ÂèñÊõ¥Â•ΩÁöÑÂºπÊÄßËÆ°ÁÆóËÉΩÂäõ„ÄÇ\nÁÑ∂ËÄåÊàë‰ª¨Â∞ÜVÁßòÁöÑÊúçÂä°ÈÉ®ÁΩ≤Âà∞Docker Swarm ClusterÊó∂ÈÅáÂà∞ÊúçÂä°ÂÆπÂô®Êó†Ê≥ïÂêØÂä®ÁöÑÈîôËØØ„ÄÇÈîôËØØ‰ø°ÊÅØÁ±ª‰ººÂ¶Ç‰∏ãÔºå\nstarting container failed: could not add veth pair inside the network sandbox: could not find an appropriate master \u0026quot;ov-000100-1wkbc\u0026quot; for \u0026quot;vethee39f9d\u0026quot;\nÁªèËøá‰∏éDocker Á§æÂå∫ÁöÑÂõûÈ¶àËÆ®ËÆ∫ÔºåÊöÇÊó∂ÈÄöËøáÂçáÁ∫ßDocker‰∏ªÊú∫(OS: Ubuntu 14.04 LTS)ÁöÑÂÜÖÊ†∏ÁâàÊú¨Ëß£ÂÜ≥‰∫ÜËøô‰∏™ÈîôËØØ„ÄÇ\nÂÖ∑‰ΩìÊñπÊ≥ïÂ¶Ç‰∏ãÔºå\n1root@swarm1:~# uname -r 23.13.0-32-generic 3 4root@swarm1:~# apt-get install linux-generic-lts-vivid 5root@swarm1:~# reboot 6 7root@swarm1:~# uname -r 83.19.0-69-generic Ëá≥‰∫éËøô‰∏™ÈîôËØØÁöÑÊ†πÊú¨ÂéüÂõ†ÊòØDockerÁöÑbugËøòÊòØÂØπLinux KernelÊúâÁâπÊÆäÁöÑË¶ÅÊ±ÇÔºåÈúÄË¶ÅDockerÂºÄÂèëËøõ‰∏ÄÊ≠•Á°ÆËÆ§„ÄÇÂ¶ÇÊûúÂØπÊ≠§ÈóÆÈ¢òÊúâÊõ¥Â§öÂÖ¥Ë∂£ÔºåÂèØ‰ª•ÂÖ≥Ê≥®docker issue #25039„ÄÇ\n","link":"https://kane.mx/posts/2016/docker-swarm-mode-in-ubuntu-1404/","section":"posts","tags":["docker","docker-swarm","ubuntu-1404"],"title":"ÂàõÂª∫‰∫éDocker SwarmÁöÑÊúçÂä°Êó†Ê≥ïÂú®Ubuntu 14.04 LTS‰∏≠ËøêË°å"},{"body":"","link":"https://kane.mx/tags/angularjs/","section":"tags","tags":null,"title":"Angularjs"},{"body":"","link":"https://kane.mx/tags/nginx/","section":"tags","tags":null,"title":"Nginx"},{"body":"","link":"https://kane.mx/tags/seo/","section":"tags","tags":null,"title":"Seo"},{"body":"","link":"https://kane.mx/tags/single-page-app/","section":"tags","tags":null,"title":"Single-Page-App"},{"body":"Âú®‰πãÂâçÁöÑÊñáÁ´†ÊàëÊõæÊèêÂà∞Âü∫‰∫éAngularjsÁöÑÂçïÈ°µÈù¢Â∫îÁî®Âú®Áî®Êà∑‰ΩìÈ™å‰∏äÁöÑÁßçÁßçÂ•ΩÂ§Ñ„ÄÇÁÑ∂ËÄå‰ªª‰Ωï‰∫ãÊÉÖÈÉΩ‰∏çÊòØÂÆåÁæéÁöÑÔºåAngularÂíåÁ±ª‰ººÁöÑÊ°ÜÊû∂ÈÄöËøáÂ∫îÁî®ÂÜÖÂÅöÈ°µÈù¢Ë∑ØÁî±ÁöÑÂÆûÁé∞ÁªôSEOÔºà‰πü‰øóÁß∞ÊêúÁ¥¢ÂºïÊìé‰ºòÂåñÔºâÂ∏¶Êù•‰∫Ü‰∏çÂ∞ëÈ∫ªÁÉ¶„ÄÇ\nÈ¶ñÂÖàÔºåÊàë‰ª¨Êù•ÁúãÁúãÈ°µÈù¢ÂÜÖË∑ØÁî±ÊòØÂ¶Ç‰ΩïÂÆûÁé∞ÁöÑ„ÄÇÈªòËÆ§AngularjsÁîüÊàêÁöÑÈ°µÈù¢uriÁ±ªÂûãÂ¶Ç‰∏ãÔºå\nhttp://mydomain.com/#/app/page1\nÊµèËßàÂô®ËØ∑Ê±Ç‰∏äÈù¢Ëøô‰∏™uriÁöÑÊó∂ÂÄôÔºåÂÆûÈôÖÂèëÈÄÅÁªôÊúçÂä°Âô®ÁöÑËØ∑Ê±ÇÂú∞ÂùÄÊòØhttp://mydomain.com/, webÊúçÂä°Âô®‰ºöÂ∞ÜÈªòËÆ§ÁöÑÈ°µÈù¢ÂìçÂ∫îÁªôÊµèËßàÂô®ÔºåÊØîÂ¶Çindex.htmlÊàñindex.phpÁ≠â„ÄÇ\nÊµèËßàÂô®ËøîÂõûÁöÑÈ°µÈù¢ÈáåÈù¢ÂºïÂÖ•‰∫ÜAngularjsÂíåÂÖ∂‰ªñÂ∫îÁî®ÈúÄË¶ÅÁöÑJSÂ∫ì„ÄÇAngularjsÂ∫îÁî®ÂºÄÂßãÊâßË°åÂêéÔºåÂ∞ùËØïÂ§ÑÁêÜË∑ØÁî±**/app/page1**„ÄÇÂ¶ÇÊûúÂ∫îÁî®ÂÆö‰πâ‰∫ÜËØ•Ë∑ØÁî±ÔºåÂ∞ÜÂä†ËΩΩÂøÖË¶ÅÁöÑJSÂ∫ìÂíåÂÖ∂‰ªñhtmlÁâáÊÆµÊù•ÂÆåÊàêÈ°µÈù¢ÁöÑÊ∏≤Êüì„ÄÇ\nÁêÜËß£‰∫ÜAngularjsÈ°µÈù¢ÂÜÖË∑ØÁî±ÁöÑÂéüÁêÜÂêéÔºåÊàë‰ª¨Áü•ÈÅì‰∫ÜÂØπÊµèËßàÂô®ÊàñÊêúÁ¥¢ÂºïÊìéÁà¨Ëô´ËÄåË®ÄÔºåÂçïÈ°µÈù¢Â∫îÁî®ÊâÄÊúâÁöÑÈ°µÈù¢ÂØπÊµèËßàÂô®ÂíåÊêúÁ¥¢ÂºïÊìéÈÉΩÊòØ‰∏Ä‰∏™ÁΩëÂùÄÔºåÊØîÂ¶Çhttp://mydomain.com/„ÄÇËøôÊ†∑ÂØπÁà¨Ëô´ÊäìÂèñÁ´ôÂÜÖÈìæÊé•ÈÄ†Êàê‰∫ÜÂõ∞ÈöæÔºåÂõ†‰∏∫ÊâÄÊúâÂ∫îÁî®ÂÜÖÁöÑÈìæÊé•ÈÉΩË¢´ËÆ§ÂÅö‰∫ÜÂêå‰∏Ä‰∏™ÈìæÊé•„ÄÇ\nÊàë‰ª¨ÁêÜËß£‰∫Üuri http://mydomain.com/#/app/page1ÁªôSEOÈÄ†ÊàêÁöÑÈ∫ªÁÉ¶ÔºåÊé•‰∏ãÊù•Â∞±ÊòØËÆ®ËÆ∫Â¶Ç‰ΩïÈíàÂØπSEOÊù•‰ΩúÁöÑ‰ºòÂåñ„ÄÇ\nÊúÄÁêÜÊÉ≥ÁöÑÊÉÖÂÜµÂΩìÁÑ∂ÊòØÊêúÁ¥¢ÂºïÊìéÁà¨Ëô´ÂèòÁöÑÊõ¥Âä†Êô∫ËÉΩÔºåÂÆÉËÉΩÁêÜËß£ÁΩëÁ´ôÁöÑÊ°ÜÊû∂ÔºåÂπ∂‰∏îÈíàÂØπÊ≠§ÁßçÊÉÖÂÜµÂÅöÂá∫‰ºòÂåñ„ÄÇ‰ΩÜÊà™Ê≠¢Âà∞ÁõÆÂâçÔºåÂåÖÊã¨GoogleÂú®ÂÜÖÁöÑÊâÄÊúâÁà¨Ëô´ÈÉΩÊó†Ê≥ïÂÅöÂà∞ËøôÁÇπ„ÄÇÈÇ£Êàë‰ª¨SEOÁöÑ‰ºòÂåñÂè™ËÉΩÂú®Â∫îÁî®ËøôËæπÊù•ÂÅö‰∫Ü„ÄÇ\nAngularjsÊèê‰æõ‰∫Ü‰∏ÄÁßçHTML5 modeÊ®°ÂºèÂèØ‰ª•Âà©Áî®HTML5 History APIÊù•ÂÆûÁé∞È°µÈù¢ÂÜÖË∑ØÁî±„ÄÇÊâìÂºÄÁöÑÊñπÊ≥ïÂ¶Ç‰∏ãÔºå\n1$locationProvider.html5Mode(true); ÂêåÊó∂Âú®index.htmlÈ°µÈù¢Âä†‰∏äÂ¶Ç‰∏ãÊ†áÁ≠æÔºå\n1\u0026lt;base href=\u0026#34;/\u0026#34;\u0026gt; Âú®ÊâìÂºÄHTML5 modeÂêéÁöÑAngularjsÂ∫îÁî®ÁöÑÈìæÊé•ÁúãËµ∑Êù•Â∞±ÊòØËøôÊ†∑‰∫ÜÔºå\nhttp://mydomain.com/app/page1\nÊñ∞ÁöÑÈìæÊé•Ê®°ÂºèÂíåÁ´ôÂÜÖË∑≥ËΩ¨ÈÄöËøáËÆøÈóÆÁΩëÁ´ô‰∏ªÈ°µËØ∑Ê±ÇÂ∞ÜÊ≤°Êúâ‰ªª‰ΩïÈóÆÈ¢ò„ÄÇÁÑ∂ËÄåÁõ¥Êé•Âú®ÊµèËßàÂô®ËØ∑Ê±ÇÂ¶Ç‰∏äÈìæÊé•ÁöÑËØùÔºåWebÊúçÂä°Âô®Â∞ÜÂ∞ùËØïËØ∑Ê±Ç/app/page1ÔºåÈÄöÂ∏∏‰ºöÂæóÂà∞404ÁöÑÈ°µÈù¢ÂìçÂ∫î„ÄÇÂõ†‰∏∫ÊúçÂä°Âô®‰∏äÂπ∂Ê≤°ÊúâÈÉ®ÁΩ≤È°µÈù¢/app/page1„ÄÇ\nËøôÊó∂Â∞±ÈúÄË¶ÅÂú®WebÂ∫îÁî®ÊúçÂä°Âô®ÊàñÂ∫îÁî®ÈáåÈù¢ÂÆûÁé∞URL Rewrite„ÄÇÂ∞Ü/app/page1ÁöÑËØ∑Ê±ÇËΩ¨Âà∞ÂçïÈ°µÈù¢Â∫îÁî®htmlÊñá‰ª∂‰∏ä„ÄÇ\n‰∏ãÈù¢ÊòØ‰∏Ä‰∫õWebÊúçÂä°Âô®ÊàñÂ∫îÁî®ÁöÑÂèÇËÄÉÈÖçÁΩÆÔºå\nApache Rewrites\n1\u0026lt;VirtualHost *:80\u0026gt; 2 ServerName my-app 3 4 DocumentRoot /path/to/app 5 6 \u0026lt;Directory /path/to/app\u0026gt; 7 RewriteEngine on 8 9 # Don\u0026#39;t rewrite files or directories 10 RewriteCond %{REQUEST_FILENAME} -f [OR] 11 RewriteCond %{REQUEST_FILENAME} -d 12 RewriteRule ^ - [L] 13 14 # Rewrite everything else to index.html to allow html5 state links 15 RewriteRule ^ index.html [L] 16 \u0026lt;/Directory\u0026gt; 17\u0026lt;/VirtualHost\u0026gt; Nginx Rewrites\n1server { 2 server_name my-app; 3 4 root /path/to/app; 5 6 location / { 7 try_files $uri $uri/ /index.html; 8 } 9} Azure IIS Rewrites\n1\u0026lt;system.webServer\u0026gt; 2 \u0026lt;rewrite\u0026gt; 3 \u0026lt;rules\u0026gt; 4 \u0026lt;rule name=\u0026#34;Main Rule\u0026#34; stopProcessing=\u0026#34;true\u0026#34;\u0026gt; 5 \u0026lt;match url=\u0026#34;.*\u0026#34; /\u0026gt; 6 \u0026lt;conditions logicalGrouping=\u0026#34;MatchAll\u0026#34;\u0026gt; 7 \u0026lt;add input=\u0026#34;{REQUEST_FILENAME}\u0026#34; matchType=\u0026#34;IsFile\u0026#34; negate=\u0026#34;true\u0026#34; /\u0026gt; 8 \u0026lt;add input=\u0026#34;{REQUEST_FILENAME}\u0026#34; matchType=\u0026#34;IsDirectory\u0026#34; negate=\u0026#34;true\u0026#34; /\u0026gt; 9 \u0026lt;/conditions\u0026gt; 10 \u0026lt;action type=\u0026#34;Rewrite\u0026#34; url=\u0026#34;/\u0026#34; /\u0026gt; 11 \u0026lt;/rule\u0026gt; 12 \u0026lt;/rules\u0026gt; 13 \u0026lt;/rewrite\u0026gt; 14\u0026lt;/system.webServer\u0026gt; Express Rewrites\n1var express = require(\u0026#39;express\u0026#39;); 2var app = express(); 3 4app.use(\u0026#39;/js\u0026#39;, express.static(__dirname + \u0026#39;/js\u0026#39;)); 5app.use(\u0026#39;/dist\u0026#39;, express.static(__dirname + \u0026#39;/../dist\u0026#39;)); 6app.use(\u0026#39;/css\u0026#39;, express.static(__dirname + \u0026#39;/css\u0026#39;)); 7app.use(\u0026#39;/partials\u0026#39;, express.static(__dirname + \u0026#39;/partials\u0026#39;)); 8 9app.all(\u0026#39;/*\u0026#39;, function(req, res, next) { 10 // Just send the index.html for other files to support HTML5Mode 11 res.sendFile(\u0026#39;index.html\u0026#39;, { root: __dirname }); 12}); 13 14app.listen(3006); //the port you want to use ASP.Net C# Rewrites\n1private const string ROOT_DOCUMENT = \u0026#34;/default.aspx\u0026#34;; 2 3protected void Application_BeginRequest( Object sender, EventArgs e ) 4{ 5 string url = Request.Url.LocalPath; 6 if ( !System.IO.File.Exists( Context.Server.MapPath( url ) ) ) 7 Context.RewritePath( ROOT_DOCUMENT ); 8} ","link":"https://kane.mx/posts/2016/seo-optimization-for-angularajs-based-app/","section":"posts","tags":["angularjs","single-page-app","seo","nginx","ÊêúÁ¥¢ÂºïÊìé‰ºòÂåñ"],"title":"Âü∫‰∫éAngularjsÂçïÈ°µÈù¢Â∫îÁî®ÁöÑSEO‰ºòÂåñ"},{"body":"","link":"https://kane.mx/tags/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E4%BC%98%E5%8C%96/","section":"tags","tags":null,"title":"ÊêúÁ¥¢ÂºïÊìé‰ºòÂåñ"},{"body":"","link":"https://kane.mx/tags/session-management/","section":"tags","tags":null,"title":"Session-Management"},{"body":"","link":"https://kane.mx/tags/spring-boot/","section":"tags","tags":null,"title":"Spring-Boot"},{"body":"","link":"https://kane.mx/tags/spring-framework/","section":"tags","tags":null,"title":"Spring-Framework"},{"body":"","link":"https://kane.mx/tags/spring-session/","section":"tags","tags":null,"title":"Spring-Session"},{"body":"Âú®ÂæÆÊúçÂä°ÂíåÂÆπÂô®Á≠âÊäÄÊúØÁöÑÂ∏ÆÂä©‰∏ãÔºåWebÂ∫îÁî®ÂèØ‰ª•ËæÉ‰∏∫ÂÆπÊòìÁöÑËøõË°åÊ∞¥Âπ≥Êâ©Â±ïÔºåÊù•ÈÉ®ÁΩ≤Êõ¥Â§öÁöÑÂ∫îÁî®ÂÆû‰æãÊù•ÊèêÂçáËØ∑Ê±ÇÂ§ÑÁêÜÊï∞QPS„ÄÇÂΩìWebÊúçÂä°ÊúâÁä∂ÊÄÅÁöÑÊó∂ÂÄôÔºåÂ¶Ç‰ΩïÂú®ÈõÜÁæ§‰∏ãÁÆ°ÁêÜÁî®Êà∑sessionÊàê‰∏∫Êñ∞ÁöÑÂæÖËß£ÂÜ≥ÈóÆÈ¢ò„ÄÇ\nSpring FrameworkÈíàÂØπÊ≠§ÈóÆÈ¢òË°çÁîüÂá∫‰∫Ü‰∏Ä‰∏™Â≠êÈ°πÁõÆSpring SessionÊù•ÂÆûÁé∞ÈõÜÁæ§‰∏ãÁöÑsessionÁÆ°ÁêÜ„ÄÇËØ•È°πÁõÆÊèê‰æõ‰∫Ü‰ª•‰∏ãÂäüËÉΩÔºö\nÊèê‰æõAPIÂíåÂÆûÁé∞ÁÆ°ÁêÜÁî®Êà∑session HttpSession - ÊõøÊç¢ÂÆûÁé∞Â∫îÁî®ÂÆπÂô®(tomcat)‰∏≠ÁöÑHttpSession Clustered Sessions - ÂÆûÁé∞ÈõÜÁæ§ÁöÑsessionËÄå‰∏ç‰æùËµñ‰ªª‰ΩïÂ∫îÁî®ÂÆπÂô®ÁâπÂÆöÁöÑËß£ÂÜ≥ÊñπÊ°à Multiple Browser Sessions - ÊîØÊåÅÂ§ö‰∏™Áî®Êà∑session‰øùÂ≠òÂú®Âêå‰∏Ä‰∏™ÊµèËßàÂô®ÂÆû‰æã‰∏≠ (‰æãÂ¶ÇÔºåÁ±ª‰ººGoogleÁöÑÂ§öÁî®Êà∑ËÆ§ËØÅ). RESTful APIs - ÈÄöËøáÊîØÊåÅsession idsÂú®HttpËØ∑Ê±ÇÂ§¥Êù•ÊîØÊåÅRestful APIÁöÑËÆ§ËØÅ WebSocket - ËÉΩÂ§ü‰øùËØÅHttpSessionÁöÑÂ≠òÊ¥ªÂΩìÂú®Êé•ÂèóWebSocketÊ∂àÊÅØÊó∂ ‰ªé‰∏äÈù¢ÁöÑÂäüËÉΩÂàóË°®‰∏≠ÔºåÊàë‰ª¨ÂèØ‰ª•ÁúãÂà∞Spring SessionËÉΩÂ§üÊª°Ë∂≥ÈõÜÁæ§‰∏ãÂêÑÁßçsessionÁöÑ‰ΩøÁî®Âú∫ÊôØÂíåÈúÄÊ±Ç„ÄÇ\nSpring SessionÂú®1.0.0 GAÂèØ‰ª•‰ΩøÁî®RedisÂÅö‰∏∫sessionÂÇ®Â≠òÁöÑbackend„ÄÇ\nÈÄöËøáchangelogÔºåÂú®ÊúÄÊñ∞ÁöÑ1.1.0 GA‰∏≠ÊîØÊåÅËá™ÂÆö‰πâCookieÁöÑÂàõÂª∫ÔºåÂÖÅËÆ∏Ëá™ÂÆö‰πâCookieÁöÑËøáÊúüÊó∂Èó¥Ôºå‰ΩúÁî®ÂüüÁ≠â„ÄÇÂú®Âç≥Â∞ÜÂèëÂ∏ÉÁöÑ1.2.0 GAÁâàÊú¨‰∏≠ÔºåÂ∞ÜÊ∑ªÂä†ÊîØÊåÅJDBCÁöÑÂÖ≥Á≥ªÊï∞ÊçÆÂ∫ìÂíåMongoDB‰Ωú‰∏∫session‰øùÂ≠òÁöÑbackend„ÄÇ\nÊ≠§Â§ñÔºåSpring SessionÂêåSpring-bootÁöÑÂ∫îÁî®ÊúâÂæàÂ•ΩÁöÑÈõÜÊàêÔºåÂè™ÈúÄË¶ÅÂçÅÂ§öË°å‰ª£Á†ÅÂèäÈÖçÁΩÆÂç≥ÂèØÈõÜÊàêÔºÅ\n","link":"https://kane.mx/posts/2016/clustered-session-under-spring-framework/","section":"posts","tags":["web-2.0","session-management","spring-framework","spring-session","spring-boot"],"title":"SpringÊ°ÜÊû∂‰∏ãÁöÑÂàÜÂ∏ÉÂºèsessionÁÆ°ÁêÜ"},{"body":"","link":"https://kane.mx/tags/web-2.0/","section":"tags","tags":null,"title":"Web-2.0"},{"body":"","link":"https://kane.mx/tags/architecture/","section":"tags","tags":null,"title":"Architecture"},{"body":"Êò•Â§©Êù•‰∫ÜÔºåVÁßòÂ§ßÂÆ∂Â∫≠‰πüÊñ∞Â¢û‰∫Ü‰∏§‰Ωç10ÂêéÁöÑ‰º†‰∫∫„ÄÇÊñ∞Áà∏Áà∏ÁªèËøá‰∏ÄÁï™Âøô‰π±ÂêéÔºåÂ∏åÊúõÂú®ËøôÈáå‰∏éÂ§ßÂÆ∂ÂàÜ‰∫´VÁßòÁöÑÊû∂ÊûÑÔºåÂÖ±ÂêåÊé¢ËÆ®Â¶Ç‰ΩïÂø´ÈÄüÁöÑÊûÑÂª∫È´òÂèØÁî®ÔºåÈ´òÊÄßËÉΩÁöÑWebÊúçÂä°„ÄÇ\nVÁßòËá¥Âäõ‰∫éÊèê‰æõÊúÄÂ•ΩÁöÑÂú®Á∫øËßÜÈ¢ëÂà∂‰Ωú‰∫ëÂπ≥Âè∞„ÄÇËÆ©Áî®Êà∑ÈöèÊó∂ÈöèÂú∞Èõ∂Èó®ÊßõÁöÑÂø´ÈÄüÂà∂‰ΩúÂá∫È´òË¥®ÈáèÈ´òÊ∏ÖÊô∞Â∫¶ÁöÑËßÜÈ¢ëÔºåÊù•Á∫™ÂøµËÆ∞ÂΩïÁîüÊ¥ª‰∏≠ÊúâÊÑè‰πâÁöÑÊó∂ÂàªÔºåÂêåÊó∂Â∞ÜËøô‰ªΩÂø´‰πê‰º†ÈÄíÁªôÊõ¥Â§öÁöÑÂÆ∂‰∫∫ÊúãÂèã‰∏ÄËµ∑ÂàÜ‰∫´„ÄÇ\nÁÑ∂ËÄåË¶ÅÂèØÈù†ÁöÑÂèØÊâ©Â±ïÁöÑÂÆûÁé∞ËøôÊ†∑Áúã‰ººÁÆÄÂçïÁöÑÈúÄÊ±ÇÔºåÂÖ∂ËÉåÂêéÁ°ÆÁî±‰ºóÂ§öÁü•ÂêçÂºÄÊ∫êÊäÄÊúØÔºåÂèØÈù†ÁöÑ‰∫ëÊúçÂä°Ôºå‰∏çÈó¥Ê≠áÁöÑÁõëÊéßËøêÁª¥Êù•ÂÆûÁé∞Âíå‰øùËØÅÁöÑ„ÄÇ\nVÁßòÊû∂ÊûÑÁöÑÂü∫Êú¨ÁõÆÊ†áÂ∞±ÊòØË¶ÅÂÆûÁé∞Ôºå\nÊúçÂä°ÁöÑÈ´òÊâ©Â±ïÊÄß„ÄÇÊúâÊúâÊïàÂèØÈù†ÁöÑÊñπÊ≥ïÊîØÊíëÊï∞‰∏áÂπ∂ÂèëÂà∞Êï∞ÂçÅ‰∏áÔºåÁôæ‰∏áÂèäÊõ¥Â§öÁöÑÂπ∂ÂèëËØ∑Ê±Ç„ÄÇ ÊúçÂä°ÁöÑÈ´òÂèØÁî®ÊÄß„ÄÇÂêÑÁßçÊúçÂä°ÈÉΩÊòØÂ§öÂÆû‰æãÁöÑÈõÜÁæ§ÔºåÊüê‰∫õÊúçÂä°ÊïÖÈöúÂêéÔºåÈõÜÁæ§‰∏≠ÁöÑÂÖ∂‰ªñÂÆû‰æã‰ªçÁÑ∂ËÉΩÂ§üÊèê‰æõÊúçÂä°„ÄÇ ÊúçÂä°ÁöÑËá™Âä®ÂåñÊûÑÂª∫„ÄÇ‰ªé‰ª£Á†ÅÂà∞ÊúçÂä°ÈÉ®ÁΩ≤‰∏äÁ∫øÊòØ‰∏ÄÂ•óËá™Âä®ÂåñÁöÑÊµÅÁ®ãÔºåË∂äÂ∞ëÁöÑ‰∫∫Â∑•‰ªãÂÖ•‰øùËØÅ‰∫ÜÊúçÂä°ÁöÑÂèØÁî®ÊÄß„ÄÇ Á≥ªÁªüÁöÑÂÆûÊó∂ÁõëÊéß„ÄÇ7x24Â∞èÊó∂ÁöÑÁõëÊéß‰øùËØÅÊúçÂä°ÁöÑÂèØÁî®ÊÄßÔºåÂΩìÁõëÊéßÂà∞Êï∞ÊçÆÂºÇÂ∏∏ÊàñÊúçÂä°ÂÅúÊ≠¢ËøêË°åËÉΩÂèäÊó∂ÂëäË≠¶ÂºïÂÖ•‰∫∫Â∑•ËøêÁª¥Âõ¢Èòü„ÄÇ Êõ¥Â§öÁªÜËäÇËØ∑ÂèÇÈòÖ‰∏ãÈù¢ÁöÑslides,\nHow we build Videome from Meng Xin Zhu Ê¨¢ËøéÁïôË®Ä‰∏éÊàë‰ª¨Êé¢ËÆ®‰Ω†ÁöÑÂøÉÂæóÂíåÂª∫ËÆÆ„ÄÇ\n","link":"https://kane.mx/posts/2016/how-we-build-videome/","section":"posts","tags":["web-2.0","architecture","ÁΩëÁ´ôÊû∂ÊûÑ"],"title":"VÁßòÊòØÂ¶Ç‰ΩïÊûÑÂª∫ÁöÑ"},{"body":"","link":"https://kane.mx/tags/%E7%BD%91%E7%AB%99%E6%9E%B6%E6%9E%84/","section":"tags","tags":null,"title":"ÁΩëÁ´ôÊû∂ÊûÑ"},{"body":"","link":"https://kane.mx/tags/aliyun/","section":"tags","tags":null,"title":"Aliyun"},{"body":"","link":"https://kane.mx/tags/oss/","section":"tags","tags":null,"title":"Oss"},{"body":"","link":"https://kane.mx/tags/ossfs/","section":"tags","tags":null,"title":"Ossfs"},{"body":"ÈòøÈáå‰∫ëÊèê‰æõÁöÑÂØπË±°ÊàñËÄÖÊñá‰ª∂Â≠òÂÇ®Âè´OSSÔºå‰∏∫Â∫îÁî®Á®ãÂ∫èÊèê‰æõ‰∫ÜÊµ∑ÈáèÂ≠òÂÇ®ÔºåÊåâÈúÄ‰ªòË¥πÁ≠âÊúçÂä°„ÄÇÂ∫îÁî®Á®ãÂ∫èÂàôÈúÄË¶ÅÈÄöËøáAliyun OSSÁöÑÂêÑËØ≠Ë®ÄSDKÊâçËÉΩÊìç‰ΩúÔºàËØªÔºåÂÜôÔºåÈÅçÂéÜÁ≠âÔºâOSS‰∏≠ÁöÑÊñá‰ª∂„ÄÇ\nÂØπËøêÁª¥‰∫∫ÂëòÊù•ËØ¥ÔºåÂÅö‰∏Ä‰∫õÊï∞ÊçÆÁª¥Êä§Â∑•‰ΩúÁöÑÊó∂ÂÄôÔºåÈÄöËøáSDKÊìç‰ΩúOSS‰∏≠ÁöÑÊñá‰ª∂Â∞±‰ºöÊØîËæÉÈ∫ªÁÉ¶„ÄÇÂú®linux/unixÁéØÂ¢É‰∏ãÔºåÈÄöÂ∏∏Êúâ‰∏Ä‰∫õÂ∑•ÂÖ∑ÊääËøúÁ®ãÊñá‰ª∂Á≥ªÁªüÊàñ‰∫ëÁõòÊåÇËΩΩ‰∏∫Êú¨Âú∞Êñá‰ª∂„ÄÇÂú®ÁΩëÁªúÁä∂ÂÜµÊØîËæÉÂ•ΩÁöÑÊÉÖÂÜµ‰∏ãÔºåÊìç‰ΩúËøúÁ®ãÊñá‰ª∂Â∞±ÂÉèÊìç‰ΩúÊú¨Âú∞Êñá‰ª∂‰∏ÄÊ†∑„ÄÇ‰æãÂ¶ÇÔºåÊääAmazon S3ÔºåDropbox‰∫ëÁõòÔºåÂèØÈÄöËøásshÁôªÂΩïÁöÑËøúÁ®ãÊúçÂä°Âô®‰∏äÁöÑÁ£ÅÁõòÊåÇËΩΩ‰∏∫Êú¨Âú∞Êñá‰ª∂Á≥ªÁªü„ÄÇ\n‰πãÂâç‰πüÊúâÁ¨¨‰∏âÊñπÂÖ¨Âè∏ÂºÄÂèëÁöÑÂ∑•ÂÖ∑ÊääOSS bucketÊåÇËΩΩ‰∏∫Êú¨Âú∞Á£ÅÁõò„ÄÇÂá∫‰∫éÂÆâÂÖ®ËÄÉËôë‰∏ÄÁõ¥‰∏∫Êï¢‰ΩøÁî®„ÄÇ\nÁªà‰∫éÔºåÈòøÈáå‰∫ëÊé®Âá∫‰∫ÜÂÆòÊñπÂºÄÊ∫êÁâàÊú¨ÁöÑossfsÔºåÂπ∂‰∏îÊèê‰æõÊäÄÊúØÊîØÊåÅÔºàÈÄöËøáÂ∑•ÂçïÔºâ„ÄÇ\nÊé•‰∏ãÊù•ÔºåËÅäËÅäÊàëÁöÑ‰ΩøÁî®‰Ωì‰ºö„ÄÇ\nÂÆâË£ÖÔºåÈÖçÁΩÆÈÉΩËøòÁÆÄÂçï„ÄÇ ÊñáÊ°£ÁúãËµ∑Êù•ÊØîËæÉËØ¶ÁªÜÔºå‰ΩÜÂÆûÈôÖÊìç‰ΩúËµ∑Êù•Êúâ‰∫õÂ∞±‰∏çÂØπ„ÄÇÊÑüËßâÂÜôÊñáÊ°£ÁöÑ‰∫∫ÔºåÂπ∂Ê≤°ÊúâÂú®Áõ∏Â∫îÁéØÂ¢É‰∏äÊµãËØïËøá„ÄÇ ÊùÉÈôêËÆæËÆ°ÁöÑ‰∏ÄÂ°åÁ≥äÊ∂Ç„ÄÇossfsÂü∫‰∫éFUSEÔºåÁêÜÂΩìÂÖÅËÆ∏ÈùûrootÊåÇËΩΩÊàñÂç∏ËΩΩOSS bucket„ÄÇÈùûrootÁî®Êà∑‰ΩøÁî®ossfsÊåÇËΩΩÁöÑÊñá‰ª∂ÈªòËÆ§ÁöÑownerÈÉΩÊòØroot! ËøòÂ•ΩÁõÆÂâçÊúâworkaroundÔºåÊåÇËΩΩÁöÑÊó∂ÂÄôÊåáÂÆöÂèÇÊï∞Ôºå-ouid=your_uid -ogid=your_gidÊù•ÊåáÂÆöÊñá‰ª∂ÁöÑowner„ÄÇ ÊÄßËÉΩÊûÅÂÖ∂‰Ωé‰∏ãÔºÅÔºÅÔºÅ‰∏ÄÂè∞ECS‰∏ªÊú∫ÊåÇËΩΩ‰∫Ü‰∏Ä‰∏™‰ΩøÁî®ÂÜÖÁΩëÂú∞ÂùÄÁöÑoss bucketÔºåbucketÊ†π‰∏ãÈù¢Êúâ2k+Â≠êÁõÆÂΩïÔºàÂØπÊñá‰ª∂Á≥ªÁªüËÄåË®ÄÔºâ,bucketÂÜÖÊñá‰ª∂ÊÄªËÆ°Êúâ28G„ÄÇÁÑ∂ËÄåÊâßË°åls /tmp/\u0026lt;bucket mount point\u0026gt;Ë∂ÖËøá10ÂàÜÈíüÈÉΩÊó†Ê≥ïÂÆåÊàê„ÄÇËÄåÊàë‰ª¨VÁßò‰πãÂâçÁî®JavaÂÆûÁé∞ÁöÑAliyunOSSFSÊâßË°åÂêåÊ†∑ÁöÑÊìç‰ΩúÂè™ÈúÄË¶ÅÊï∞Áßí„ÄÇ ÈòøÈáå‰∫ëÁõ∏ÂÖ≥ÁöÑÊäÄÊúØÊîØÊåÅ‰∫∫ÂëòÂèäÂÖ∂‰∏ç‰∏ì‰∏ö„ÄÇÂæàÂ§öÊñá‰ª∂Á≥ªÁªüÔºåFUSEÁ≠âÊ¶ÇÂøµÈÉΩ‰∏çÁîö‰∫ÜËß£„ÄÇË∑ü‰ªñ‰ª¨Ê≤üÈÄöËøô‰∫õÊäÄÊúØÈóÆÈ¢òÔºåÈ¶ñÂÖàË¶ÅËä±Êó∂Èó¥ËøõË°åÊïôËÇ≤„ÄÇËä±Ë¥πÂ§ßÈáèÊó∂Èó¥Êù•Ê≤üÈÄöÔºåËøõÂ±ïÁ°ÆÁºìÊÖ¢„ÄÇ ÊÄª‰πãÔºåÈòøÈáå‰∫ëossfsËøô‰∏™Â∑•ÂÖ∑ËøúËøúÊ≤°ÊúâËææÂà∞production readyÁöÑË¥®Èáè„ÄÇÊó†Ê≥ï‰ΩøÁî®Âà∞Áîü‰∫ßÁéØÂ¢É‰∏≠„ÄÇ ","link":"https://kane.mx/posts/2016/aliyun-ossfs-sucks/","section":"posts","tags":["aliyun","oss","ossfs","ÈòøÈáå‰∫ë"],"title":"ËØ¥‰∏ÄËØ¥ÈòøÈáå‰∫ëossfs"},{"body":"","link":"https://kane.mx/tags/wechat/","section":"tags","tags":null,"title":"Wechat"},{"body":"","link":"https://kane.mx/tags/weixin/","section":"tags","tags":null,"title":"Weixin"},{"body":"","link":"https://kane.mx/tags/%E5%85%AC%E4%BC%97%E5%B9%B3%E5%8F%B0/","section":"tags","tags":null,"title":"ÂÖ¨‰ºóÂπ≥Âè∞"},{"body":"ÂæÆ‰ø°ÁªôÂÖ¨‰ºóÂπ≥Âè∞Êèê‰æõ‰∫ÜÁ¥†ÊùêÁÆ°ÁêÜÁöÑÊé•Âè£ÔºåÈÄöËøáËøô‰∏ÄÁ≥ªÂàóÊé•Âè£ÂèØ‰ª•‰∏ä‰º†ÔºåÊé•Êî∂‰ª•ÂèäÁÆ°ÁêÜÂõæÁâáÔºåËßÜÈ¢ëÁ≠âÂ§öÂ™í‰ΩìÊñá‰ª∂„ÄÇÂÖ∂‰∏≠ÂèàÂàÜ‰∏∫‰∏¥Êó∂ÂíåÊ∞∏‰πÖ‰∏§ÁßçÁ±ªÂûã„ÄÇÊ∞∏‰πÖÁ¥†ÊùêÊúâÊÄªÈáèÁöÑÈôêÂà∂Ôºå‰∏¥Êó∂Á¥†ÊùêÂæÆ‰ø°ÊúçÂä°Âô®Âè™Áªô‰øùÂ≠ò3Â§©„ÄÇ\nÊúÄËøëVÁßòÂàöÂ•ΩÊúâ‰∏™ÂêåÂæÆ‰ø°Áî®Êà∑‰∫íÂä®ÁöÑÂú∫ÊôØÔºå‰∏∫Áî®Êà∑ÁæéÂåñÂæÆ‰ø°ÊãçÊëÑÁöÑÂ∞èËßÜÈ¢ë„ÄÇVÁßòÂêéÂè∞ÊúçÂä°Âô®Êî∂Âà∞Áî®Êà∑ÂèëÈÄÅËøáÊù•Â∞èËßÜÈ¢ëÔºàÂæÆ‰ø°Â∞ÜÂÖ∂ËÆ§ÂÅö‰∏¥Êó∂Á¥†ÊùêÔºâÔºåÂ∞ÜÂÖ∂ÁæéÂåñÂ§ÑÁêÜÂêéÔºåÂÜçÂ∞ÜÁæéÂåñÁöÑËßÜÈ¢ë‰∏ä‰º†‰∏∫‰∏¥Êó∂Á¥†ÊùêÔºåÊúÄÁªàÁæéÂåñÂêéÁöÑËßÜÈ¢ë‰Ωú‰∏∫ËßÜÈ¢ëÁ±ªÂûãÁöÑÂÆ¢ÊúçÊ∂àÊÅØË¢´Êé®ÈÄÅÁªôÁî®Êà∑„ÄÇÊï¥‰∏™ÊµÅÁ®ãÂæàÁÆÄÊ¥ÅÔºåÁî®Êà∑ÂèëÈÄÅÂ∞èËßÜÈ¢ëÂêéÔºåÂ∞±ÂùêÁ≠âËßÇÁúãÁæéÂåñÂêéÁöÑÂ∞èËßÜÈ¢ë‰∫Ü„ÄÇ\nÁÑ∂ËÄåÊúÄÁªàÁªèËøáVÁßòÂºÄÂèëÂõ¢ÈòüÁöÑÂÆûË∑µÂèäÊµãËØïÔºåÂæóÂá∫ÁöÑÁªìËÆ∫ÊòØÔºå\n##ÂæÆ‰ø°ÂÖ¨‰ºóÂπ≥Âè∞ÁöÑ‰∏¥Êó∂Á¥†Êùê‰∏çËÉΩÁî®ÔºÅÁªùÂØπÁöÑÈ∏°ËÇãÔºÅ\nÂÖ¨‰ºóÂπ≥Âè∞‰∏ä‰º†Á¥†ÊùêÁöÑAPI‰ª•Âèä‰ΩøÁî®Â∑≤ÊúâÁ¥†ÊùêÂèëÈÄÅËßÜÈ¢ëÊ∂àÊÅØAPIÈÉΩÂæàÂÅ•Â£Æ„ÄÇ‰ΩÜÈóÆÈ¢òÂá∫Âú®‰∫ÜÂæÆ‰ø°ÂêéÂè∞ËµÑÊ∫êÁöÑÊúçÂä°‰∏äÈù¢„ÄÇ\nÂºÄÂèëËÄÖÊääÂõæÁâáËßÜÈ¢ëÊàêÂäü‰∏ä‰º†‰∏∫‰∏¥Êó∂Á¥†ÊùêÂêéÔºå‰ºö‰ªéÂæÆ‰ø°ÁöÑÊé•Âè£ÂæóÂà∞Ëøô‰∏™Á¥†ÊùêÁöÑID„ÄÇËøô‰∏™IDÈöèÂêé‰Ωú‰∏∫ÁªôÁî®Êà∑ÂèëÈÄÅÂõæÊñáÊ∂àÊÅØÊàñËßÜÈ¢ëÊ∂àÊÅØÁöÑËµÑÊ∫ê„ÄÇÂæÆ‰ø°ÂêéÂè∞‰ºöÊääËøô‰∏™IDÂØπÂ∫îÂà∞Á¥†ÊùêÁöÑÁúüÂÆûURLË∑ØÂæÑ‰∏ä„ÄÇËøô‰∏™ËøáÁ®ãÊòØÊ≤°ÊúâÈóÆÈ¢òÁöÑ„ÄÇÂêåÊó∂ÂæÆ‰ø°‰Ωú‰∏∫‰∏Ä‰∏™Êã•ÊúâÊµ∑ÈáèÁî®Êà∑ÁöÑËΩØ‰ª∂ÔºåÂÆÉ‰ºöÂ∞ÜËøô‰∫õÂ∞ÜË¶ÅÊé®ÈÄÅÁªôÁî®Êà∑ÁöÑÁ¥†ÊùêÈÉΩÂèëÂ∏ÉÂà∞ÂÆÉÁöÑCDN„ÄÇÁî®Êà∑Êî∂Âà∞ÁöÑÊúÄÁªàÂõæÁâáËßÜÈ¢ëÁöÑÂú∞ÂùÄÂ∞±ÊòØÁ¥†ÊùêÊñá‰ª∂Âú®ÂæÆ‰ø°/ËÖæËÆØCDN‰∏äÁöÑÂú∞ÂùÄ„ÄÇÂØπCDNÊúâ‰∫ÜËß£ÁöÑÊúãÂèãÈÉΩÁü•ÈÅìÔºåCDNÊúçÂä°Âô®ÂàÜÊï£Âú®ÂÖ®ÂõΩÊàñÂÖ®‰∏ñÁïåÂêÑÂú∞ÔºåÂΩìÁî®Êà∑ËØ∑Ê±ÇËøô‰∏™ËµÑÊ∫êÁöÑÊó∂ÂÄôÔºåËØ∑Ê±Ç‰ºöË¢´Ë∑ØÁî±Âà∞Á¶ªÁî®Êà∑ÊúÄËøëÁöÑCDNÊúçÂä°Âô®‰∏ä„ÄÇÂΩìCDNÊúçÂä°Âô®‰∏äËøòÊ≤°ÊúâÁºìÂ≠òËØ∑Ê±ÇÁöÑËµÑÊ∫êÊó∂ÔºåËøôÊó∂ÂÄôÊúâ‰∏™Ê∫ØÊ∫êÁöÑËøáÁ®ã„ÄÇÂ∞±ÊòØÂéüÂßãÊñá‰ª∂‰ªéÊñá‰ª∂ÊúçÂä°Âô®‰º†ÈÄÅÂà∞ËØ•CDNÊúçÂä°Âô®ÁöÑ‰∏Ä‰∏™ËøáÁ®ã„ÄÇËøôÊó∂ÔºåÁî®Êà∑Êúâ‰∏Ä‰∏™È¢ùÂ§ñÁöÑÁ≠âÂæÖÔºåÁ≠âÂæÖÊó∂Èó¥ÂèñÂÜ≥‰∫éÊñá‰ª∂Â§ßÂ∞èÂíåCDNÊúçÂä°Âô®ÂíåÊñá‰ª∂ÊúçÂä°Âô®Èó¥ÁöÑÂ∏¶ÂÆΩ„ÄÇ\nÂæÆ‰ø°Áî®Êù•ÁªôÂÖ¨‰ºóÂè∑ÊîæÁΩÆ‰∏¥Êó∂Á¥†ÊùêÁöÑCDNÂú®Ëøô‰∏ÄÂùóÂá∫‰∫ÜÈóÆÈ¢ò„ÄÇÂú®Êàë‰ª¨ÁöÑÊµãËØï‰∏≠ÔºåÂæÆ‰ø°CDNÂèØËÉΩ‰∏ÄÁõ¥Êó†Ê≥ïÊèê‰æõËøô‰∫õ‰∏¥Êó∂Á¥†ÊùêÔºàÊüê‰∫õÊñá‰ª∂Ë∂ÖËøá1Â§©Âêé‰ªçÁÑ∂Êó†Ê≥ïËÆøÈóÆÔºâ„ÄÇËÄå‰∏îÂá∫Áé∞ÈîôËØØÁöÑÂá†ÁéáÁõ∏ÂΩìÈ´òÔºåËá≥Â∞ë20%‰ª•‰∏ä„ÄÇÁî±‰∫éCDNÊó†Ê≥ï‰∏∫‰∏¥Êó∂Á¥†ÊùêÊèê‰æõÂèØÈù†ÁöÑËÆøÈóÆ‰øùÈöúÔºåÊâÄ‰ª•Êàë‰ª¨ÂæóÂá∫ÂæÆ‰ø°ÁªôÂÖ¨‰ºóÂè∑‰∏¥Êó∂Á¥†ÊùêËøô‰∏™ÂäüËÉΩÂü∫Êú¨Â∞±ÊòØ‰∏çËÉΩÁî®„ÄÇ\n","link":"https://kane.mx/posts/2016/weixin-temporary-materials/","section":"posts","tags":["weixin","wechat","ÂæÆ‰ø°","ÂÖ¨‰ºóÂπ≥Âè∞"],"title":"Â¶Ç‰Ωï‰ΩøÁî®ÂæÆ‰ø°ÂÖ¨‰ºóÂπ≥Âè∞ÁöÑ‰∏¥Êó∂Á¥†Êùê"},{"body":"","link":"https://kane.mx/tags/%E5%BE%AE%E4%BF%A1/","section":"tags","tags":null,"title":"ÂæÆ‰ø°"},{"body":"","link":"https://kane.mx/tags/pay/","section":"tags","tags":null,"title":"Pay"},{"body":"ÈöèÁùÄAngularJSÁ≠âÂâçÁ´ØMVCÊ°ÜÊû∂ÁöÑÊµÅË°åÔºåAJAXÁöÑÂºÇÊ≠•ËØ∑Ê±ÇÊï∞ÊçÆÁªìÂêàH5ÁöÑpush stateÁ≠âÁâπÊÄßÔºåÊûÅÂ§ßÁöÑÊîπÂñÑ‰∫ÜÁΩëÁ´ôÁöÑÁî®Êà∑‰ΩìÈ™åÂíåÈ°µÈù¢Âä†ËΩΩÊÄßËÉΩ„ÄÇËøôÁ±ªÁΩëÁ´ôÂ∫îÁî®ÈÄöÂ∏∏Âè™Êúâ‰∏Ä‰∏™ÂÖ•Âè£È°µÈù¢ÔºåÈÄöËøáÂ∫îÁî®ÂÜÖË∑ØÁî±Âà∞‰∏çÂêåÁöÑÈ°µÈù¢ÔºåÊâÄ‰ª•‰øóÁß∞ÂçïÈ°µÈù¢(signle page application)Â∫îÁî®„ÄÇÈ°µÈù¢URLÁúãËµ∑Êù•Â¶Ç‰∏ãÔºå\nÁΩëÁ´ôÈ¶ñÈ°µ http://mysite.com/#/index ÂïÜÂìÅÂàóË°®È°µ http://mysite.com/#/goods/list ÂïÜÂìÅËØ¶ÊÉÖÈ°µ http://mysite.com/#/goods/skuid ÁΩëÁ´ôÂÖ≥‰∫éÈ°µ http://mysite.com/#/about ÂØπÊµèËßàÂô®ËÄåË®ÄÔºå‰∏äÈù¢Âá†‰∏™Âú∞ÂùÄÈÉΩÊòØËÆøÈóÆÁöÑÁΩëÁ´ô**/ÁõÆÂΩïÔºåÊØè‰∏™url‰∏çÂêåÁöÑÊòØhashÈÉ®ÂàÜ„ÄÇËÄåAngularJS**Ê≠£ÊòØ‰æùËµñÈ°µÈù¢ÁöÑhashÊù•ÂÅöÁöÑÂ∫îÁî®ÂÜÖË∑ØÁî±ÔºåÊ†πÊçÆ‰∏çÂêåÁöÑË∑ØÁî±Êù•Âä†ËΩΩ‰∏çÂêåÁöÑjsÂíåhtmlÁâáÊÆµÔºåÂÆûÁé∞Âä®ÊÄÅÂÜÖÂÆπÁöÑÂä†ËΩΩ„ÄÇ\n‰∏ñ‰∏äÂπ∂Ê≤°ÊúâÁªùÂØπÂÆåÁæéÁöÑ‰∫ãÊÉÖÔºåÂçïÈ°µÈù¢Â∫îÁî®Âú®Áî®Êà∑‰ΩìÈ™åÂíåÊÄßËÉΩ‰∏äËé∑Âæó‰∫ÜÂ•ΩÂ§Ñ„ÄÇÁÑ∂ËÄåÔºåÂú®Âà´ÁöÑÂú∞ÊñπÂøÖÁÑ∂‰ªòÂá∫‰ª£‰ª∑„ÄÇËøôÈáåÂ∞±ÂàÜ‰∫´‰∏Ä‰∏ãÂçïÈ°µÈù¢Â∫îÁî®ÂíåÂæÆ‰ø°ÊîØ‰ªòÈõÜÊàêÁöÑ‰∏Ä‰∫õÁªèÈ™å„ÄÇ\nËøôÈáåÁöÑÂæÆ‰ø°ÊîØ‰ªòÊåáÁöÑÊòØÔºåÂú®ÂæÆ‰ø°ÊµèËßàÂô®‰∏≠ÈÄöËøáJSÊé•Âè£Ë∞ÉËµ∑ÂæÆ‰ø°ÊîØ‰ªòÊù•ÂÆåÊàêÁΩëÈ°µÂ∫îÁî®‰∏≠ÂïÜÂìÅÁöÑË¥≠‰π∞„ÄÇÂæÆ‰ø°ÊîØ‰ªòÊú¨Ë∫´ÁöÑÂºÄÂèëÈõÜÊàêÂπ∂‰∏çÂ§çÊùÇÔºåËøôÈáåÂ∞±‰∏çËµòËø∞‰∫Ü„ÄÇÂæÆ‰ø°ÊîØ‰ªòÂá∫‰∫éÂÆâÂÖ®ËÄÉËôëÔºåË¶ÅÊ±ÇÂÖ¨‰ºóÂè∑ÂøÖÈ°ªÊ≥®ÂÜåÊîØ‰ªòÂèëËµ∑È°µÈù¢ÁöÑÂú∞ÂùÄÔºàÂà∞ÊîØ‰ªòÈ°µÈù¢ÁöÑ‰∏äÁ∫ßÁõÆÂΩï‰∏∫Ê≠¢ÔºâÔºåÂπ∂‰∏îËÉΩÂ§üÊ∑ªÂä†Âà∞ÁôΩÂêçÂçïÁöÑÂú∞ÂùÄ‰∏çË∂ÖËøá3‰∏™„ÄÇ‰πüÂ∞±ÊòØÂ¶ÇÊûúÂ∫îÁî®Âú®ÂïÜÂìÅËØ¶ÊÉÖÈ°µÂèëËµ∑ÊîØ‰ªòËØ∑Ê±ÇÔºåÈÇ£‰πàÂú∞ÂùÄ**http://mysite.com/#/goods/**ÂøÖÈ°ªÂú®ÁôΩÂêçÂçïÂàóË°®„ÄÇ\nÁõÆÂâç‰∏∫Ê≠¢Ôºå‰∏ÄÂàáÈÉΩÂæàÂ•ΩÁêÜËß£ÔºåÊääÊîØ‰ªòÈ°µÈù¢Âä†Âà∞ÂæÆ‰ø°ÊîØ‰ªòÁôΩÂêçÂçï‰∏çÂ∞±‰∏á‰∫ãÂ§ßÂêâ‰∫Ü„ÄÇÂèØÁªèËøáÂÆûÊµãÔºå‰∫ãÂÆûÁ°Æ‰∏çÊòØËøô‰πàÁÆÄÂçïÔºÅ\nÂú®ÂæÆ‰ø°iOSÁâàÊú¨‰∏≠ÔºåÂæÆ‰ø°ÊîØ‰ªòJS‰ºöÈîôËØØÁöÑ‰ΩøÁî®landingÁΩëÁ´ôÈ°µÈù¢ÁöÑURLÔºåËÄå‰∏çÊòØÂèëËµ∑ÊîØ‰ªòÁöÑÈ°µÈù¢URLÔºÅÊØîÂ¶ÇÁî®Êà∑ÈÄöËøáÁΩëÁ´ôÈ¶ñÈ°µ**http://mysite.com/#/indexËøõÂÖ•Â∫îÁî®ÔºåÈÄöËøáÁ´ôÂÜÖÈìæÊé•ÊµèËßàÂà∞‰∫ÜÊüêÂïÜÂìÅËØ¶ÊÉÖÈ°µhttp://mysite.com/#/goods/skuidÂπ∂ÂèëËµ∑‰∫ÜÊîØ‰ªò„ÄÇ‰ΩÜÂæÆ‰ø°JS‰ºöÊäälandingÈ°µÈù¢URLhttp://mysite.com/#/index**Âà§ÂÆö‰∏∫ÊîØ‰ªòÁöÑÂèëËµ∑È°µÈù¢Ôºå‰ªéËÄåÂØºËá¥ÊîØ‰ªòJSË∞ÉÁî®Â§±Ë¥•ÔºÅ\nÂõ†‰∏∫Â∫îÁî®Â≠òÂú®Â§ö‰∏™È°µÈù¢Ôºå‰∏çÂèØËÉΩÊääÊâÄÊúâÁöÑÈ°µÈù¢ÈÉΩÂä†Âà∞ÊîØ‰ªòÁôΩÂêçÂçï‰∏≠(Êúâ3‰∏™Êï∞ÁõÆÈôêÂà∂ÔºåÂπ∂‰∏îÂ∑•‰ΩúÈáè‰πüÂ§ßÂà∞‰∏çÁé∞ÂÆû)„ÄÇË¶ÅËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÂè™Â•ΩÂè¶ËæüËπäÂæÑ„ÄÇÊàëÁõÆÂâçÊâæÂà∞ÁöÑÊñπÊ≥ïÊòØÔºåÂº∫Âà∂Âà∑Êñ∞È°µÈù¢ÂΩìÊâìÂºÄÂïÜÂìÅËØ¶ÊÉÖÈ°µÁöÑÊó∂ÂÄô„ÄÇÁ≠âÂêå‰∫éÁõ¥Êé•Âú®ÂæÆ‰ø°ÊµèËßàÂô®‰∏≠ÊâìÂºÄ‰∫ÜÂïÜÂìÅËØ¶ÊÉÖÈ°µ„ÄÇËôΩÁÑ∂ÂØπÁî®Êà∑‰ΩìÈ™åÊúâ‰∫õÂΩ±ÂìçÔºå‰ΩÜÊîØ‰ªòÂäüËÉΩÊ≠£Â∏∏Â∑•‰Ωú‰∫Ü„ÄÇ\n","link":"https://kane.mx/posts/2016/single-page-app-meets-weixin-pay/","section":"posts","tags":["weixin","wechat","pay","ÊîØ‰ªò"],"title":"ÂçïÈ°µÈù¢Â∫îÁî®(single page application)‰∏≠‰ΩøÁî®ÂæÆ‰ø°ÊîØ‰ªò"},{"body":"","link":"https://kane.mx/tags/%E6%94%AF%E4%BB%98/","section":"tags","tags":null,"title":"ÊîØ‰ªò"},{"body":"ÊúÄËøëÂú®Êèê‰∫§ÂâçÁ´Ø‰ª£Á†ÅÂêéÔºåÂâçÁ´Ø‰ª£Á†ÅÁöÑËá™Âä®ÂèëÂ∏ÉËÄÅÊòØÂ§±Ë¥•„ÄÇÂ§±Ë¥•ÁöÑÂéüÂõ†Â§öÊòØÁºñËØëDockerÈïúÂÉèÊó∂Âú®ÊâßË°åCOPYËØ≠Âè•Êã∑Ë¥ùÊñá‰ª∂Âà∞ÈïúÂÉèÊñá‰ª∂Á≥ªÁªüÊó∂ÔºåÊâîÂá∫‰∫Ü'No space left on device'Ëøô‰∏™ÈîôËØØ„ÄÇËøô‰∏™ÈîôËØØÊ†πÊçÆÊèèËø∞ÈùûÂ∏∏Â•ΩÁêÜËß£ÔºåÂ∞±ÊòØdockerÊñá‰ª∂Á≥ªÁªüÊâÄÂú®Á£ÅÁõòÊ≤°Êúâ‰∫ÜÁ©∫Èó¥„ÄÇ\n‰ΩÜÊòØÈÄöËøádf -hÂëΩ‰ª§ÔºåËØ•Á£ÅÁõòËá≥Â∞ëËøòÊúâ3Ôºå4‰∏™GÁöÑÂâ©‰ΩôÁ©∫Èó¥„ÄÇËÄåÂâçÁ´ØÈïúÂÉèÁöÑÊñá‰ª∂Â§ßÂ∞èÊúÄÂ§ö‰πü‰∏çË∂ÖËøá300M„ÄÇÂú®ËØ•Á£ÅÁõòÈÄöËøátouch,cp‰ªçÁÑ∂ÂèØ‰ª•ÂàõÂª∫Êñá‰ª∂„ÄÇ\nÊâÄ‰ª•Ëøô‰∏™ÈóÆÈ¢òÈùûÂ∏∏Â•áÊÄ™Ôºå‰∏∫‰ªÄ‰πàdockerÊàñËÄÖÊìç‰ΩúÁ≥ªÁªüÊä±ÊÄ®Á£ÅÁõòÊ≤°Êúâ‰∫ÜÁ©∫Èó¥ÔºüÂú®Á£ÅÁõò‰ªçÁÑ∂Ââ©‰ΩôÊï∞‰∏™GÁöÑÊÉÖÂÜµ‰∏ãÔºü\nÂÜçÈÄöËøáÁõ∏ÂÖ≥ÁöÑÊü•ÊâæÂêéÔºådockerÁöÑËøô‰∏™issueÁªô‰∫ÜÊàëÂêØÂèë„ÄÇLinuxÊñá‰ª∂Á≥ªÁªüÁöÑinodeÂú®ËÄóÂ∞ΩÂêéÔºåËØ•Êñá‰ª∂Á≥ªÁªüÂ∞Ü‰∏çËÉΩÂÜçÂàõÂª∫Êñ∞Êñá‰ª∂„ÄÇÂõ†‰∏∫ÂâçÁ´ØÈ°µÈù¢ÊòØÂü∫‰∫énodejsÁöÑÁ®ãÂ∫èÔºåÂÆÉ‰æùËµñÁöÑpackages‰∫ßÁîü‰∫ÜÂ§ßÈáèÊñá‰ª∂ÔºåÂú®ÂèçÂ§çÂà∂‰Ωú‰∏çÂêåÁöÑdocker imagesÊó∂ÔºåËøô‰∫õ‰æùËµñÊñá‰ª∂ÂèàË¢´ÂèçÂ§çÂ§çÂà∂ÔºåÂØºËá¥Êñá‰ª∂Êï∞ÈáèËøúËøúË∂ÖËøá‰∫ÜÈªòËÆ§inodeÂíåÁ£ÅÁõòÂ§ßÂ∞èÁöÑÊØî‰æãÔºåÊúÄÁªàinodeÂÖà‰∫éÁ£ÅÁõòÁ©∫Èó¥Ë¢´ÂÖ®ÈÉ®‰ΩøÁî®„ÄÇ\nÈÅáÂà∞Á±ª‰ººÈóÆÈ¢òÁöÑÂêåÂ≠¶ÔºåÂèØ‰ª•ÈÄöËøádf -iÊü•ÁúãinodeÁöÑ‰ΩøÁî®ÊÉÖÂÜµÊù•ÊéíÊü•ÈóÆÈ¢òÊòØÂê¶Áî±‰∫éinodeËÄóÂ∞ΩÂØºËá¥Ëøô‰∏™ÈîôËØØ„ÄÇ\n","link":"https://kane.mx/posts/2016/docker-build-no-space-left-caused-by-inode-exhausted/","section":"posts","tags":["docker","troubleshoot"],"title":"Êñá‰ª∂Á≥ªÁªüÁöÑInodeËÄóÂ∞ΩÔºå‰ºöÂØºËá¥DockerÁºñËØëÈïúÂÉèÂá∫Áé∞'No space left on device'ÈîôËØØ"},{"body":"","link":"https://kane.mx/tags/daemon/","section":"tags","tags":null,"title":"Daemon"},{"body":"Recently I wrote a Linux like initd script to start/stop my web application.\nThe script works well when running it in shell of linux. The web application will run in background by daemon.\nHowever I found both daemon and web application(java) exited immediately if I started the script in Jenkins as a shell step of build process.\nI put below simple script in 'Execute shell' block,\n1daemon --name=test-daemon -- sleep 200sleep 60 The process 'daemon' and 'sleep 200' should exit after 200 seconds the 'sleep' exits. The jenkins job will be finished in 60 secs.\njenkins 9954 9950 0 21:48 ? 00:00:00 sleep 60 jenkins 9955 1 0 21:48 ? 00:00:00 daemon ‚Äîname=test-daemon ‚Äî sleep 200 jenkins 9956 9955 0 21:48 ? 00:00:00 sleep 200 Above is the process info queried via ps command. The father pid of daemon is 1, not the script generated by Jenkins.\nBut both the process 'daemon' and 'sleep 200' immediately exited when the script finished. Should be something wrong in Jenkins to cause daemon exited unexpected.\nIt's something really frustrating to use daemon to stop/start the web application in Jenkins.\nFinally I used docker container to run my web application, which easily can be stopped/started via script in Jenkins.\n","link":"https://kane.mx/posts/archive/blogspot/daemon-hell-in-jenkins/","section":"posts","tags":["docker","daemon","jenkins","jenkins-cli"],"title":"Daemon hell in Jenkins"},{"body":"","link":"https://kane.mx/tags/jenkins-cli/","section":"tags","tags":null,"title":"Jenkins-Cli"},{"body":"","link":"https://kane.mx/tags/java/","section":"tags","tags":null,"title":"Java"},{"body":"","link":"https://kane.mx/tags/mac-osx/","section":"tags","tags":null,"title":"Mac OSX"},{"body":"After uninstalling some applications from my Mac OSX, I found the applications that depends on JRE totally does not work. I noticed below symptoms,\nEclipse Mars can not be launched, even though I specified the launching vm to another one(`java -version` still work). The SWT native library failed to resolve the dependencies to '/System/Library/Frameworks/JavaVM.framework/Versions/A/JavaVM' which does not exists. I tried to reinstall Oracle 1.8.0_u45 via both brew and dmg image downloaded from Oracle website, both ways were failed as well. The Mac pkg Installer can not be started due to dylib broken. It means I can't install any pkg via GUI. The command line(such as sudo installer -verboseR -target / -pkg /Volumes/OS\\ X\\ 10.10.4\\ Update\\ Combo/OSXUpdCombo10.10.4.pkg) still works. Finally I realized the problem was caused by I uninstalled the out of date Apple Java 6. Looks like all of above failures are required the system built-in Java. It really does not make sense the Oracle 1.8 installer script to depend on the out of date Java.\nFinally I reinstalled¬†Java for OS X 2014-001 to make everything working again. The GUI installer for pkg still does not work, you need use below command to use the pkg.\nsudo installer -verboseR -target / -pkg /Volumes/Java\\ for\\ OS\\ X\\ 2014-001/JavaForOSX.pkg\n","link":"https://kane.mx/posts/archive/blogspot/the-symptoms-of-java-broken-in-mac-osx/","section":"posts","tags":["Java","Mac OSX","troubleshoot"],"title":"The symptoms of Java broken in Mac OSX 10.10 and fix solution"},{"body":"","link":"https://kane.mx/tags/groovy/","section":"tags","tags":null,"title":"Groovy"},{"body":"Jenkins supports ssh authentication in CLI.\nBelow is a command to verify that I am authenticated:\n1 2java -jar jenkins-cli.jar -s¬†http://myserver/jenkins¬†who-am-i 3 4¬†Authenticated as: myuser 5¬†Authorities: 6¬†authenticated However you still would meet permission error when running groovy script in CLI.\n1 2java -jar jenkins-cli.jar -s¬†http://myserver/jenkins¬†groovysh \u0026#39;jenkins.model.Jenkins.instance.pluginManager.plugins.each { println(\u0026#34;${it.longName} - ${it.version}\u0026#34;) };\u0026#39; 3 4Exception in thread \u0026#34;main\u0026#34; java.lang.reflect.UndeclaredThrowableException 5at $Proxy2.main(Unknown Source) 6at hudson.cli.CLI.execute(CLI.java:271) 7at hudson.cli.CLI._main(CLI.java:417) 8at hudson.cli.CLI.main(CLI.java:322) It's a bug of Jenkins. The workaround is create a groovy script, then run that script via Jenkins CLI.\n1java -jar jenkins-cli.jar -s¬†http://myserver/jenkins/¬†groovy¬†test_script.gsh ","link":"https://kane.mx/posts/archive/blogspot/run-groovy-script-via-jenkins-cli/","section":"posts","tags":["groovy","jenkins","jenkins-cli"],"title":"Run groovy script via Jenkins CLI"},{"body":"","link":"https://kane.mx/tags/lucene/","section":"tags","tags":null,"title":"Lucene"},{"body":"","link":"https://kane.mx/tags/solr/","section":"tags","tags":null,"title":"Solr"},{"body":"The index has a field named 'create_time' that is the timestamp of document created time. The query string can boost the latest created document like below,\n{!boost b=recip(ms(NOW,create_time),3.16e-11,0.08,0.05)}name:keyword\nThere is another field named 'important' that indicates whether the document is important or not. The query string can boost the document is important like below,\nq={!boost b=$importfunc}name:keyword\u0026amp;importfunc=query({!v='important:true'})\nAbove query string uses a sub query in boost function.\nFinally I want to boost both above two fields, and 'important' field has higher priority. The query string looks like below,\ndefType=edismax\u0026amp;q=name:keyword\u0026amp;bf=query({!v='import:true'})^20.0 recip(ms(NOW,create_time),3.16e-11,0.08,0.05)^10.0\u0026quot;)\n","link":"https://kane.mx/posts/archive/blogspot/solr-boost-examples/","section":"posts","tags":["lucene","solr"],"title":"Solr boost examples"},{"body":"","link":"https://kane.mx/tags/django/","section":"tags","tags":null,"title":"Django"},{"body":"It's a common and ugly problem when using non-ascii characters in Django.\nThe general solution is below,\nput # -- coding: utf-8 -- at beginning of every python source files that are using utf-8 characters declare every string variable as unicode, such as str_var = u'‰∏≠ÊñáÂ≠óÁ¨¶' add a __unicode__ method in your model classes if you are running server on apache/mod_wsgi or ngnix, you need configure web server to use utf-8 encoding ","link":"https://kane.mx/posts/archive/blogspot/djangos-unicdoe-encode-error/","section":"posts","tags":["django","encoding","python"],"title":"Django's unicdoe encode error"},{"body":"","link":"https://kane.mx/tags/encoding/","section":"tags","tags":null,"title":"Encoding"},{"body":"","link":"https://kane.mx/tags/python/","section":"tags","tags":null,"title":"Python"},{"body":"","link":"https://kane.mx/tags/eclipse/","section":"tags","tags":null,"title":"Eclipse"},{"body":"","link":"https://kane.mx/tags/mountain-lion/","section":"tags","tags":null,"title":"Mountain Lion"},{"body":"","link":"https://kane.mx/tags/php/","section":"tags","tags":null,"title":"Php"},{"body":"I installed both Zend CE and zend debugger of Eclipse on my Mac. Both of them work well in Mac lion.¬†However they don't work any more after I upgraded my Mac to mountain lion.¬†After some investigation I found some extensions of Zend PHP can't be loaded due to shared library dependency can't be found in mountain lion. The xslt module of PHP depends on some system libraries(suc as /usr/local/libxslt-1.1.23/lib/libxslt.1.dylib) that have been removed by mountain lion.\nThe temporary solution is disabling xlst module of zend PHP if your application doesn't need them.¬†The workaround fix of Zend CE on Mac,¬†rename /usr/local/zend/lib/php_extensions/xsl.so to any other name\nThe workaround fix of zend debugger for Eclipse,¬†Delete the line extension=xsl.so from file /plugins/org.zend.php.debug.debugger.macosx_5.3.18.v20110322/resources/php53/php.ini\n","link":"https://kane.mx/posts/archive/blogspot/workaround-of-making-zend-ce-mountain-lion/","section":"posts","tags":["zend ce","zend debugger","workaround","Eclipse","php","mountain lion"],"title":"The workaround of making Zend CE/Zend debugger work on mountain lion"},{"body":"","link":"https://kane.mx/tags/workaround/","section":"tags","tags":null,"title":"Workaround"},{"body":"","link":"https://kane.mx/tags/zend-ce/","section":"tags","tags":null,"title":"Zend Ce"},{"body":"","link":"https://kane.mx/tags/zend-debugger/","section":"tags","tags":null,"title":"Zend Debugger"},{"body":"","link":"https://kane.mx/tags/dual-monitor/","section":"tags","tags":null,"title":"Dual Monitor"},{"body":"I had two monitors for my workstation. One is 22' and the another is 17'. I used the small one as a extend desktop.\nToday I get a another 23' monitor to replace the small one. However the resolution of the 23' monitor can't be changed after pluging it in. It always used the resolution matching the 17' one.\nBoth 'Setting - Display' and 'AMD Catalyst control' can't adjust it as higher resolution.\nAfter some tuning, I found a workaround.\nI totally remove all config of small one from /etc/X11/xorg.conf. Then change its resolution in 'AMD Catalyst control', it works!\n","link":"https://kane.mx/posts/archive/blogspot/dual-monitors-on-ubuntu/","section":"posts","tags":["Tip","Ubuntu","Dual monitor","Trick"],"title":"Dual monitors on Ubuntu"},{"body":"","link":"https://kane.mx/tags/trick/","section":"tags","tags":null,"title":"Trick"},{"body":"","link":"https://kane.mx/tags/ubuntu/","section":"tags","tags":null,"title":"Ubuntu"},{"body":"I want to create a test server for my application. Using embedding Http server in equinox is my first option.\nI had experience using simple http service implementation of equinox, however I want to play with Jetty this time.\nFollowing the guide of Equinox server, I can't running a Jetty server with my servlet in Eclipse Indigo. Obviously the guide is out of date.\nAfter tuning it, I found below bundles are minimum collection to run Jetty inside OSGi runtime.\nYou only need create a run configuration of OSGi framework, add your bundles with servlets and above bundles.\n","link":"https://kane.mx/posts/archive/blogspot/embedding-http-server-in-equinox/","section":"posts","tags":["Equinox","Jetty","OSGi"],"title":"Embedding an HTTP server in Equinox"},{"body":"","link":"https://kane.mx/tags/equinox/","section":"tags","tags":null,"title":"Equinox"},{"body":"","link":"https://kane.mx/tags/jetty/","section":"tags","tags":null,"title":"Jetty"},{"body":"","link":"https://kane.mx/tags/osgi/","section":"tags","tags":null,"title":"OSGi"},{"body":"Sometimes I need access the Intranet of company, however I don't like to create VPN connection. The connection is slow, waste time to create the connection and have to change password due to security policy.\nMy workstation is Linux, which has a lot of utility tools to help me access Intranet at home without VPN.\nFirstly I set up a ssh server on my personal computer. It's quite easy if you are using Linux, for Windows I installed Copssh.\nThen register a free domain name and configure it in my router. And let router forward port 22(or any port you wan to use) to my personal computer.\nIn my working Linux machine, create a ssh tunnel to my personal computer. Must use the public/private key for authenticating. For example,\nIt means remote server can access my workstation's port 22 via accessing its port 1002 after the ssh tunnel is created successfully. Above command line also forwards the ports 5900 and 6500. The default VNC session will listen the port 5900.\nBut it only works when my personal computer is running. And the connection can't be reconnected after it fails once.\nThe graceful solution is installing 'autossh' in my Linux, which is an utility to retry the ssh connection with an interval if it's disconnected or failed.\nThen create a script and running it when OS is booted. The script will be executed by root user, so we need configure it ran by the normal user.\nAfter my personal computer is booted a while(the default interval of autossh is 300 seconds), I can use localhost:10002 to login my workstation, localhost:5900 to access my VNC session. Of course you can use 'froxyproxy' of Firefox via a localport to access web page of Intranet.\n","link":"https://kane.mx/posts/archive/blogspot/acess-intranet-without-vpn/","section":"posts","tags":["ssh"],"title":"Acess Intranet without VPN"},{"body":"","link":"https://kane.mx/tags/ssh/","section":"tags","tags":null,"title":"Ssh"},{"body":"","link":"https://kane.mx/tags/configuration/","section":"tags","tags":null,"title":"Configuration"},{"body":"","link":"https://kane.mx/tags/gerrit/","section":"tags","tags":null,"title":"Gerrit"},{"body":"An internal Gerrit server was moved, so the hostname of server is changed. However we are using OpenID for user control, the OpenID provider(such as Google account) will generate a new token for the new server(hostname changing will impact the identity token of Google account) when we login Gerrit with same OpenID account. Gerrit will create a new internal account by default even though my OpenID account has existed in the system and has a lot of activities.\nThe solution is updating the 'ACCOUNT_EXTERNAL_IDS' table of Gerrit via gsql. Setting the 'ACCOUNT_ID' to your existing account_id for the new record whose 'EXTERNAL_ID' is the new token gotten from Google.\nupdate ACCOUNT_EXTERNAL_IDS set ACCOUNT_ID='1000001' where EXTERNAL_ID='https://www.google.com/accounts/o8/id?id=xxxxxxxxxx';\nThen search the documentation of Gerrit, I find a configuration property looks like supporting such a migration for OpenID authentication.\nauth.allowGoogleAccountUpgrade\nAllows Google Account users to automatically update their Gerrit account when/if their Google Account OpenID identity token changes. Identity tokens can change if the server changes hostnames, or for other reasons known only to Google. The upgrade path works by matching users by email address if the identity is not present, and then changing the identity.\nThis setting also permits old Gerrit 1.x users to seamlessly upgrade from Google Accounts on Google App Engine to OpenID authentication.\nHaving this enabled incurs an extra database query when Google Account users register with the Gerrit server.\nBy default, unset/false.\n","link":"https://kane.mx/posts/archive/blogspot/how-to-reuse-existing-openid-accounts/","section":"posts","tags":["OpenID","gerrit","configuration"],"title":"How to reuse the existing OpenID accounts after the host name of Gerrit server is changed"},{"body":"","link":"https://kane.mx/tags/openid/","section":"tags","tags":null,"title":"OpenID"},{"body":"","link":"https://kane.mx/tags/certificate/","section":"tags","tags":null,"title":"Certificate"},{"body":"The problem came from I tried to set up send mail server(SMTP) for my Gerrit server. My Gerrit server is using OpenID for user authorization, so I registered a new email account to send notification from Gerrit.\nMost of email service providers require the secure authorization when using its SMTP server to send mail. However the root CA of my email provider is not added into the default certificate of JRE. So Gerrit always failed to send email due to ssl verification exception.\nMy solution is adding the certificate of SMTP server into the certificate used by JRE.\nThe detail steps are below,\nUse open_ssl utility to the certificate of SMTP server or its root CA of email provider. Below command can list the certificate of SMTP and its chain. You can paste any of them into a file.\nopenssl s_client -connect smtp.163.com:465\nThen import the certificate saved in previous step into my JRE's key store. The default password of JRE's default keystore is 'changeit'. You can find the cacerts under jre/lib/security folder.\nsudo keytool -import -keystore cacerts -alias Smtp163com -file /tmp/smtp.163.PEM\n","link":"https://kane.mx/posts/archive/blogspot/jrejdks-certificate-issue-and-solution/","section":"posts","tags":["Java","gerrit","configuration","smtp","certificate"],"title":"JRE/JDK's certificate issue and solution"},{"body":"","link":"https://kane.mx/tags/smtp/","section":"tags","tags":null,"title":"Smtp"},{"body":"","link":"https://kane.mx/tags/build/","section":"tags","tags":null,"title":"Build"},{"body":"","link":"https://kane.mx/tags/maven/","section":"tags","tags":null,"title":"Maven"},{"body":"I successfully converted our product build from PDE build to Maven/Tycho. Something is worth to be documented here.\nThere are several examples and posts to demonstrate how using Tycho building your Eclipse plug-ins, features, applications and products. The most helpful example is the demo of Tycho project.\nBelow are some traps I met when building my project by Tycho,\nproduct build\nOur product is based on plug-ins, however we added the 'featurelist' in build.properties of PDE build to include some root binary for the product. However Tycho doesn't support this type of build, we create some features as the placeholder of plug-ins. Then change the product as features based. You have to manually remove the plugins tag in .product definition file, otherwise Tycho will fail on strange error if the .produce has both features and plugins tag. Then configure the director plugin as not installing features.\norg.eclipse.tycho\ntycho-p2-director-plugin\n${tycho-version}\nmaterialize-products\nmaterialize-products\nfalse\nmyappprofile\narchive-products\narchive-products\nAnd I used below way to customize the qualifier string of our build.\norg.eclipse.tycho tycho-packaging-plugin ${tycho-version} '${qualifier-prefix}_'yyyyMMddHHmm An limitation of director plugin is that no way using different profile name for the application installed on different hosts. I contributed a patch on bug 362550 for this enhancement.\nfeature build\nWe have some features to pack some binary files as root files. But Tycho doesn't support root folder that is recognized by PDE build. The workaround is creating an additional folder, then put the root files into it.\nMeanwhile Tycho doesn't support wildcard to other native touch points, such as changing the files permission. For static file list use comma separated list as workaround.\neclipse test plug-in\nI have a plug-in whose scope is 'test', but it doesn't have test case and no dependency for any test framework, such as junit 3.8 or junit 4. And it's used for mocking test server. Configure surefire plugin to let it build as test plug-in as well.\norg.eclipse.tycho\ntycho-surefire-plugin\n${tycho-version}\njunit\njunit\n4.1\nfalse junit\njunit\n4.1\nAnd configure the surefire plugin like below to test code in Maven build.\norg.eclipse.tycho\ntycho-surefire-plugin\n${tycho-version}\nmy.group\nmy.feature\n${version}\neclipse-feature\nmy.group\nmy.testserver\n1.0.0\neclipse-plugin\n${testSuiteName}\n${testClassName} -Dcom.sun.management.jmxremote\n-consoleLog\norg.eclipse.equinox.ds\n1\ntrue\nsign jars\nAdd below signjar plugin into parent pom.xml, however I met the md5 error when materializing the repository built on .product. There is a workaround mentioned on Bug 344691.\norg.apache.maven.plugins maven-jarsigner-plugin 1.2 ${keystore} MyCompany ${storepass} ${keypass} true ${skip.jar.signing} -tsa https://timestamp.geotrust.com/tsa **/artifacts.jar **/content.jar jar eclipse-plugin eclipse-feature eclipse-test-plugin sign sign verify verify ","link":"https://kane.mx/posts/archive/blogspot/tips-of-maventycho-building/","section":"posts","tags":["Maven","Eclipse","build","Tycho"],"title":"The tips of Maven/Tycho building crossplatform RCP and repository"},{"body":"","link":"https://kane.mx/tags/tycho/","section":"tags","tags":null,"title":"Tycho"},{"body":"","link":"https://kane.mx/tags/clearcase/","section":"tags","tags":null,"title":"Clearcase"},{"body":"Several days ago I had a post to record the unsuccessful experience to migrate source code from Clearcase to Git.\nWe have a new way after doing some brain storms. This way still is not a perfect solution, but it's much better than previous trial.\nUse clearexport_ccase to export the source folder to intermittent data. See documentation of Clearcase admin. Create a temporary vob for importing the data later. See example. Import the data into temporary vob. See example. Repeat step 1 to 3 for importing all necessary data into temporary vob. Use the SVN Importer to import the temporary vob as Subversion repository. Last steps refer to a documentation of succeeded migration case of one of Eclipse project from Subversion to Git. Git definitely is greatest SCM tool now. The size of Subversion repository is around 10GB, finally the Git repository is less than 700MB, which saves more than 10 times disk space. It's awesome!\nThe flaw of this way is that the removed elements in Clearcase(said using Main/LATEST as cspec of Clearcase vob when exporting) would lose after importing into a temporary vob. So switching to a maintenance branch or tag like 1.0/2.0 in Git, the source code is incomplete. The files existed in that branch or tag, then removed in latest code base are lost. The workaround could be manually checking in GA version to have complete code.\nIf anybody have graceful and perfect solution to migrate Clearcase to Git, I think he could start a new business. :)\n","link":"https://kane.mx/posts/archive/blogspot/migration-clearcase-to-git-part-2/","section":"posts","tags":["Git","Clearcase"],"title":"Migration Clearcase to Git -- part 2"},{"body":"I tried to migrate the source code of project from Clearcase to Git repository. As far as I know there is no elegant solution for such migration. For purpose of this migration, I want to keep the history and label of files in Clearcase after migrating to Git repository.\nThere are mature tools to migrate CVS/SVN repository to Git, so I tried to use Subversion as a bridge for my migration.\nI used a free software 'SVN Importer' to import the Clearcase vobs to Subversion. The tool is great, and it keeps the history of files, labels and branches. The entire size of new Subversion repository has near 50GB which is unacceptable size of Git repository. The subversion repository contains a lot of legacy code and unwanted binaries, so removing those revisions could significantly reduce the size of subversion repository. And subversion provides some admin tools to manipulate the metadata of subversion, it's possible to remove the unnecessary revisions and re-create a subversion repository with refined content. But I don't have any experience to use the admin tool of subversion before, I failed to filter the unwanted data. It's not worthy of costing too much effects on it. Finally I failed to filter the subversion repository.\nActually the detail history of files is rarely used. If need, we still can find it in Clearcase. At last I manually checked in the released version of our project into Git repository, and tagged them.\nWrote this unsuccessful idea here for elapsed efforts.\n","link":"https://kane.mx/posts/archive/blogspot/migrate-clearcase-to-git/","section":"posts","tags":["Git","Clearcase"],"title":"Migrate Clearcase to Git"},{"body":"","link":"https://kane.mx/tags/p2/","section":"tags","tags":null,"title":"P2"},{"body":"Our p2 based on installer suffered performance issue when querying IUs from repositories. Though the repositories have a large number of IUs to be queried, but we find the performance of using QL is unacceptable in some special scenarios.\nI published several different methods to find the expected IUs. Thomas pointed out the better expression of QL and finally helped us to find out the our repository without IIndexProvider implementation.\nIIndexProvider implementation of a repository is quite important to improve the performance of QL, especially use the 'traverse' clause to query something.\nAnd Slicer API is an alternative method when querying the complete dependencies.\n","link":"https://kane.mx/posts/archive/blogspot/p2-query-performance/","section":"posts","tags":["p2","performance"],"title":"p2 query performance"},{"body":"","link":"https://kane.mx/tags/performance/","section":"tags","tags":null,"title":"Performance"},{"body":"","link":"https://kane.mx/tags/compile/","section":"tags","tags":null,"title":"Compile"},{"body":"Yesterday I modified an existing c++ application for Windows. And its default build environment is Makefile and MinGW.\nHowever I used a newly Windows API that is not included by header files of MinGW.\nFirst of all, I copied the constant definition from header file of Windows SDK, and defined the Windows API method as a extern C method. So it's no problem to compile the code in MinGW.\nSecondly I have to fix the link issue. Because the symbol of the Windows API also can't be found by gcc link.\nHere great thanks to Google. It's quite easy to get the knowledge from others.\nI found a way to create an library by using dlltool. Dlltool is a utility to create an library with specified methods from existing dll library, which can be used by gcc link later.\nBelow are links I referred to create an import library,\n[1] http://www.emmestech.com/moron_guides/moron1.html\n[2] http://www.mingw.org/wiki/CreateImportLibraries\n[3] http://lists-archives.org/mingw-users/19461-import-library-for-c.html\n","link":"https://kane.mx/posts/archive/blogspot/create-import-library-for-building/","section":"posts","tags":["compile","MinGW"],"title":"Create an import library for building application in MinGW"},{"body":"","link":"https://kane.mx/tags/mingw/","section":"tags","tags":null,"title":"MinGW"},{"body":"The documentation of PDE has a chapter for this topic. Basically it's simply. Copy the template scripts what you want from templates/headless-build folder under org.eclipse.pde.build plug-in to your build configuration directory that is the folder has build.properties file.\nHowever I found the variables listed in template 'customAssembly.xml' can't be used in the runtime. I filed bug 346370 against it.\n","link":"https://kane.mx/posts/archive/blogspot/customize-pde-build/","section":"posts","tags":["PDE","Eclipse","build"],"title":"Customize PDE build"},{"body":"","link":"https://kane.mx/tags/pde/","section":"tags","tags":null,"title":"PDE"},{"body":"","link":"https://kane.mx/tags/code-signing/","section":"tags","tags":null,"title":"Code Signing"},{"body":"I did sign the jars via reusing the existing certificate of Windows code signing several months ago. Writing it down for further reference.\nWhatever your purpose of reusing the existing Windows code certificate, I only document the way from technical perspective.\nAfter buying the certificate of Windows code signing from CA, you will get a .pvk file that stores both the certificate and private key. PVK file is the PKCS12 format[1], however java uses JKS format by default. So you need convert the pvk file to JKS keystore and certificate.\nSince 6.0 JDK supports PKCS12 directly, you can use 'jarsigner' and PVK file to sign jars directly[2].\n1jarsigner -keystore /working/mystore.pvk -storetype pkcs12 -storepass myspass -keypass j638klm -signedjar sbundle.jar bundle.jar jane Or using keytool to convert the PKCS#12 to JKS format[3] if using Eclipse PDE build to sign your jars.\n1keytool -importkeystore -srckeystore KEYSTORE.pvk -destkeystore KEYSTORE.jks¬†-srcstoretype PKCS12 -deststoretype JKS -srcstorepass mysecret -deststorepass mysecret -srcalias myalias -destalias myalias -srckeypass mykeypass -destkeypass mykeypass -noprompt¬†[1] http://en.wikipedia.org/wiki/PKCS\n[2] http://download.oracle.com/javase/6/docs/technotes/tools/solaris/jarsigner.html\n[3] http://shib.kuleuven.be/docs/ssl_commands.shtml#keytool\n","link":"https://kane.mx/posts/archive/blogspot/using-certificate-of-windows-code/","section":"posts","tags":["code signing","java","certificate"],"title":"Using the certificate of Windows code signing to sign jars"},{"body":"I met that firefox/thunderbird complained another its instance running even if no a running firefox/thunderbird process. Finally let them run again after removing the '.parentlock' file in their default profile.\nstrace utility helps me a lot to find the solution.\nstrace -f -e file firfox\n","link":"https://kane.mx/posts/archive/blogspot/unlock-locked-profile-if/","section":"posts","tags":null,"title":"Unlock the locked profile if firefox/thunderbird crash"},{"body":"I implemented the replication tool at the end of 2009, then published it to Eclipse Marketplace in May 2010. However it's not pervasively used due to users have to install that plug-in firstly.\nI searched a similar request on bugzilla, then I initialized my contribution in the early of this year. Finally it was accepted and will release as part of eclipse itself since Eclipse 3.7 M7! I hope it would benefit the users of Eclipse more and more.\nAnd I was nominated and elected as the committer of Equinox p2, it's a great honor for me. :)\n","link":"https://kane.mx/posts/archive/blogspot/eclipse-p2s-importexport-capability/","section":"posts","tags":["Equinox","p2","Eclipse","feature"],"title":"Eclipse P2's import/export capability"},{"body":"","link":"https://kane.mx/tags/feature/","section":"tags","tags":null,"title":"Feature"},{"body":":g!/some expression/d\n","link":"https://kane.mx/posts/archive/blogspot/vim-delete-lines-not-contain-words/","section":"posts","tags":null,"title":"[vim] delete the lines not contain words"},{"body":"Recently our installer met a strange bug, it didn't uninstall all legacy bundles after updating to new version. Finally I found it's due to a magic fragment is missing in the profile due to some causes.\ninstallBundle(bundle:${artifact})\nuninstallBundle(bundle:${artifact})\nsetStartLevel(startLevel:4);\nIt has 'hostRequirements' element that represents it's a fragment IU and match all the eclipse's plug-ins in that profile. And this fragment defines the touch point actions for its hosts that will do installBundle action during 'install' phrase and uninstallBundle action during 'uninstall' phrase. It's a very good way to remove the duplicate touch point definitions for all eclipse's plug-ins in the profile.\nBTW, p2's engine also doesn't attach this fragment to the eclipse's plug-in IU if the top level IU doesn't have the STRICT rule. I'm not sure the root cause of designing for it, but it's the fact.\n","link":"https://kane.mx/posts/archive/blogspot/inside-p2s-profile-2-fragment-matches/","section":"posts","tags":["p2","Eclipse","profile"],"title":"Inside P2's profile (2) - the fragment matches all osgi bundles"},{"body":"","link":"https://kane.mx/tags/profile/","section":"tags","tags":null,"title":"Profile"},{"body":"You would see some interesting properties at the bottom of eclipse's profile.\nFor example,\nIt attaches a property named 'org.eclipse.equinox.p2.internal.inclusion.rules' with value 'STRICT' on the IU 'org.eclipse.sdk.ide' with version 3.6.1.M20100909-0800.\nIt's a very important property for the p2 engine. It means the IU 'org.eclipse.sdk.ide' has been explicitly installed into the profile, so it's not allowed be implicitly updated or removed.\nFor example,\nWe have top feature IU 'org.eclipse.sdk.ide' that represents the Eclipse SDK,¬†'org.eclipse.pde.feature' that represents the Plug-in Development Tool and 'org.eclipse.jdt.feature' that represents the Java Development Tool. And both JDT and PDT are part of Eclipse SDK, so 'org.eclipse.pde.feature' and 'org.eclipse.jdt.feature' are required by 'org.eclipse.sdk.ide'.\nIf the profile only has the STRICT rule for 'org.eclipse.sdk.ide', 'org.eclipse.jdt.feature' and 'org.eclipse.pdt.feature' will implicitly be updated to 3.6.2 when updating 'org.eclipse.sdk.ide' from 3.6.1 to 3.6.2.\nHowever the profile has below STRICT rule for PDT feature,\nThe p2 engine will report errors due to 'org.eclipse.pdt.feature' has STRICT rule for updating. Hence third-party must explicitly update both 'org.eclipse.sdk.ide' and 'org.eclipse.pdt.feature' from 3.6.1 to 3.6.2.\n","link":"https://kane.mx/posts/archive/blogspot/inside-p2s-profile-1-inclusion-rules/","section":"posts","tags":["p2","Eclipse","profile"],"title":"Inside P2's profile (1) - inclusion rules"},{"body":"Latest gcc compiler enables the stack overflow protector that is since GLIBC 2.4. So the library or executable is compiled by latest gcc could be loaded or executed in RHEL4 or Solaris 9 that only have GLIBC 2.3. Hence using option '-fno-stack-protector' to compile the library or executable to make sure it could be executed in older linux release.\ng++ -fno-stack-protector -o test.o test\n","link":"https://kane.mx/posts/archive/blogspot/stack-overflow-protector/","section":"posts","tags":null,"title":"stack overflow protector"},{"body":"Recently I just know such a useful syntax usage of java.\naLoopName: for (;;) { // ...¬†while (someCondition) // ...¬†if (otherCondition) continue aLoopName; ","link":"https://kane.mx/posts/archive/blogspot/loop-name-for-for-clause-in-java/","section":"posts","tags":null,"title":"the loop name for 'for' clause in java"},{"body":"It's a powerful command to rename files in a batch.\nUsage:\nrename 's/(\\d+)$/$1\\.txt/' * rename add '.txt' extension name for all files that ends with number.\n","link":"https://kane.mx/posts/archive/blogspot/rename-command/","section":"posts","tags":null,"title":"rename command"},{"body":"If you have http proxy, set it to system environment,\nexport http_proxy=http://127.0.0.1:8000 Then start the application in that same terminal.\nIf the proxy is socks proxy, use 'tsocks' to wrap the application in terminal.\n","link":"https://kane.mx/posts/archive/blogspot/applying-proxy-for-softwares-without/","section":"posts","tags":null,"title":"applying proxy for the softwares without proxy support in linux"},{"body":"Honestly speaking, you have eaten the best delicious food if you're living in China. Though we have more and more concerns on the safety of food, we have to recognize that Chinese food is more delicious than others.\nThe cuisine is simple in Austria. People always use pork, beef, flour, tomato, potato and few green vegetables. So they surprised Chinese cost several hours to make the food.\nGulasch, it's good tasted after eating pizza several times\npasta\nAbout the drinking, most of them directly drink the water from water pipe. And some of them like the special water that mixes water with gas. The coffee and beer are the favorite of local citizens. You can find more than one hundred beer brand in the city, and some of them have been found for centuries. Indeed they're good tasted.\nspecial water\nDie Weissf\nWieninger, it comes from Vienna\nStiegl, local famous brand\n","link":"https://kane.mx/posts/archive/blogspot/food-and-drinking/","section":"posts","tags":["salzburg","travel","tour"],"title":"Food and Drinking"},{"body":"","link":"https://kane.mx/tags/salzburg/","section":"tags","tags":null,"title":"Salzburg"},{"body":"","link":"https://kane.mx/tags/tour/","section":"tags","tags":null,"title":"Tour"},{"body":"","link":"https://kane.mx/tags/travel/","section":"tags","tags":null,"title":"Travel"},{"body":"","link":"https://kane.mx/categories/trip/","section":"categories","tags":null,"title":"Trip"},{"body":"Joel posted a blog related to how to hire the great programmers. One of his key points is building comfortable workspaces.\nI believe every programmer loves the workspace like Google and Fog Creek. The workspace of Google has been very famous due to its French chef, gymnasium and big sofas. Why is Fog Creek? It's the company created by Joel, he also practiced his theory on his company. Ruan YiFeng posted a blog for it. I bet you would envy the guys working in that office.\nHow about the workspace of the office in Salzburg? Let me show some pictures.\nSpace\nprogrammers have two monitors\nReading\nnon-technique magazines\ntechnique books\nDrinking\ncoffee machine\nKitchen\nfreezer for fast food\nEntertainment\ntable football game\n","link":"https://kane.mx/posts/archive/blogspot/working-workspace/","section":"posts","tags":["salzburg","travel","tour"],"title":"Working Workspace"},{"body":"Salzburg is a small city and is on the banks of the Salzach River. It's easy to go through the city by bus in 30 minutes.\nSalzach River\noutline view\nRiding the bicycle is a very good way to enjoy the beautiful sight of the city. You could see many kids with parents riding bicycle in the sunny weekend.\nThe train station and a major bus transient station are the same one that is called as 'main station' by local residents. it's not far from the office of company, about 20 minutes by foot.\nThe ticket system of bus is more complicated than Beijing's. People can buy the ticket for single, 24 hours, 48 hours, a week and even a year. The children can get discount. There's no ticket seller in the bus. Usually nobody checks whether you have valid ticket. Pressing the button to open the door when getting on/off the bus.\nThe public transportation is designed well. There are different tickets for different people. For example, tourists would prefer to buy 24 hours ticket or 48 hours tickets. 24 hours ticket means the passengers can take any bus in the 24 hours after it's used first time. So it's very convenient for tourists. 24 hours ticket is 4.2‚Ç¨ for adults, 2.1‚Ç¨ for children.¬†The price of train tickets is same. The faster train \u0026quot;ICE\u0026quot; have higher price. The regular train is much cheaper. And the ticket allows a family not exceed 5 persons to go back and forth another city in a day. It's very cheap for a family to enjoy weekend in another city or town by train. I think it's a good approach to use public transportation more to reduce environment pollution.\nround-trip ticket\nTaking the train is more convenient than China. There is no security checking, no long distance between gate and platform and even no staffs in the platform. Meanwhile there is no any limitation to travel among the European countries. I went to Munich of Germany by train, I felt it's even more convenient than taking subway in Beijing. Both the train and bus have a lot of humanization design for disability people and people with their bicycles or pets.\n","link":"https://kane.mx/posts/archive/blogspot/transportation/","section":"posts","tags":["salzburg","travel","tour"],"title":"Transportation"},{"body":"There are 20+ staffs in Salzburg office. Most of them are developers, one is administrator of office.\nGenerally the staffs in Salzburg work more flexible than the staffs working in Beijing.\nSome of them live in German. Even though it's not so far as Salzburg, they also need come to Salzburg by train. So sometimes they work at home, use internet and phone as communication tool.\nAnd they have different responsibilities for products. For example, Helmut works on installer, Matthias and Michael are responsibility for QFT testing, Martin N. is focus on license API developing. So everybody has himself schedule, he can decide when he come to office and when leave office based on his working schedule. Nobody cares when you come/leave office or how long you work every day. I believe all of them do well on their jobs.\nFurthermore you can work with your dogs together if nobody takes care of them at home.\nMax's dogs\nMost of foreign like coffee, so there is a kitchen with seats in the office. Some of them like to drink a cup of coffee or tea as a break, and it's a good chance to talk with others. It's a relaxing time for changing your mind out of work. Besides drinking some things, there is a room for playing table football game. It's a small amount of exercise, it's good for body.\nAnother thing makes me very impressive is that the team is very stable than any company I know in China. Most of them work in company more than 10 years. So I think I know that's why they know more than us. Everyone could be expert after doing the same thing more than 10 years. They love the work of coding, and they would like to do coding until retiring. That's why I can see some of them are more than 40, even 50 years old.\n","link":"https://kane.mx/posts/archive/blogspot/working-style/","section":"posts","tags":null,"title":"Working style"},{"body":"Â∫¶ËøáÈïøÈÄîÈ£ûË°åÁöÑÊóÖÁ®ã‰∏çÊòØ‰∏Ä‰ª∂ÂÆπÊòìÁöÑ‰∫ã„ÄÇË¶ÅÂú®Áã≠Â∞èÁöÑÁ©∫Èó¥ÈáåÂæÖ‰∏äËøë10‰∏™Â∞èÊó∂ÔºåÂ•ΩÂú®ÊòØ‰∏§‰∫∫Âá∫Ë°åÔºåÂçäÁù°ÂçäËÅäÁöÑÊâìÂèëËøá‰∫ÜÊó∂Èó¥„ÄÇ\nÂà∞ËææÁª¥‰πüÁ∫≥‰πãÂêéÔºåÂá∫‰∫ÜÁôªÊú∫ÈÄöÈÅìÁúãÂà∞ÁöÑÂ±ÖÁÑ∂ÊòØ‰∏Ä‰∏™ÈÖíÂêßÁ±ªÁöÑÈ§êÈ¶Ü„ÄÇÊÑüËßâÂæàÁ®ÄÂ•áÔºå‰πüÂæàÊúâÂë≥ÈÅì„ÄÇ\nÂë®Âõ¥ÂΩìÁÑ∂Â∞ë‰∏ç‰∫ÜÂÖçÁ®éÂ∫óÂíåÂïÜÈì∫Ôºå‰ΩÜËøúÊ≤°ÊúâÈ¶ñÈÉΩÊú∫Âú∫ÈÇ£Ê†∑ÁöÑËßÑÊ®°„ÄÇÊÄª‰ΩìÊÑüËßâÂ∞±ÂÉèÂõΩÂÜÖÂ§ßÂûãË∂ÖÂ∏ÇË¥≠Áâ©Âá∫Âè£‰∏ÄÊ†∑ÔºåËÄå‰∏î‰∫∫‰πü‰∏çÂ§ö„ÄÇ\nÂè¶Â§ñËµû‰∏Ä‰∏ãÁª¥‰πüÁ∫≥Êú∫Âú∫ÁöÑWifiÔºåÁÆÄÂçïÈÖçÁΩÆÂ∞±K‰∫ÜÔºåÂì™ÂÉèÂõΩÂÜÖÁöÑÔºåÂèàË¶ÅÁßªÂä®Âè∑Á†ÅÔºåËøòË¶ÅÁü≠‰ø°Ëé∑ÂèñÔºåÊêûÂçäÂ§©‰πüÊ≤°ÂºÑÂÆö„ÄÇ\nËΩ¨Êú∫‰πãÂâçËøòÊúâÊÆµÊó∂Èó¥ÔºåÂ∞±Âú®Êú∫Âú∫ÈáåÂà∞Â§ÑÈÄõ‰∫ÜÈÄõ„ÄÇÈ°∫‰æøÂú®‰∏Ä‰∏™ÂêßËß£ÂÜ≥‰∫ÜÊôöÈ§êÔºåÂêåÊó∂‰πüÂ∞ù‰∫ÜÊùØÂΩìÂú∞Âï§ÈÖí„ÄÇ\nÂéªËê®Â∞îËå®Â†°ÁöÑÈ£ûÊú∫ËøòÊòØÂ∏¶Ëû∫ÊóãÊ°®ÁöÑÔºåÂ§¥Ê¨°ÂùêËøôÊ†∑ÁöÑËÄÅÂºèÈ£ûÊú∫„ÄÇ\n","link":"https://kane.mx/posts/archive/blogspot/day-1/","section":"posts","tags":null,"title":"day 1"},{"body":"Check out this SlideShare Presentation:\nDiscovering the p2 API\nView more presentations from Sonatype.\n","link":"https://kane.mx/posts/archive/blogspot/discovering-p2-api/","section":"posts","tags":null,"title":"Discovering the p2 API"},{"body":"Days ago I updated my p2 replication tool. It's easier to install it in your Eclipse.\nA new component named 'Eclipse marketplace' is added into Eclipse SDK since Helios, which is an application store for Eclipse. People could be easy to install third party plug-ins into their Eclipse.\nYou can launch marketplace via 'Help' - 'Eclipse Marketplace...', then search key word 'p2' or 'replication' to find the tool. Finally click next to install it.\nIt's a very graceful workflow to install some add-ons like firefox.\nAnd then p2 replication tool could help you replicate your environment. This tool supports install components from another existing Eclipse instance to save the time cost on downloading them from Internet now! Enjoy it.\n","link":"https://kane.mx/posts/archive/blogspot/p2-replication-tool-lives-on-eclipse/","section":"posts","tags":null,"title":"P2 replication tool lives on Eclipse Marketplace"},{"body":"1. tcpdump\ntcpdump -n port 80 -i eth0|lo\nmonitor all package transferred on 80 port on the network interface eth0/lo\n2. netstat\nnetstat -anp|grep java\ntrace all network traffic on the process named java\nnetstat -anp|grep 128.224.159.xxx\ntrace all network traffic on the host whose ip address is 128.224.159.xxx\n3. nslookup\nnslookup 206.191.52.46\nlook up the domain name whose ip address is 206.191.52.46\n","link":"https://kane.mx/posts/archive/blogspot/useful-network-utility-tools/","section":"posts","tags":null,"title":"useful network utility tools"},{"body":"Using -exec command like below, need add escape character for semicolon that separated two commands in shell\n1find directory/ -type d -exec chmod a+x {} \\\\; Feb 24, 2010 - update:\n1find . -maxdepth 4 -type d¬†-name \u0026#39;g-vxworks\u0026#39; 2\u0026gt;/dev/null -print Jan. 7, 2024 - update: You might see No such file or directory when combining --exec rm to delete the found files.\nYou can add -depth option to mitigate the message.\n1find deployment/g -depth -name \u0026#39;asset.*\u0026#39; -type d -exec rm -rf {} \\; ","link":"https://kane.mx/posts/archive/blogspot/find-exec-tip/","section":"posts","tags":["Shell"],"title":"[tip]Find -exec tip"},{"body":"","link":"https://kane.mx/tags/shell/","section":"tags","tags":null,"title":"Shell"},{"body":"I suffered p2 installation failed on the configure parse. Becase I try to add vm arguments for my application.\nFor example, I added '-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=8272' in the product configuration.\nP2 will fail when parsing the argument, because it contains ':' and ',' that should be escaped.\nIt works again after replacing it to '-agentlib${#58}jdwp=transport=dt_socket${#44}server=y${#44}suspend=n${#44}address=8272'.\nThe more detail note could be found in p2 touchpoint wiki.\nAnd I also opened bug to request improving it.\n","link":"https://kane.mx/posts/archive/blogspot/special-characters-in-p2-touchpoint/","section":"posts","tags":null,"title":"special characters in p2 touchpoint instruction"},{"body":"mount -t cifs -o username=xxx,password=xxx,workgroup=xx,iocharset=utf8 //share.domain/folder /localfolder\n","link":"https://kane.mx/posts/archive/blogspot/mount-windows-share-folder/","section":"posts","tags":null,"title":"mount windows share folder"},{"body":"1. open the file in vim, :%!xxd\n2. hexdump\n","link":"https://kane.mx/posts/archive/blogspot/way-to-dump-hex-file/","section":"posts","tags":null,"title":"the way to dump hex file"},{"body":"public class NameRuleTest { @Rule public TestName name = new TestName(); @Test public void testA() { assertEquals(\u0026quot;testA\u0026quot;, name.getMethodName()); }¬†@Test public void testB() { assertEquals(\u0026quot;testB\u0026quot;, name.getMethodName()); }} ","link":"https://kane.mx/posts/archive/blogspot/how-to-get-name-of-running-test-case-in/","section":"posts","tags":null,"title":"How to get the name of running test case in JUnit4"},{"body":"P2 install wizard firstly query the repository to find out the root installable unit(as well as top installable). Then p2 recalculate the dependency and try to search the requirements in all available repositories after user submits their installation request. Go to the license agreement page if all the dependencies are satisfied.\nP2 agreement page obtains all the units to be installed from the operands of provision plan. The number always is much greater than the number submitted by user. Because the submitted IUs only are the root IUs.\nP2 UI would check the unaccepted licenses comparing to before records. The policy class of p2 UI provides the license manager to record the even accepted license. It traverses all the installable units, querying its license whether it has already been accepted if it has. If the license agreement has been accepted, it would be ignored, won't be shown in the agreement page. Otherwise, new record is created to mark it as accepted by the license manager and display it in the agreement wizard page.\nThe default implementation of license manager would persist the accepted information in the file -- /.metadata/.plugins/org.eclipse.equinox.p2.ui.sdk/license.xml.\n","link":"https://kane.mx/posts/archive/blogspot/how-p2-ui-handles-with-license/","section":"posts","tags":null,"title":"How p2 UI handles with license agreements"},{"body":"Setup SSH without password.\na) execute \u0026quot;ssh-keygen -t rsa\u0026quot; under your linux/unix login to get the RSA keys.\n(press Enter for all)\nYou will get 2 files uner ~/.ssh/, id_rsa and id_rsa.pub\nb) Put the public key id_rsa.pub to your remote host: ~/.ssh/authorized_keys If the remote host share the same nfs, just try \u0026quot; cat id_rsa.pub \u0026gt;\u0026gt; ~/.ssh/authorized_keys\u0026quot;\n* Remember to modify hostname or ip info in ~/.ssh/authorized_keys to \u0026quot;\u0026quot;, so that you can login from any host without password in your NIS domain.\nFor example:\nssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA4Ri5J0s1BL/+mR7RfAuDW6FY2P6ILc61Zvw1BdDkvHMFrTzaC/AUMw33H7biAMCXuCleakCuSoV8ZDiGHYs4wOVvet5sDmphkwdiC4xTekdl3dRNvGjMVbvFUta/Y5CiayL6YIu47Ro6Vvu4Mutsrv/13pTlifrEz+NTR/+bzMb9nTniCwiryMyYod3E46b8WvS8yE3WK+tH4BZE8bjiCwdvAzSdPyk/OFNrlBNuF1yewwnxv1roRD3UalT2+7O4kfEG9sMvvBHjuX2l7xlUe3stBftYpigBbwGmmadxjRpNIlk88t5xKcQX6nSu7V8HI3GWPHI0D+ISIlbfU5Sunw== kzhu0@\n","link":"https://kane.mx/posts/archive/blogspot/ssh-key/","section":"posts","tags":["ssh","Tip"],"title":"[tip]ssh key"},{"body":"I wrote a plug-in to simplify the process to install the same plug-ins in different platform or different workstation.\nAnyone is interested in it, pls follow below guide to freely use it.\nhttp://code.google.com/p/kane-toolkit/wiki/P2Replication\nEnjoy it.\n","link":"https://kane.mx/posts/archive/blogspot/p2-replication-plug-in/","section":"posts","tags":["p2","Eclipse"],"title":"[Eclipse][P2]P2 replication plug-in"},{"body":"How Equinox load bundles Equinox launcher is responsible to start OSGi framework. The system bundle would be created and marked as installed when initializing the framework. Equinox also tries to install the installed bundles if finding them in persistence data during the initializing period. Of course there is no extra bundles would be installed when launching Equinox first time.\nThen Equinox launcher would install the bundles specified by vm's system property 'osgi.bundles'. And start the initial bundles that are marked as early start. For example, let's have a look at the configuration/config.ini of Eclipse, you would find a line similar as below,\nosgi.bundles=reference\\:file\\:org.eclipse.equinox.simpleconfigurator_1.0.200.v20090831.jar@1\\:start\nIt means the start level of bundle 'org.eclipse.equinox.simpleconfigurator_1.0.200.v20090831.jar' is 1, and it would be started after installing it.\nHere you would ask there are only two bundles are installed(one is system bundle 'org.eclipse.osgi', the other is 'org.eclipse.equinox.simpleconfigurator') when launching Equinox, how the other bundles are installed? It's done by the activate method of 'simpleconfigurator' bundle. The available bundles are recorded in plain text file configuration/org.eclipse.equinox.simpleconfigurator/bundles.info, simpleconfigurator read the file then install those bundles.\nIt's a new bundle management introduced by p2. P2 also supports the traditional way to install extensions, such as link file, .eclipseproduct file and directly copying features/plugins.\nBelow table lists the p2 bundles to implement the compatibility installation feature,\nBundle\nUsage\norg.eclipse.equinox.p2.directorywatcher\nthe definition and implementation of directory watcher API\norg.eclipse.equinox.p2.updatesite\nthe implementation of updatesite repository\norg.eclipse.equinox.p2.extensionlocation\nthe implementation of extension repository\norg.eclipse.equinox.p2.reconciler.dropins\nscan dropin folder and link files; watch the traditional configuration file used by update manager\nP2 reconciler would scan the dropin, link folder and legacy configuration file in every Equinox launching. You can disable the capability by marking it not be early start.\norg.eclipse.equinox.p2.reconciler.dropins,1.0.100.v20091010,plugins/org.eclipse.equinox.p2.reconciler.dropins_1.0.100.v20091010.jar,4,false\nIf finding some new bundles in dorpin folder, the reconciler would add the new bundles into a local metadata repository that is stored as OSGi data of Equinox. Then synchronize the bundles into the current p2 profile, then add the new bundles into bundles.info file.\n","link":"https://kane.mx/posts/archive/blogspot/how-equinox-load-bundles/","section":"posts","tags":["Equinox","Eclipse"],"title":"[eclipse]How Equinox load bundles"},{"body":"Learn p2 step by step See this link for detail\nLearn P2 step by step\np2 concept p2 install p2 install practice p2 repository publish customized p2 touchpoint p2 repository publish practice Example Code Reference p2 concept È¶ñÂÖàÊù•ÁêÜËß£p2ÂºïÂÖ•ÁöÑÂá†‰∏™Ê¶ÇÂøµ[1]\np2 / Agent\nThe provisioning infrastructure on client machines\nInstallable Unit (IU)\nMetadata that describes things that can be installed/configured\nArtifact\nThe actual content being installed/configured(e.g., bundle JARs)\nRepository\nA store of metadata or artifacts\nProfile\nThe target of install/management operations\nPlanner\nThe decision-making entity in the provisioning system\nEngine\nThe mechanism for executing provisioning requests\nTouchpoint\nThe part of the engine responsible for integrating the provisioning\nsystem to a particular runtime or management system\nIUÊØîËæÉÂ•ΩÁêÜËß£ÔºåÂ∞±ÊòØÂØπÂèØÂÆâË£ÖÊàñÈÖçÁΩÆÁöÑÈÉ®ÂàÜ‰∏ÄÁßçÊèèËø∞ÔºåÂπ∂‰∏çÂØπÂ∫îÂÆûÈôÖË¶ÅÂÆâË£ÖÁöÑÊñá‰ª∂„ÄÇ\nArifactÂ∞±ÊòØÊù•ÊèèËø∞ÂÆûÈôÖË¶ÅÂÆâË£ÖÁöÑÊñá‰ª∂ÔºåbundleÁ±ªÂûãÁöÑjarÔºåfeatureÔºåbinaryÊñá‰ª∂„ÄÇ\nËøôÊó∂Â∞±Êúâ‰∫ÜRepositoryÔºà‰ªìÂ∫ìÔºâËøô‰∏™Ê¶ÇÂøµÔºåÊòØÁî®Êù•‰øùÂ≠òartifacts‰ø°ÊÅØÔºå‰ª•ÂèäartifactsÁöÑÂÖÉÊï∞ÊçÆ„ÄÇÂÖÉÊï∞ÊçÆÂåÖÊã¨‰∫ÜÂØπartifactÁöÑÂîØ‰∏ÄÊ†áËØÜÁ¨¶ÔºåÁâàÊú¨ÔºåÂØπÂ§ñÊö¥Èú≤ÁöÑÊé•Âè£‰ø°ÊÅØÔºå‰ª•ÂèäÂÆÉ‰æùËµñÁöÑÊé•Âè£ÂèäÂÖ∂ÁâàÊú¨‰ø°ÊÅØÔºåÂêÑ‰∏™ÂÆâË£ÖÈò∂ÊÆµÈúÄË¶ÅÊâßË°åÁöÑÈÖçÁΩÆ„ÄÇÂú®p2ÈªòËÆ§ÁöÑÂÆûÁé∞ÈáåÈù¢ÔºåËøô‰∏§‰∏™repositoryÁî®xmlÊñá‰ª∂Êù•ÊèèËø∞ÔºåÂêåÊó∂Ë¢´ÂéãÁº©‰∏∫artifacts.jar, content.jarÊù•ÂáèÂ∞èÊñá‰ª∂Â§ßÂ∞èÔºåÁº©Áü≠‰º†ËæìÊó∂Èó¥„ÄÇ\n‰ªéEclipse 3.4Ëµ∑ÔºåÂΩì‰ªéËøúÁ®ãsiteÂÆâË£ÖÊñ∞ÁöÑËΩØ‰ª∂Êó∂ÔºåÂ∞±‰ºöÁúãÂà∞Êúâ‰∏™work threadÂú®ÂêéÂè∞‰∏ãËΩΩcontent.jarÊñá‰ª∂„ÄÇp2Âú®ÂÆâË£ÖÊó∂ÂÄôÔºåÈ¶ñÂÖà‰ºöÊ†πÊçÆcontent.xmlÔºàmetadata repositoryÔºâÊù•Ëß£ÊûêÊ≠£Âú®ÂÆâË£ÖËΩØ‰ª∂ÁöÑ‰æùËµñ„ÄÇÂú®ÂΩìÂâçruntimeÈáåÈù¢Êü•Êâæmetadata‰∏≠ÊåáÂÆöÁöÑ‰æùËµñÔºåÂ¶ÇÊûúÊª°Ë∂≥ÊâçÁªßÁª≠ÂÆâË£Ö„ÄÇÊçÆÊàë‰∏™‰∫∫ÁªèÈ™åÔºåÂ¶ÇÊûúÂÆâË£ÖÁöÑËΩØ‰ª∂ÊØîËæÉÂ§çÊùÇÔºåÈÇ£ÂÆÉ‰∫ßÁîüÁöÑmetadataÊñá‰ª∂Â∞±‰ºöÊØîËæÉÂ§ßÔºàÂæàÂÆπÊòì‰∏äÂÖÜÔºâÔºå‰∏ãËΩΩËøô‰∏™Êñá‰ª∂‰ª•ÂèäËß£ÊûêÂÆÉÁöÑÂÜÖÂÆπÈÉΩ‰ºöÊØîËæÉÊÖ¢Ôºå‰ªéËÄåÂΩ±ÂìçÁî®Êà∑‰ΩìÈ™å„ÄÇ\nÊØîËæÉÁÅµÊ¥ªÁöÑÊòØÔºåÁî®Êà∑ÂèØ‰ª•ÂÆûÁé∞Ëá™Â∑±ÁöÑArtifactRepositoryÂíåMetadataRepositoryÔºåÊ≥®ÂÜåÂà∞ÂÆÉ‰ª¨ÂêÑËá™ÁöÑManagerÈáåÈù¢Â∞±ÂèØ‰ª•‰∫Ü„ÄÇÊâÄÊúâËøô‰∫õÊúçÂä°ÈÉΩË¢´ÂÆûÁé∞‰∏∫OSGi Service.\n‰∏ã‰∏Ä‰∏™ProfileÔºåÊòØÁî®Êù•ÁÆ°ÁêÜÂÆâË£ÖÁõÆÊ†áÈáåÁöÑËΩØ‰ª∂‰ø°ÊÅØ„ÄÇp2Âú®Ë¢´ËÆæËÆ°ÁöÑÊó∂ÂÄôÔºåÂ∏åÊúõËß£ÂÜ≥Â§ö‰∏™eclipseÂÆû‰æãÂÖ±‰∫´‰∏Ä‰ªΩÂÆâË£ÖÁöÑÊüêËΩØ‰ª∂„ÄÇÊØîÂ¶Ç‰∏∫‰∫ÜÊüêÁßçÁõÆÁöÑÊàëÊú∫Âô®‰∏äÊúâÂ•ΩÂá†‰∏™EclipseÔºåÂêåÊó∂ÂÆÉ‰ª¨ÈÉΩÈúÄË¶ÅCDTÔºåÂÖçÂéª‰∏∫ÈáçÂ§çÂÆâË£ÖÁöÑÈ∫ªÁÉ¶„ÄÇprofileÂ∞±‰ºöËÆ∞ÂΩïÊØèÊ¨°ÂÆâË£ÖÁöÑÂÜÖÂÆπÔºåËÆ©Êï¥‰∏™Â∫îÁî®Á®ãÂ∫èË¢´ÁÆ°ÁêÜËµ∑Êù•„ÄÇÂú®GalileoÈáåÂÆâË£ÖÁöÑËΩØ‰ª∂ÈÉΩÂèØ‰ª•ËΩØ‰ª∂ÁÆ°ÁêÜÈáåÈù¢Êü•ÊâæÂà∞„ÄÇ\nPlannerÂíåEngineÂÆåÂÖ®Â∞±ÊòØp2ÂÜÖÈÉ®ÁöÑ‰∏úË•ø„ÄÇ‰ªª‰Ωïp2ÁöÑÊìç‰ΩúÔºàÂÆâË£ÖÔºåÂà†Èô§ÔºåÈÖçÁΩÆÔºâÈÉΩÈúÄË¶ÅPlannerÂÆû‰æãÊù•ÊèèËø∞„ÄÇÊúâ‰∫ÜPlanner‰ª•ÂêéÔºåËøòÈúÄË¶ÅÂàõÂª∫‰∏Ä‰∏™EngineÂØπË±°ÔºåÈÄöËøáengineÊù•ÊâßË°åÂØπÂ∫îÁöÑplan„ÄÇËøôÂ∞±ÊòØÁõÆÂâçË∞ÉÁî®p2 APIÊù•ÂÆåÊàêÂÆâË£ÖÁöÑ‰∏Ä‰∏™ËøáÁ®ã„ÄÇ\nÊúÄÂêé‰∏Ä‰∏™Touchpoint„ÄÇÁ®ãÂ∫èÂú®ÂÆâË£ÖÁöÑÊó∂ÂÄôÔºåÂèØËÉΩ‰ºöÊ†πÊçÆruntime(os, ws, archÁ≠âÔºâÊàñÈò∂ÊÆµ(ÂÆâË£ÖÔºåÂç∏ËΩΩÔºåÈÖçÁΩÆÁ≠â)ÊâßË°åÊüê‰∫õÈÖçÁΩÆÔºåtouchpointÂ∞±ÊòØÂ∏ÆÂä©ÂÆûÁé∞Ëøô‰∫õÈÖçÁΩÆ„ÄÇÂÖ∑‰ΩìÊìç‰ΩúÊòØ‰ª•IU‰∏∫Âçï‰ΩçËÆ∞ÂΩïÂú®metadata repositoryÈáåÁöÑ„ÄÇp2ÈªòËÆ§ÂÆûÁé∞‰∫Ü‰∏Ä‰∫õEclipse touchpointÔºåÊØîÂ¶ÇÊã∑Ë¥ùÔºåÂà†Èô§Êñá‰ª∂ÔºåÊâßË°åÂ§ñÈÉ®Á®ãÂ∫èÁ≠â„ÄÇÂ¶ÇÊûúÁî®Êà∑ÊúâËá™Â∑±ÁâπÊÆäÁöÑnativeÊìç‰ΩúÈúÄË¶ÅÊâßË°åÔºåÂèØ‰ª•Ëá™Â∑±ÂÆûÁé∞Ëá™ÂÆö‰πâÁöÑtouchpoint„ÄÇ\np2 install Êúâ‰∫ÜËøô‰∫õÊ¶ÇÂøµ‰ª•ÂêéÔºåÊàë‰ª¨Êù•ÁúãÁúãÂ¶Ç‰Ωï‰ΩøÁî®p2 API„ÄÇ‰ª•ÂÆâË£Ö‰∏∫‰æãÔºå\nÈ¶ñÂÖàÈúÄË¶ÅÂæóÂà∞ÂΩìÂâçÂÆâË£ÖÁöÑprofile„ÄÇÂ¶ÇÊûúÊòØÂÖ®Êñ∞ÂÆâË£ÖÔºåÈÄöËøáIProfileRegistry.addProfileÂàõÂª∫‰∏Ä‰∏™Êñ∞profile„ÄÇÊòØÊõ¥Êñ∞ÂÆâË£ÖÁöÑËØùÔºåÂèØ‰ª•ÈÄöËøáIProfileRegistryÊü•ËØ¢Âà∞ÊúüÊúõÊõ¥Êñ∞ÁöÑprofile„ÄÇÂàõÂª∫profileÁöÑÊó∂ÂÄôÔºåÈúÄË¶ÅÊ≥®ÊÑèËÆæÁΩÆprofileÁöÑÂ±ûÊÄßÔºå\nMap\u0026lt;String, String\u0026gt; profileProperties = new HashMap\u0026lt;String, String\u0026gt;();\nprofileProperties.put(IProfile.PROP_INSTALL_FOLDER, installLocation.getAbsolutePath());\nprofileProperties.put(IProfile.PROP_FLAVOR, \u0026quot;tooling\u0026quot;); //$NON-NLS-1$\nprofileProperties.put(IProfile.PROP_ENVIRONMENTS, \u0026quot;osgi.os=\u0026quot; + Platform.getOS() + \u0026quot;,osgi.ws=\u0026quot; + Platform.getWS() + \u0026quot;,osgi.arch=\u0026quot; + Platform.getOSArch()); //$NON-NLS-1$;\nprofileProperties.put(IProfile.PROP_NL, \u0026quot;en_US\u0026quot;); //$NON-NLS-1$\nprofileProperties.put(IProfile.PROP_INSTALL_FEATURES, \u0026quot;true\u0026quot;);\nprofileProperties.put(IProfile.PROP_CONFIGURATION_FOLDER, new File(installLocation, \u0026quot;configuration\u0026quot;).getAbsolutePath());\nprofileProperties.put(IProfile.PROP_ROAMING, \u0026quot;true\u0026quot;);\nprofileProperties.put(IProfile.PROP_CACHE, installLocation.getAbsolutePath());\ncurrentProfile = registry.addProfile(PROFILE_ID, profileProperties);\nPROP_INSTALL_FOLDERËÆæÁΩÆÂÆâË£ÖÁöÑÁõÆÂΩïÔºåPROP_CACHEËÆæÁΩÆ‰øùÂ≠ò‰∏ãËΩΩÊù•ÁöÑEclipse IU(features/plugins)ÁöÑÁõÆÂΩïÔºåÂ¶ÇÊûúrepositoryÊòØ‰ª•feature‰∏∫Âçï‰ΩçÊù•ÂèëÂ∏ÉÁöÑËØùÔºåÈúÄË¶ÅËÆæÁΩÆPROP_INSTALL_FEATURES‰∏∫true„ÄÇÂ¶ÇÊûúrepositoryÂåÖÊã¨nativeÁöÑbinaryÔºàÊØîÂ¶ÇlauncherÔºâ‰πüÈúÄË¶ÅÊåáÂÆöÊ≠£Á°ÆÁöÑPROP_ENVIROMENTSÔºåÂåÖÊã¨OS,WS,ARCHÊàñPROCESSOR„ÄÇ\nÁÑ∂ÂêéÈúÄË¶ÅËé∑ÂæóÂ∞ÜË¶ÅÂÆâË£ÖÁöÑIMetadataRepositoryÈõÜÂêà„ÄÇÊØîÂ¶ÇÔºö\nArrayList ius = new ArrayList();\nIMetadataRepositoryManager repositoryManager = (IMetadataRepositoryManager) ServiceHelper.getService(Activator.getDefault().getBundle().getBundleContext(),\nIMetadataRepositoryManager.class.getName());¬†if (repositoryManager == null)¬†throw new InterruptedException(\u0026quot;Failed to get IMetadataRepositoryManager.\u0026quot;);\ntry {\nfor (URI uri : uris) {\nIMetadataRepository metaRepo = repositoryManager.loadRepository(uri, progress.newChild(50/uris.length));\nCollector collector = metaRepo.query(new AccpetQuery(), new LatestNoninstalledIUCollector(currentProfile), progress.newChild(50/uris.length));\nius.addAll(collector.toCollection());\n}\n} catch (ProvisionException e) {\nthrow new InterruptedException(\u0026quot;Failed to get IMetadataRepository.\u0026quot;);\n}\n-ÂêåÊó∂ËøôÈáå‰πüÊü•ÊâæÂá∫IMetaRepository‰∏≠Ê≤°ÂÆâË£ÖËøáÁöÑIUs„ÄÇËøôÂ∞±ÈúÄË¶ÅÂêåÂΩìÂâçÂÆâË£ÖÁöÑprofile‰∏≠Â∑≤ÁªèÂÆâË£ÖËøáÁöÑÂÜÖÂÆπÊù•ÊØîËæÉÔºå\nCollector collector = metaRepo.query(new AccpetQuery(), new LatestNoninstalledIUCollector(currentProfile), progress.newChild(50/uris.length));\nËøôÈáåÈúÄË¶ÅÊåáÂá∫ÁöÑÊòØÔºåIMetadataRepositoryÂÆûÁé∞‰∫ÜIQueryableÊé•Âè£„ÄÇIQueryableÊòØp2ÂºïÂÖ•ÁöÑÊü•ÊâæÊé•Âè£ÔºåËøîÂõûÊª°Ë∂≥ÁâπÊÆäÊü•ËØ¢Êù°‰ª∂ÁöÑÈõÜÂêàÔºåÂêåÊó∂‰º†ÂÖ•‰∫Ü‰∏Ä‰∏™IProgressMonitorÂØπË±°ÔºåÂèØ‰ª•ÂèçÂ∫îÊü•ÊâæËøõÂ∫¶„ÄÇËøôÈáåÁöÑAcceptQueryÔºåLatestNoninstalledIUCollectorÊòØËá™ÂÆö‰πâÁöÑQueryÂíåCollectorÂØπË±°„ÄÇp2Â∑≤ÁªèÂÆûÁé∞‰∫ÜËÆ∏Â§öÊúâÁî®ÁöÑQueryÔºåÁªèÂ∏∏Áî®Âà∞ÁöÑÊúâInstallableUnitQueryÔºåIUPropertyQueryÔºåRangeQuery„ÄÇ\n-Êé•‰∏ãÊù•ÁîüÊàêIEngineÊâÄÈúÄÁöÑProvisionPlan„ÄÇÈ¶ñÂÖàÂàõÂª∫ProfileChangeRequestÂØπË±°ÔºåÂ∞ÜÂÖàÂâçÊü•ÊâæÂá∫ÁöÑË¶ÅÂÆâË£ÖÁöÑIUsÊ∑ªÂä†ËøõÂéª„ÄÇ\nrequest.addInstallableUnits(ius);\nÂà†Èô§ÁöÑËØùÂàô‰∏é‰πãÁõ∏Âèç„ÄÇÊõ¥Êñ∞ÁöÑËØù‰πüÈúÄË¶ÅÈÄöËøáProfileChangeRequest.removeInstallableUnits()ÂéªÊéâÊóßÁâàÊú¨ÁöÑIUs„ÄÇ\nË∞ÉÁî®IPlanner serviceÁöÑgetProvisioningPlan(ProfileChangeRequest, ProvisioningContext, IProgressMonitor)ÂæóÂà∞ÂØπÂ∫î‰∫éÂΩìÂâçrequestÁöÑplan„ÄÇ\n-ÊúÄÂêéÂ∞±ÊòØË∞ÉÁî®IEngine.perform(IProfile, PhaseSet, Operand[], ProvisioningContext, IProgressMonitor)Êù•ÊâßË°åprovisioningÊìç‰Ωú„ÄÇËøôÈáåÁöÑPhaseSetÊòØÁî®Êù•ÊåáÂÆöEngineÂ∞ÜË¶ÅÊâßË°åÁöÑÂá†‰∏™Èò∂ÊÆµÔºå‰ª•ÂèäÊØè‰∏™Èò∂ÊÆµÁöÑÊâßË°åÊó∂Èó¥ÊùÉÈáç„ÄÇËøô‰∫õÈò∂ÊÆµÂåÖÊã¨‰∫ÜCollect, Unconfigure, Uninstall, Property, CheckTrust, Install, Configure. Â¶ÇÊûúÁÜüÊÇâEclipse‰πãÂâçÁöÑInstaller HandlerÔºåÂØπUnconfigure/Uninstall/Install/ConfigureÂ∫îËØ•ÈÉΩÂæàÁÜüÊÇâ„ÄÇ Âú®p2ÈáåÔºåÊõ¥ÊòØÂ∞ÜCollect, CheckTrustËøô‰∫õËøáÁ®ã‰πüÊö¥Èú≤‰∫ÜÂá∫Êù•„ÄÇ‰∏ãÈù¢ÊòØp2ÈáåÈªòËÆ§PhaseSetÁöÑÂÆûÁé∞Ôºå\npublic DefaultPhaseSet() {\nthis(new Phase[] {new Collect(100), new Unconfigure(10, forcedUninstall), new Uninstall(50, forcedUninstall), new Property(1), new CheckTrust(10), new Install(50), new Configure(10)});\n}\nOperand[]ÈÄöËøáProvisionPlan.getOperands()Ëé∑Âæó„ÄÇ\np2 install practice ÂÖàÂà∂‰Ωú‰∏Ä‰∏™ÂèØÂÆâË£ÖÁöÑrepositoryÔºåËøôÈáåÁöÑÊñπÊ≥ïÊòØÂü∫‰∫éEclipseÊèê‰æõÁöÑÊ®°ÁâàÂàõÂª∫‰∏Ä‰∏™RCPÁ®ãÂ∫èÔºåÊØîÂ¶Çmail template,\nÁÑ∂ÂêéÂàõÂª∫‰∏Ä‰∏™featureÂåÖÂê´ÂàöÊâçÂàõÂª∫Âá∫Êù•ÁöÑplug-in 'com.example.mail'„ÄÇ\nÂü∫‰∫éÂ≠òÂú®ÁöÑ‚Äòcom.example.mail.product‚ÄôÂàõÂª∫product configurationÔºåÂ∞ÜÂÖ∂ËÆæÁΩÆ‰∏∫base on features, ÂêåÊó∂Âú®dependenciesÈ°µÈù¢Ê∑ªÂä†‰ª•‰∏ãfeature„ÄÇfeatureÁöÑqaulifier id‰æùËµñ‰∫éÁî®Âà∞ÁöÑEclipsseÁâàÊú¨Ôºå‰ªé‰∏ãÂõæÁúãÂà∞ÊàëËøôÈáå‰ΩøÁî®ÁöÑÊòØEclipse 3.5.1„ÄÇÂ¶ÇÊûúË¶ÅËÆ©RCPÁ®ãÂ∫èÂÖ∑ÊúâÂÆâË£ÖÊèí‰ª∂ÁöÑËÉΩÂäõÔºàÂåÖÂê´p2Âíåp2 UIÔºâÔºåÂ∞±ÈúÄË¶Å‰æùËµñÊõ¥Â§öÁöÑfeature„ÄÇÂêéÈù¢ÁöÑexampleÈáåÈù¢‰ºöÂÆûÁé∞ËøôÈÉ®ÂàÜÂäüËÉΩ„ÄÇÂè¶Â§ñÊ≥®ÊÑèÔºöID‰∏çËÉΩÂåÖÊã¨Á©∫Ê†ºÂ≠óÁ¨¶„ÄÇ\nÊé•‰∏ãÊù•‰ΩøÁî®Eclipse Product Export WizardÁîüÊàêrepository„ÄÇËÆ∞ÂæóË¶ÅÂãæÈÄâ‰∏ägenerate metadata repository„ÄÇ\nÂú®ÊàêÂäüÂàõÂª∫‰∫ÜMail ApplicationÁöÑrepositoryÂêéÔºåËØïÁî®Êàë‰ª¨Ëá™Â∑±ÁöÑp2 installerÊù•ÂÆâË£ÖËøô‰∏™Â∫îÁî®Á®ãÂ∫è„ÄÇÂÆâË£ÖËøáÁ®ãÁ±ª‰ºº‰∏ãÈù¢ÁöÑÊà™Âõæ„ÄÇÁÑ∂ÂêéÊâßË°å/folk/kzhu0/tmp/mailrcp/mailÊù•ËøêË°åMail Application.\np2 repository publish Ëøô‰∏ÄËäÇÂ∞Ü‰ºöÂ±ïÁ§∫Â¶Ç‰ΩïÂèëÂ∏É/‰∫ßÁîüÂü∫‰∫ép2ÁöÑrepository„ÄÇÂú®p2ÊúÄÊó©ÁöÑÁâàÊú¨Eclipse 3.4‰∏≠Â∞ÜÁîüÊàêrepositoryËøô‰∏™Á®ãÂ∫èÁß∞‰∏∫generatorÔºåËÄå3.5ÂØπÊ≠§ÈáçÊûÑÂêéÂëΩÂêç‰∏∫publisher„ÄÇÈáçÊûÑÂêéÁöÑpublishËøáÁ®ãÁÆÄÂçïÊòé‰∫Ü„ÄÇÈ¶ñÂÖàÈúÄË¶ÅÂàõÂª∫‰∏Ä‰∏™IPublishInfoÂØπË±°ÔºåÂÆÉË¥üË¥£Êèê‰æõÂ∞ÜË¶ÅÁîüÊàêÁöÑrepositoryÁöÑÊÉÖÂÜµ„ÄÇÂåÖÊã¨‰∫Ümeta repository, artifact repositoryÁöÑ‰ø°ÊÅØÔºåÂ±ûÊÄßÔºå‰ª•ÂèäÊèê‰æõËæÖÂä©‰ø°ÊÅØÁöÑadviceÂØπË±°„ÄÇIPublisherAdviceÂèØ‰ª•Áúã‰ΩúÁ±ª‰ººÂàõÂª∫RCPÁ™óÂè£Êó∂ÂÄôÁöÑWorkbenchAdviceÂíåWorkbenchWindowAdviceÁ≠âËæÖÂä©Á±ª„ÄÇÂÆÉÁî®Êù•Êèê‰æõÈúÄË¶ÅËÆ∞ÂΩïÂú®repository‰∏≠ÁöÑIUÁâπÊÆä‰ø°ÊÅØ„ÄÇÊØîÂ¶ÇIUÁöÑÂ±ûÊÄßÔºåtouchpointÁöÑÁ±ªÂûãÂèäÂêÑ‰∏™Èò∂ÊÆµÊâßË°åÁöÑactionÔºåÂØπÂèØÊâßË°åÊñá‰ª∂ÊàñÈÖçÁΩÆÊñá‰ª∂IUÁöÑÂ§ÑÁêÜ„ÄÇ\nÊ≠§Â§ñËøòÈúÄË¶ÅÂàõÂª∫IPublisherActionÊù•Â§ÑÁêÜ‰∏çÂêåÁ±ªÂûãÁöÑIUÂèëÂ∏ÉËøáÁ®ã„ÄÇ‰æãÂ¶ÇBundlesActionÊù•ÂÆûÁé∞ÂèëÂ∏ÉbundlesÂà∞repositoryÔºåFeaturesActionÂàôÊòØÂ§ÑÁêÜfeature„ÄÇÊ≠§Â§ñp2Â∑≤Êèê‰æõÁöÑIPublisherActionËøòÂåÖÊã¨product action, config action, launcher actionÂíåjre actionÁ≠âÁ≠â[2]„ÄÇ\nÊúâ‰∫ÜÊèèËø∞repositoryÊÉÖÂÜµÁöÑpublishinfoÂíåÂèëÂ∏ÉÂêÑÁßçIUsÁöÑactionÂêéÔºåË∞ÉÁî®Publisher.publishÊñπÊ≥ïÂÆåÊàêrepositoryÁöÑÂèëÂ∏É„ÄÇ\nIPublisherInfo info = createPublisherInfo();\nIPublisherAction[] actions = createActions();\nPublisher publisher = new Publisher(info);\npublisher.publish(actions, new NullProgressMonitor());\nËøôÈáåÊúâ‰∏ÄÁÇπÈúÄË¶ÅÊ≥®ÊÑèÔºåpublishÂè™ÊòØÊääÂ∞ÜË¶ÅÁî®‰∫éÈÉ®ÁΩ≤ÁöÑfeatures/plugins/binaryÂèëÂ∏ÉÂà∞repositoryÔºåÂπ∂‰∏çË¥üË¥£ÁºñËØëÊâìÂåÖÂÆÉ‰ª¨„ÄÇÂÖàÂâçÊàë‰ª¨‰ΩøÁî®ËøáEclipse ExportÂäüËÉΩÊó¢ÁºñËØëÊâìÂåÖfeatures/pluginsÂêåÊó∂ÂèàÁîüÊàêrepository„ÄÇExportÂÆûÁé∞ÁöÑËøáÁ®ãÈ¶ñÂÖàÊòØË∞ÉÁî®PDEÊù•ÁºñËØëÊâìÂåÖfeatures/pluginsÔºåÂÜçË∞ÉÁî®ÂØπÂ∫îÁöÑpublisherÂ∫îÁî®Á®ãÂ∫èÂ∞ÜÁºñËØëÂêéÁöÑfeatures/plugins/productÂèëÂ∏É‰∏∫repository„ÄÇ\ncustomized p2 touchpoint ÂâçÈù¢‰∏ÄËäÇÂ∑≤ÁªèÊèêËøáIPublishInfoÈÄöËøáÈ¢ùÂ§ñÁöÑIPublisherAdviseÊù•ÂÆöÂà∂ÂèëÂ∏ÉÂà∞repositoryÁöÑIU‰ø°ÊÅØ„ÄÇËøôÈáå‰ªãÁªç‰∏∫Ëá™Â∑±ÁöÑIUÂÆöÂà∂Êñ∞ÁöÑtouchpointÁ±ªÂûãÔºåÂπ∂‰∏îË¶ÅÊ±ÇÂú®ÈÖçÁΩÆÈò∂ÊÆµÂú®Êìç‰ΩúÁ≥ªÁªüÊ°åÈù¢ÂàõÂª∫Â∫îÁî®Á®ãÂ∫èÁöÑÂêØÂä®Âø´Êç∑ÊñπÂºè„ÄÇÈ¶ñÂÖà‰∏∫Êàë‰ª¨ÁöÑPublisherInfoÊ∑ªÂä†Â§ÑÁêÜtouchpoint dataÁöÑadviceÔºåNativeLauncherTouchPointÂÆûÁé∞‰∫ÜITouchpointAdviceÊé•Âè£ÔºåpublisherÂú®ÂèëÂ∏ÉÁöÑÊó∂ÂÄôÂΩìÂ§ÑÁêÜÂà∞touchpoint dataÈÉ®ÂàÜÔºå‰ºöÊü•ÊâæÂÆûÁé∞‰∫ÜITouchpointAdviceÊé•Âè£ÁöÑadvice„ÄÇÂ¶ÇÊûúÊúâadviceÂèØÁî®ÔºåÂ∞Ü‰ºöËÆ©Ëøô‰∫õadviceÂ§ÑÁêÜÁé∞ÊúâÁöÑtouchpoint dataÔºåÂπ∂‰∏îÂæóÂà∞Êñ∞ÁöÑtouchpoint dataÔºåÂπ∂ÊääÁªìÊûú‰øùÂ≠òÂà∞metadata repositoryÂΩì‰∏≠„ÄÇ\nPublisherInfo result = new PublisherInfo();\nresult.addAdvice(new NativeLauncherTouchPoint());\nNativeLauncherTouchPointÂ∞ÜÊåáÂÆö‰∏∫ÁâπÂÆöÁöÑIUÂú®configureÈò∂ÊÆµÊâßË°åcreateDesktopÊìç‰ΩúÔºå‰ª•ÂèäÁõ∏ÂèçÁöÑÊìç‰ΩúÔºåunconfigureÈò∂ÊÆµÊâßË°ådeleteDesktopÊìç‰Ωú„ÄÇ\nÊõ¥Êîπtouchpoint typeÁöÑÊñπÊ≥ïÂ¶Ç‰∏ã„ÄÇÂΩìÁÑ∂‰πüÂèØ‰ª•‰∏∫Áé∞ÊúâÁöÑtouchpoint typeÊâ©Â±ïaction„ÄÇÂÜÖÁΩÆÁöÑtouchpointÁ±ªÂûãÂíåactionÁöÑÂÖ∑‰ΩìÂëΩ‰ª§Áî®Ê≥ïÔºåËØ∑ÂèÇËÄÉp2 wiki[3]„ÄÇ\niu.setTouchpointType(DesktopTouchpoint.TOUCHPOINT_TYPE);\ntouchpointÁ±ªÂûãÂíåactionÈÉΩÊòØÈÄöËøáextension pointÊù•Êâ©Â±ïÁöÑ„ÄÇÈÄöËøáÊâ©Â±ï‚Äúorg.eclipse.equinox.p2.engine.touchpoints‚ÄùÊù•Ê∑ªÂä†Êñ∞ÁöÑtouchpointÁ±ªÂûãÔºåÊâ©Â±ï‚Äùorg.eclipse.equinox.p2.engine.actions‚ÄúÂ∞ÜÊñ∞ÁöÑactionÂêåÊüê‰∏™Á±ªÂûãÂÖ≥ËÅîËµ∑Êù•„ÄÇ\np2 repository publish practice Êàë‰ª¨ÂàõÂª∫plug-in 'com.example.p2.touchpoint'Êù•ÂÆûÁé∞Ê°åÈù¢Âø´Êç∑ÊñπÂºèÁöÑÊâ©Â±ïÔºåÂπ∂‰∏îÂàõÂª∫'com.example.p2.feature'ÂåÖÂê´touchpointÂÆûÁé∞ÁöÑplug-in„ÄÇÂÖ∑‰ΩìÂÆûÁé∞ËØ∑ÂèÇËÄÉp2 exampleÊ∫êÁ†Å„ÄÇ\nÁÑ∂Âêé‰∏∫Mail ApplicationÊ∑ªÂä†p2Áõ∏ÂÖ≥featureÁöÑ‰æùËµñÔºåÈáçÊñ∞ÂèëÂ∏ÉÂæóÂà∞ÊîØÊåÅÂÆâË£ÖËΩØ‰ª∂ÁöÑÊñ∞ÁâàÊú¨„ÄÇÂπ∂‰∏îÁî®p2 example installerÂÆâË£ÖÂÆÉ„ÄÇp.s: ‰∏™‰∫∫ÊÑüËßâEclipseÂú®ÂåÖÂê´Á¨¨‰∏âÊñπplug-inÊó∂ÔºåÂ±ÇÊ¨°Êúâ‰∫õÈóÆÈ¢ò„ÄÇp2‰Ωú‰∏∫‰∏Ä‰∏™runtimeÁöÑprojectÔºàË∑üequinox, ECFÂêåÁ∫ßÔºâÔºåÂ±ÖÁÑ∂ÈúÄË¶ÅÁõ¥Êé•ÊàñÈó¥Êé•‰æùËµñhelp, rcp.platformËøôÊ†∑ÁöÑ‰∏äÂ±ÇÊ®°Âùó„ÄÇ\nÊé•‰∏ãÊù•ÂàõÂª∫plug-in 'com.example.mail.desktop' Âíå feature 'com.example.mail.desktop.feature'Ôºå‰Ωú‰∏∫Êèê‰æõÊ°åÈù¢Âø´Êç∑ÊñπÂºèÁöÑIU„ÄÇÁî®Eclipse Export FeatureÂ∞Ü'com.example.mail.desktop.feature'ÂØºÂá∫ÔºåÂÆûÈôÖÂ∞±ÊòØÁî®PDEÊõøÊàë‰ª¨ÁºñËØëÊâìÂåÖ:)„ÄÇ\nËøêË°å‚Äòcom.example.p2.generator'Êèê‰æõÁöÑheadless publisherÊù•ÁîüÊàêÊàë‰ª¨ÂÆöÂà∂ÁöÑrepository„ÄÇ‚Äô/folk/kzhu0/tmp/mail/desktop-deploy'ÊòØÂÖàÂâçdesktop featureÂØºÂá∫ÂêéÁöÑË∑ØÂæÑÔºåËÄå'/folk/kzhu0/tmp/mail/desktop'ÊòØÁîüÊàêrepositoryÁöÑË∑ØÂæÑ„ÄÇ\nËøêË°åÊñ∞ÁâàÊú¨ÁöÑMail ApplicationÔºåÂú®HelpËèúÂçï‰∏ãÈù¢‰ºöÂ§öÂá∫Install New SoftwareÈÄâÈ°π„ÄÇÂ∞ÜËá™ÂÆö‰πâpublisherÁîüÊàêÁöÑDesktop feature repositoryÊ∑ªÂä†‰∏∫Êñ∞ÁöÑËΩØ‰ª∂Ê∫êÔºåÂÆâË£ÖMail Desktop Feature„ÄÇÂÆâË£ÖÂÆåÊàêÂêéÔºåÂ∞ÜÂú®Ê°åÈù¢ÊâæÂà∞Mail ApplicationÁöÑÂø´Êç∑ÊñπÂºè„ÄÇÂú®Installation DetailÈáåÈù¢Â∞Ü‰ºöÂá∫Áé∞ËøôÊ¨°ÂÆâË£ÖÁöÑÂÜÖÂÆπ„ÄÇÈÄâ‰∏≠Desktop FeatureÂêéÈÄâÊã©Âç∏ËΩΩÔºåÊ°åÈù¢ÁöÑÂø´Êç∑ÊñπÂºèÊñá‰ª∂Â∞Ü‰ºöË¢´Âà†Èô§Êéâ„ÄÇÂΩìÁÑ∂‰πüÂèØ‰ª•‰ΩøÁî®p2 example installerÊù•‰∏∫Mail ApplicationÂÆâË£Ödesktop feature„ÄÇp.s: example‰ª£Á†ÅÈáåÂè™ÂÆûÁé∞‰∫ÜÂàõÂª∫linux/unixÊ°åÈù¢Âø´Êç∑ÊñπÂºè„ÄÇ\nExample Code Example CodeÂ∫îËØ•Âè™ËÉΩÁºñËØëËøêË°åÂú®Eclipse 3.5.x„ÄÇExample Code‰ΩøÁî®ÁöÑÈÉΩÊòØp2 internal API, ËÄåp2 public APIÂ∞Ü‰ºöÈöèEclipse 3.6È¶ñÊ¨°ÂèëÂ∏É„ÄÇËøô‰∫õÁ±ªÂíåÊñπÊ≥ïÂü∫Êú¨ÈÉΩ‰ºö‰øùÁïôÔºå‰ΩÜÂëΩÂêçÔºåÂåÖ‰∏ÄÂÆö‰ºöÊúâÈáçÊûÑ„ÄÇ\nhttp://code.google.com/p/kane-toolkit/source/browse/#svn/trunk/p2-example\nReference [1] http://wiki.eclipse.org/Equinox/p2/Concepts\n[2] http://wiki.eclipse.org/Equinox/p2/Publisher\n[3] http://wiki.eclipse.org/Equinox/p2/Engine/Touchpoint_Instructions\n","link":"https://kane.mx/posts/archive/blogspot/learn-p2-step-by-step/","section":"posts","tags":["Equinox","p2","Eclipse"],"title":"[Eclipse][P2]Learn p2 step by step"},{"body":"ssh -qTfnN -D LocalPort remotehost\nAll the added options are for a ssh session that's used for tunneling.\n-q :- be very quite, we are acting only as a tunnel.\n-T :- Do not allocate a pseudo tty, we are only acting a tunnel.\n-f :- move the ssh process to background, as we don't want to interact with this ssh session directly.\n-N :- Do not execute remote command.\n-n :- redirect standard input to /dev/null.\nIn addition on a slow line you can gain performance by enabling compression with the -C option.\n","link":"https://kane.mx/posts/archive/blogspot/ssh-forward/","section":"posts","tags":["ssh","Tip"],"title":"[tip]ssh forward"},{"body":"-Dosgi.install.area=\u0026lt;launcher's folder\u0026gt;\n-Declipse.p2.profile=\n","link":"https://kane.mx/posts/archive/blogspot/simulate-p2-self-host-in-eclipse-run/","section":"posts","tags":["p2","Eclipse"],"title":"Simulate p2 self host in Eclipse run"},{"body":"The IPreferenceStore API of Eclipse is based on OSGi's preferences service.¬†Equinox implements¬†several scope context for different preferences, such DefaultScope, InstanceScope and ConfigurationScope. The IPreferenceStore is the wrapper of instance scope for back-compatibility. It stored the data in workspace(osgi.data.area).\nThe workspace folder would be created when launching RCP application if it doesn't exist. But we can use argument '-data @none' to suppress the creation of workspace. If that, the instance scope/IPreferenceStore can't store any value any more.\nThere is a workaround to resolve such issue. Use ConfigurationScope instead of InstanceScope. Both of them are implemented the same interface, so it's easy to migrate to use ConfigurationScope. The data of configuration scope would be stored in @config.dir/.setting folder.\n","link":"https://kane.mx/posts/archive/blogspot/eclipseosgi-preference/","section":"posts","tags":["Equinox","OSGi","Eclipse"],"title":"Eclipse/OSGi preference"},{"body":"Eclipse platform register an OSGi service 'IProxyService' to manage network connection, which has capability to set proxy setting. There are three types of proxy working mode,\nDirect(no proxy), Manual(specified by user), Native(using OS's proxy setting, such as gnome-proxy, IE). There are three types of proxy supported by IProxyService. They're http, https and socks.\nIt also allows to add/remove ip address from white list, which are accessed without connecting proxy.\nEnd users can manage the proxy setting of Eclipse via Preference - General - Network Connections. Eclipse would do persistence of user's setting. Other components of Eclipse also use those proxy settings to access network, such as ECF.\nBelow code snippet shows how to use proxy API to manually specify proxy server,\nproxyService.setProxiesEnabled(true);\nproxyService.setSystemProxiesEnabled(false); IProxyData[] datas = proxyService.getProxyData(); IProxyData proxyData = null; for(IProxyData data : datas) { // clean old data ((ProxyData)data).setSource(\u0026quot;Manual\u0026quot;); //$NON-NLS-1$ data.setUserid(null); //$NON-NLS-1$ data.setPassword(null); //$NON-NLS-1$ if(proxyType == SOCKSPROXY \u0026amp;\u0026amp; IProxyData.SOCKS_PROXY_TYPE.equals(data.getType())) {\nproxyData = data; continue; }else if(proxyType == WEBPROXY \u0026amp;\u0026amp; IProxyData.HTTP_PROXY_TYPE.equals(data.getType())){\nproxyData = data; continue; } data.setHost(null); //$NON-NLS-1$ data.setPort(0); } if(proxyData != null){ proxyData.setHost(proxyServer); proxyData.setPort(proxyPort); } try { proxyService.setProxyData(datas); } catch (CoreException e) { proxyService.setProxiesEnabled(false); proxyService.setSystemProxiesEnabled(false); return false; }\nOfficial API Reference\n","link":"https://kane.mx/posts/archive/blogspot/usage-of-eclipses-proxy-api/","section":"posts","tags":["Eclipse"],"title":"The usage of Eclipse's Proxy API"},{"body":"tune2fs -i 0 -c 0 /dev/sdx\n","link":"https://kane.mx/posts/archive/blogspot/turnoff-automatically-scanning-disk/","section":"posts","tags":["Tip","Linux"],"title":"[tip]Turn off automatically scanning disk when reboot"},{"body":"","link":"https://kane.mx/tags/linux/","section":"tags","tags":null,"title":"Linux"},{"body":"add below lines in ~/.gnomerc\nexport XMODIFIERS=\u0026quot;@im=fcitx\u0026quot;\nexport GTK_IM_MODULE=\u0026quot;xim\u0026quot;\n","link":"https://kane.mx/posts/archive/blogspot/how-to-set-default-input-method-for/","section":"posts","tags":null,"title":"How to set default input method for GNOME"},{"body":"create symbol link under lib/plugins of firefox to link jre/plugins/i386/ns**/libjavaplugin_oji.so\n","link":"https://kane.mx/posts/archive/blogspot/how-to-set-up-jre-environment-in/","section":"posts","tags":["JRE","Firefox","HowTo"],"title":"[HowTo]How to set up jre environment in firefox"},{"body":"","link":"https://kane.mx/tags/firefox/","section":"tags","tags":null,"title":"Firefox"},{"body":"","link":"https://kane.mx/tags/howto/","section":"tags","tags":null,"title":"HowTo"},{"body":"","link":"https://kane.mx/tags/jre/","section":"tags","tags":null,"title":"JRE"},{"body":"Such as,\n(gdb) handle SIGPIPE nostop noprint pass\n","link":"https://kane.mx/posts/archive/blogspot/how-to-ignore-specified-signal-when/","section":"posts","tags":["Tip","GDB","Debug"],"title":"[Debug][tip]How to ignore specified signal when debugging program via gdb"},{"body":"","link":"https://kane.mx/tags/debug/","section":"tags","tags":null,"title":"Debug"},{"body":"","link":"https://kane.mx/tags/gdb/","section":"tags","tags":null,"title":"GDB"},{"body":"Close Notes\nDouble click c:\\notes\\notes.ini to open it.\nAdd one new line \u0026quot;Display_font_adjustment=n\u0026quot; after the third line in notes.ini file. \u0026quot;n\u0026quot; is the number.It can be 1or 2 or 3....and the font will be larger with the number increasing.\nLaunch note\n","link":"https://kane.mx/posts/archive/blogspot/how-to-adjust-font-size-of-notes-editor/","section":"posts","tags":["Tip","IBM Notes"],"title":"[HowTo][tip]How to adjust the font size of Notes editor"},{"body":"","link":"https://kane.mx/tags/ibm-notes/","section":"tags","tags":null,"title":"IBM Notes"},{"body":"Set vm arguments 'osgi.framework.extensions' and 'osgi.frameworkClassPath' when vm starts. If those value are set, those jar or path would be added into the classloader when starting EclipseStarter.\nSee org.eclipse.equinox.launcher.Main for more details in the source code of Eclipse 3.4.\nBest Regards\nKane\n","link":"https://kane.mx/posts/archive/blogspot/add-custom-jar-or-path-into-equinox/","section":"posts","tags":["Equinox","OSGi","Eclipse"],"title":"[OSGi][Eclipse]Add custom jar or path into Equinox Framework"},{"body":"The answer is very simple, using the service 'org.eclipse.service.PackageAdmin'.\n","link":"https://kane.mx/posts/archive/blogspot/osgihow-to-acquire-fragments-of/","section":"posts","tags":["Equinox","OSGi","Eclipse"],"title":"[OSGi]How to acquire the fragments of specified bundle"},{"body":"Equinox uses the adaptor hooks to implement the class loader.\nSee http://wiki.eclipse.org/Adaptor_Hooks for more detail\nBaseClassLoadingHook would search the native code on itself. If it find the file in that jar file, it would extract the native library into its storage folder.\nEclipseClassLoadingHook defines some variables to search the native library. Belows are built-in variables:\nresult.add(\u0026quot;ws/\u0026quot; + info.getWS() + \u0026quot;/\u0026quot;); //$NON-NLS-1$ //$NON-NLS-2$\nresult.add(\u0026quot;os/\u0026quot; + info.getOS() + \u0026quot;/\u0026quot; + info.getOSArch() + \u0026quot;/\u0026quot;); //$NON-NLS-1$ //$NON-NLS-2$ //$NON-NLS-3$\nresult.add(\u0026quot;os/\u0026quot; + info.getOS() + \u0026quot;/\u0026quot;); //$NON-NLS-1$ //$NON-NLS-2$\nSo the classloader can find your native library that under those path. If your bundle is jar file, equinox would extract your native library into its storage folder.\nI prefer to use OSGi header(Bundle-NativeCode) defining the path of native code, which still works on other OSGi implementations.\nEquinox defines its url schema, one of them is named as 'BundleURLConnection'. From its name, we know it's used for describing the files of bundle. You can obtain the url of file that is located on bundle by Bundle.getResource()/Bundle.getEntry()/Bundle.findEntries()/Bundle.getResources(). The return value of those functions are an object of BundleURLConnection. Once it's used as the argument of FileLocator.toFileURL(URL), the jar bundle would be unpacked into its storage folder recursively.\n","link":"https://kane.mx/posts/archive/blogspot/eclipseequinoxs-classloader-and-its-url/","section":"posts","tags":["Equinox","OSGi","Eclipse"],"title":"[Eclipse]Equinox's classloader and its URL schema"},{"body":"¬†1. Â∞ÜË¶ÅÊéíÁâàÁöÑÊñáÂ≠óË¥¥Âà∞vim‰∫Ü\n2. set textwidth=70\n3. visualÊ®°Âºè‰∏ãÈÄâÊã©Ë¶ÅÊéíÁâàÁöÑÊñáÂ≠ó,Êåâgq, Â∞±ÂèòÊàê70Â≠óÊØç1Ë°åÁöÑÊ†ºÂºè‰∫Ü\n","link":"https://kane.mx/posts/archive/blogspot/tipvim/","section":"posts","tags":null,"title":"[tip][vim]ÊéíÁâàÂ∞èÊäÄÂ∑ß"},{"body":"In vi/vim,\nÔªøset file format=unix\nor dos2unix, unix2dos\n","link":"https://kane.mx/posts/archive/blogspot/tipconvert-dos-format-to-unix/","section":"posts","tags":["Tip","Linux"],"title":"[tip]convert dos format to unix"},{"body":"Build c/c++ project always need third party library on linux, such as gtk+, glib. Writing their absolute path in Makefile is not flexible way. You can use pkg-config instead of the absolute path. Below is code snippet:\nGTK_LIB=$(shell pkg-config --libs gtk+-2.0)\nGTK_INC=$(shell pkg-config --cflags gtk+-2.0)\ngcc -o yourlibrary.so $(GTK_INC) $(GTK_LIB)\n","link":"https://kane.mx/posts/archive/blogspot/makefile-tip/","section":"posts","tags":["Tip","Linux","Makefile"],"title":"[tip]Makefile"},{"body":"","link":"https://kane.mx/tags/makefile/","section":"tags","tags":null,"title":"Makefile"},{"body":"OSGi provides a mechanism to let user contribute custom schemes automatically. It avoid some restriction with Java facilities for extending the handlers. The more detail could be found from OSGi specification R4, which has description how OSGi implements URL Handler Service.\nUse a sample to illustrate how to contribute your scheme(protocol):\n1. register your URLStreamHandlerService implementation, which must contain a property named \u0026quot;url.handler.protocol\u0026quot;. below register my scheme 'smb'\npublic void start(BundleContext context) throws Exception {\nHashtable properties = new Hashtable();\nproperties.put( URLConstants.URL_HANDLER_PROTOCOL, new String[] { \u0026quot;smb\u0026quot; } );\ncontext.registerService(URLStreamHandlerService.class.getName(), new SmbURLHandler(), properties );\n}\n2. your URL Handler extends AbstractURLStreamHandlerService, and implements abstract function 'openConnection(URL)'\npublic class SmbURLHandler extends AbstractURLStreamHandlerService {\npublic URLConnection openConnection(URL url) throws IOException {\nreturn new SmbURLConnection(url);\n}\n}\n3. your URL Connection extends java.net.URLConnection\npublic class SmbURLConnection extends URLConnection {\nprotected SmbURLConnection(URL url) {\nsuper(url);\n}\npublic void connect() throws IOException {\n}\n}\n","link":"https://kane.mx/posts/archive/blogspot/url-handlers-service/","section":"posts","tags":["Equinox","URL Handler Service","OSGi"],"title":"[OSGi][Equinox]URL Handlers Service"},{"body":"","link":"https://kane.mx/tags/url-handler-service/","section":"tags","tags":null,"title":"URL Handler Service"},{"body":"OSGi Spec defines Bundle-NativeCode header to contain a specification of native code libraries contained in that bundle. All magic things are initialized by org.eclipse.osgi.internal.baseadaptor.DefaultClassLoader.findLibrary(String) and org.eclipse.osgi.framework.internal.core.BundleLoader.findLibrary(String). Then BundleLoader uses the org.eclipse.osgi.baseadaptor.BaseData(an implementation of BundleData) to find the library path, if the bundle is NOT a jar file, it would directly get the absolute path of library. Otherwise, the BaseData would extract the library file if it could NOT find it in OSGi bundle storage(located in ${data}/org.eclipse.osgi/bundles/[bundle_id]/.cp/). Refer to org.eclipse.osgi.baseadaptor.BaseData.findLibrary(String) for more detail.\n","link":"https://kane.mx/posts/archive/blogspot/bundle-nativecode-implementation-in/","section":"posts","tags":["Equinox","NativeCode","Bundle","OSGi","Eclipse"],"title":"[OSGi][Equinox]the Bundle-NativeCode implementation in Equinox"},{"body":"","link":"https://kane.mx/tags/bundle/","section":"tags","tags":null,"title":"Bundle"},{"body":"","link":"https://kane.mx/tags/nativecode/","section":"tags","tags":null,"title":"NativeCode"},{"body":"1\u0026gt; ÊòØËæìÂá∫Ê≠£Á°ÆÊï∞ÊçÆÔºå 2\u0026gt; ÂàôÊòØÈîôËØØÊï∞ÊçÆËæìÂá∫È°πÁõÆ, Ëã•Ë¶ÅÂêåÊó∂ÂÜôÂÖ•Âêå‰∏Ä‰∏™Ê°£Ê°àÈúÄË¶Å‰ΩøÁî® 2\u0026gt;\u0026amp;1 /dev/null ÊòØ‰ªÄ‰πàÂë¢ÔºüÂü∫Êú¨‰∏äÔºåÈÇ£Â∞±ÊúâÁÇπÂÉèÊòØ‰∏Ä‰∏™„ÄéÈªëÊ¥û„ÄèÁöÑÂûÉÂúæÊ°∂ÂäüËÉΩÔºÅÂΩì‰Ω†ËæìÂÖ•ÁöÑ‰ªª‰Ωï‰∏úË•øÂØºÂêëÂà∞Ëøô‰∏™ËôöÊãüÁöÑÂûÉÂúæÊ°∂Ë£ÖÁΩÆÊó∂Ôºå„Äé‰ªñÂ∞±‰ºöÂá≠Á©∫Ê∂àÂ§±‰∏çËßÅ‰∫ÜÔΩûÔΩû„Äè\n","link":"https://kane.mx/posts/archive/blogspot/linux-shelllearning-note-31908/","section":"posts","tags":["Linux","Shell"],"title":"[shell]Learning Note - 3/19/08"},{"body":"You must see the qualifier string property when exporting your features and plug-ins by Eclipse pde. But specified qualifier string won't appear after you export the features successfully.\nIf you want to use the qualifier string, you must define your feature and plug-in version like below:\n1.0.0.qualifier, 2.2.2.qaulifier\n:)\n","link":"https://kane.mx/posts/archive/blogspot/how-to-use-qualifier-string-when/","section":"posts","tags":["PDE","Eclipse"],"title":"[Eclipse]How to use qualifier string when exporting features and plug-ins"},{"body":"ÊúÄËøëÂá†Â§©Ë¢´‰∏Ä‰∏™Ê≥®ÂÜåË°®Áõ∏ÂÖ≥ÁöÑdefectÊêûÁöÑÁÑ¶Â§¥ÁÉÇÈ¢ù„ÄÇ\nËÉåÊôØÊòØËøôÊ†∑ÁöÑÔºå‰∫ßÂìÅÂú®ÂÆâË£ÖÁöÑÊó∂ÂÄôÈúÄË¶ÅÈÄöËøá‰øÆÊîπÊ≥®ÂÜåË°®Ê≥®ÂÜåÊñá‰ª∂ÂÖ≥ËÅîÁ≠â‰ø°ÊÅØ„ÄÇÂú®ÂÖàÂâçÂÆâË£ÖÁ®ãÂ∫èÂü∫‰∫éInstallShieldÊó∂Â∑•‰ΩúÊ≠£Á°ÆÔºå‰ΩÜÂú®ÊúÄËøëÂÆâË£ÖÁ®ãÂ∫èÊîπÁî®MSIÂêéÔºåÊàë‰ª¨ÂÜôÂÖ•Ê≥®ÂÜåË°®ÁöÑ‰ø°ÊÅØÊ≤°ÊúâË¢´ÂÜôÂà∞ÊâÄÊúüÊúõÁöÑ‰ΩçÁΩÆ„ÄÇ\nÈÄöËøáÂêÑÁßçËØïÈ™åÔºåÊü•ÊâæËµÑÊñôÔºåÁªà‰∫éÊêûÊòéÁôΩÂéüÂõ†„ÄÇÊàë‰ª¨‰øÆÊîπÊ≥®ÂÜåË°®ÁöÑËøõÁ®ã‰∏çÊòØÂΩìÂâçÁî®Êà∑ËøõÁ®ãÔºåËÄåÊòØÁ≥ªÁªüËøõÁ®ãÔºåÂõ†Ê≠§ÂÜôÂÖ•Âà∞HKEY_CURRENT_USER‰∏ãÁöÑÊï∞ÊçÆ‰∏çËÉΩË¢´ÂÜôÂÖ•Âà∞ÂΩìÂâçÁôªÈôÜÁî®Êà∑‰∏ã„ÄÇ\nWe should not use \u0026quot;HKEY_CURRENT_USER\u0026quot; to retrival current user's registry key value. Because Windows Services always startup before user login. It may happen some error or loading the wrong setting profile. If you still insist on using the current user registry key setting, please refer \u0026quot;RegOpenCurrentUser\u0026quot;.\nÊúÄÂêéÂè™Â•ΩÂ∞ÜËøô‰∫õÊï∞ÊçÆÂÜôÂà∞‰∫ÜLocal MachineÈîÆÂÄº‰∏ã„ÄÇ\n","link":"https://kane.mx/posts/archive/blogspot/suck-windows-registry/","section":"posts","tags":["Ê≥®ÂÜåË°®"],"title":"‰∏áÊÅ∂ÁöÑÊ≥®ÂÜåË°®"},{"body":"","link":"https://kane.mx/tags/%E6%B3%A8%E5%86%8C%E8%A1%A8/","section":"tags","tags":null,"title":"Ê≥®ÂÜåË°®"},{"body":"When you develop a rich client application base on eclipse framework, and your application require eclipse platform feature, you would find that your application has some menu items contributed by eclipse platform. Those menu items are defined by several plug-ins' implementation of actionSet extention point. In fact Eclipse provides an activity mechanism to suppress the extension points which you don't want to use. However, you must know the identification name of extension points which you want to suppress. It's a hard work to find out all of them from dozens of plugins. so, I wrote a utility function to list all the extension points of specified name.\nIExtensionRegistry registry = Platform.getExtensionRegistry(); IExtensionPoint extensionPoint = registry.getExtensionPoint(\u0026quot;org.eclipse.ui.actionSets\u0026quot;); IExtension\\[\\] extensions = extensionPoint.getExtensions(); for(int i = 0; i \u0026lt; extensions.length; i++){ IConfigurationElement elements\\[\\] = extensions\\[i \\].getConfigurationElements(); for(int j = 0; j \u0026lt; elements.length; j++){ String pluginId = elements\\[j\\].getNamespaceIdentifier(); if(pluginId.indexOf(\u0026quot;org.eclipse\u0026quot;) \u0026gt; -1){ //$NON-NLS-1$ IConfigurationElement\\[\\] subElements = elements\\[j\\].getChildren(\u0026quot;action\u0026quot;); for(int m = 0; m \u0026lt; subElements.length; m++){ System.out.println(\u0026quot;Plugin: \u0026quot; + pluginId + \u0026quot; Id: \u0026quot; + subElements\\[m\\].getAttribute(\u0026quot;id\u0026quot;)); } } } } and the follow snippet is about the activities of menus of eclipse platform:\n","link":"https://kane.mx/posts/archive/blogspot/get-rid-of-menus-of-eclipse-platform/","section":"posts","tags":["Eclipse","RCP"],"title":"[Eclipse]get rid of the menus of eclipse platform"},{"body":"","link":"https://kane.mx/tags/rcp/","section":"tags","tags":null,"title":"RCP"},{"body":"","link":"https://kane.mx/tags/jni/","section":"tags","tags":null,"title":"JNI"},{"body":"javaÁ®ãÂ∫èÂºÄÂèë‰∏≠ÁªèÂ∏∏Áî®Âà∞JNIË∞ÉÁî®Êú¨Âú∞library, ÂêåÊó∂ÂèàÂ∏åÊúõÂ∞ÜlibraryÂêåclassÊñá‰ª∂ÁºñËØëÊàê‰∏Ä‰∏™jarÊñá‰ª∂‰ª•Êñπ‰æødeploy.\n‰ΩÜÊòØJDKÁöÑclassloader‰∏çÊîØÊåÅ‰ªéjarÊñá‰ª∂‰∏≠Âä†ËΩΩlibrary, ‰∏Ä‰∏™ÂèòÈÄöÁöÑÊñπÊ≥ïÂ∞±ÊòØjarÈáåÁöÑlibrary‰ª•‰∏¥Êó∂Êñá‰ª∂ÁöÑÊñπÂºèÂÜôÂà∞‰∏¥Êó∂ÁõÆÂΩïÊàñjava.libraryÁõÆÂΩï.\nÈôÑ‰∏ä‰∏§ÁØáÊñáÊ°£ÈìæÊé• :\n**Load Library inside a jar file\n**\n‰ΩøÁî®JNIÊó∂ÔºåË£ÖËΩΩÊú¨Âú∞Â∫ìÁöÑÂ∞èÊäÄÂ∑ß\n","link":"https://kane.mx/posts/archive/blogspot/jar/","section":"posts","tags":["Java","JNI"],"title":"Âä†ËΩΩjarÊñá‰ª∂ÈáåÁöÑÊú¨Âú∞Â∫ì"},{"body":"Those days my work is focus on eclipse's update. Now I understand the general mechanism and meet some issues when using it in development work.\nThe update mechanism includes four major types: install, enable, disable and uninstall. And all of those operations can be executed by command line, such as installing a feature can use following line:\n-application org.eclipse.update.core.standaloneUpdate -command install -featureId my.feature -version 1.0.0 -from file:/v:/local_updateSite/ -to file:/v:/eclipse/.\nThe installation process would copy the feature and plugins which are included by the feature to the local site from the update site, then execute the feature's global install handler if it has one.\nSome strange issue occurs when I want to disable a feature.Then I try to disable the feature with command,\n-command disable -featureId my.feature -version 1.0.0 -to file:/v:/eclipse/\nThe output of command means that the command is executed successfully.\nBut I list the status of features with command line \u0026quot;-command listFeatures\u0026quot;, the status of my.feature is still enable.\nThen I try to uninstall my.feature with command,\n-command uninstall -featureId my.feature -version 1.0.0 -to file:/v:/eclipse/\nIt fails, and the following is the root cause found in log file.\n!MESSAGE [Cannot find unconfigured feature my.feature with version 1.0.0]\nunconfigured feature means the feature is disabled.\nI posted my question in forum, and one guy told me that it might be a bug of eclipse and advised me to fire a bug for it.\n","link":"https://kane.mx/posts/archive/blogspot/eclipse-update-support/","section":"posts","tags":["Update","Eclipse","RCP"],"title":"[Eclipse]Eclipse update support"},{"body":"","link":"https://kane.mx/tags/update/","section":"tags","tags":null,"title":"Update"},{"body":"I met a defect that dynamically created menu items disappear after creating a new viewPart. It caused me overtime last Friday. Today I find the root cause.\nThe scenario is:\nopen first document, the items are shown well\nopen another document, the items disappear\nThe requirement is that showing the menu items while current part is document, otherwise hide them.\nSo our implementation is:\nwhen current document part is deactivated, set menu items invisible\nwhen document part is activated, set menu items visible\nAfter debugging, I found that menu items was updated before the part activated listener was notified. Hence the menu is invisible while the parent menu is updated. The resolved solution is that setting menu items visible while part opened listener is notified.\n","link":"https://kane.mx/posts/archive/blogspot/call-sequence-between-partactivated-and/","section":"posts","tags":["Eclipse","RCP"],"title":"[Eclipse]The call sequence between partActivated and menu update"},{"body":"I need use remote debug in our project, however just some experience in Weblogic were found from internet. After my investigation, I got some experience about using Eclipse remote debug RCP.\nThere are two important parameters for jvm. And we must launch remote java app with those two parameters.\n-Xdebug //tells jvm starting with debug mode\n-Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=1044 //transport=dt_socket represents communication with socket, address=1044 represents that the port number is 1044\nThen there are 3 steps in local env:\n1.import source code into eclipse's project\n2.Debug-Remote Java Application, see attachement as a sample\n3.insert breakpoint,\nupdate:\na simpler way:\n-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8000\n","link":"https://kane.mx/posts/archive/blogspot/remote-debug-in-eclipse/","section":"posts","tags":["Java","Eclipse","Debug"],"title":"[debug][java]Remote debug in Eclipse"}]